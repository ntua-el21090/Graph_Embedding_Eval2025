{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
        "!pip install torch-sparse  -f https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
        "!pip install torch-geometric\n",
        "\n",
        "!pip install --use-deprecated=legacy-resolver karateclub networkx numpy pandas matplotlib scikit-learn\n",
        "\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch-geometric \\\n",
        "    -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n",
        "\n",
        "\n",
        "!pip install optuna\n",
        "!pip install karateclub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "XytJ5_IHYPuq",
        "outputId": "e8b26068-943a-46f5-ff7f-999c62f28252"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_scatter-2.1.2%2Bpt22cu121-cp312-cp312-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt22cu121\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_sparse-0.6.18%2Bpt22cu121-cp312-cp312-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt22cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2026.1.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0\n",
            "Collecting karateclub\n",
            "  Downloading karateclub-1.3.3.tar.gz (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.12/dist-packages (from karateclub) (4.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from karateclub) (4.67.1)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.12/dist-packages (from karateclub) (0.16)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from karateclub) (1.16.3)\n",
            "Collecting pygsp (from karateclub)\n",
            "  Downloading pygsp-0.6.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gensim>=4.0.0 (from karateclub)\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from karateclub) (1.17.0)\n",
            "Collecting python-Levenshtein (from karateclub)\n",
            "  Downloading python_levenshtein-0.27.3-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim>=4.0.0->karateclub) (7.5.0)\n",
            "Collecting Levenshtein==0.27.3 (from python-Levenshtein->karateclub)\n",
            "  Downloading levenshtein-0.27.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim>=4.0.0->karateclub) (2.0.1)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.3->python-Levenshtein->karateclub)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: karateclub\n",
            "  Building wheel for karateclub (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for karateclub: filename=karateclub-1.3.3-py3-none-any.whl size=101979 sha256=17c4976764feb585d89c4af80260222c4da5f90f13f65c2074679375405d164f\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/59/5b/cec587a448c281393eeed3604826bc3e3460970d69b23f7fe4\n",
            "Successfully built karateclub\n",
            "Installing collected packages: pygsp, gensim, rapidfuzz, Levenshtein, python-Levenshtein, karateclub\n",
            "\u001b[31mERROR: pip's legacy dependency resolver does not consider dependency conflicts when selecting packages. This behaviour is the source of the following dependency conflicts.\n",
            "karateclub 1.3.3 requires networkx<2.7, but you'll have networkx 3.6.1 which is incompatible.\n",
            "karateclub 1.3.3 requires numpy<1.23.0, but you'll have numpy 2.0.2 which is incompatible.\n",
            "karateclub 1.3.3 requires pandas<=1.3.5, but you'll have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Levenshtein-0.27.3 gensim-4.4.0 karateclub-1.3.3 pygsp-0.6.1 python-Levenshtein-0.27.3 rapidfuzz-3.14.3\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cpu)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cpu)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.2)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.9.0+cpu.html\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2026.1.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.7.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.9/413.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.7.0\n",
            "Requirement already satisfied: karateclub in /usr/local/lib/python3.12/dist-packages (1.3.3)\n",
            "Collecting numpy<1.23.0 (from karateclub)\n",
            "  Downloading numpy-1.22.4.zip (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m95.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSWv9udbOcKr",
        "outputId": "cb404692-daca-41c7-b877-98422eff7e1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Device: cpu\n"
          ]
        }
      ],
      "source": [
        "import os, time, json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from torch_geometric.nn import GINConv, global_add_pool\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import copy\n",
        "import networkx as nx\n",
        "from karateclub import Graph2Vec, NetLSD\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Paths\n",
        "CLASS_DIR = \"/content/drive/MyDrive/InformationSystems/Classification/embeddings\"\n",
        "BASE_DIR  = \"/content/drive/MyDrive/InformationSystems/Stability\"\n",
        "RESULTS_DIR = f\"{BASE_DIR}/results\"\n",
        "PLOTS_DIR   = f\"{BASE_DIR}/plots\"   # προαιρετικό, για plots stability curves\n",
        "\n",
        "os.makedirs(BASE_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(dataset,root=\"/content/data\"):\n",
        "    ds = TUDataset(root=root, name=dataset)\n",
        "    # ds[i] is a torch_geometric.data.Data with x, edge_index, y\n",
        "    print(f\"Loaded {dataset}:\", len(ds), \"graphs\",\n",
        "          \"| num_classes:\", ds.num_classes,\n",
        "          \"| num_node_features:\", ds.num_node_features)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "IHUs5wvSQhIT"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_nonempty_graph_indices(dataset):\n",
        "    \"\"\"Return indices of graphs with at least 3 nodes. \"\"\"\n",
        "    idxs = []\n",
        "    for i in range(len(dataset)):\n",
        "      d = dataset[i]\n",
        "      n = int(d.num_nodes) if d.num_nodes is not None else 0\n",
        "      if n > 2:\n",
        "        idxs.append(i)\n",
        "\n",
        "    removed = len(dataset) - len(idxs)\n",
        "    if removed > 0:\n",
        "        print(f\"[INFO] Filtered out {removed} ENZYMES graphs with 0 nodes for stability.\")\n",
        "\n",
        "    return idxs"
      ],
      "metadata": {
        "id": "o17ureayivmC"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _to_undirected_edge_index(edge_index, num_nodes):\n",
        "    # ensure undirected without duplicates\n",
        "    # edge_index: [2, E]\n",
        "    row, col = edge_index\n",
        "    rev = torch.stack([col, row], dim=0)\n",
        "    ei = torch.cat([edge_index, rev], dim=1)\n",
        "    # remove duplicates\n",
        "    ei = torch.unique(ei, dim=1)\n",
        "    # remove self-loops (optional)\n",
        "    mask = ei[0] != ei[1]\n",
        "    return ei[:, mask]\n",
        "\n",
        "def perturb_edges(data,\n",
        "                  remove_pct: float = 0.10,\n",
        "                  add_pct: float = 0.10,\n",
        "                  seed: int = 42,\n",
        "                  allow_self_loops: bool = False):\n",
        "    \"\"\"\n",
        "    Randomly remove % of edges and add % new random edges.\n",
        "    Returns a NEW Data object.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    d = copy.deepcopy(data)\n",
        "\n",
        "    num_nodes = int(d.num_nodes)\n",
        "    ei = d.edge_index.clone()\n",
        "\n",
        "    # Make undirected (ENZYMES is undirected)\n",
        "    ei = _to_undirected_edge_index(ei, num_nodes)\n",
        "\n",
        "    # Convert edges to numpy pairs\n",
        "    edges = ei.t().cpu().numpy()  # (E,2)\n",
        "    E = edges.shape[0]\n",
        "\n",
        "    # --- remove edges ---\n",
        "    n_remove = int(remove_pct * E)\n",
        "    if n_remove > 0 and E > 0:\n",
        "        idx_remove = rng.choice(E, size=min(n_remove, E), replace=False)\n",
        "        keep_mask = np.ones(E, dtype=bool)\n",
        "        keep_mask[idx_remove] = False\n",
        "        edges = edges[keep_mask]\n",
        "    else:\n",
        "        edges = edges\n",
        "\n",
        "    # --- add edges ---\n",
        "    edges_set = set(map(tuple, edges))\n",
        "    n_add = int(add_pct * max(1, E))\n",
        "\n",
        "    tries = 0\n",
        "    max_tries = max(1000, 10 * n_add)\n",
        "    added = 0\n",
        "\n",
        "    while added < n_add and tries < max_tries:\n",
        "        u = int(rng.integers(0, num_nodes))\n",
        "        v = int(rng.integers(0, num_nodes))\n",
        "        tries += 1\n",
        "\n",
        "        if (not allow_self_loops) and (u == v):\n",
        "            continue\n",
        "\n",
        "        # undirected: store both directions\n",
        "        if (u, v) in edges_set or (v, u) in edges_set:\n",
        "            continue\n",
        "\n",
        "        edges_set.add((u, v))\n",
        "        edges_set.add((v, u))\n",
        "        added += 1\n",
        "\n",
        "    new_edges = np.array(list(edges_set), dtype=np.int64)\n",
        "    new_ei = torch.from_numpy(new_edges).t().contiguous()\n",
        "\n",
        "    d.edge_index = new_ei\n",
        "    return d"
      ],
      "metadata": {
        "id": "Dnoxu69zQjVw"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shuffle_node_attributes(data, seed: int = 42, mode: str = \"permute_rows\"):\n",
        "    \"\"\"\n",
        "    Shuffle node attributes.\n",
        "    mode:\n",
        "      - \"permute_rows\": permute nodes (shuffle x rows)\n",
        "      - \"shuffle_columns\": shuffle each feature column independently\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    d = copy.deepcopy(data)\n",
        "\n",
        "    if getattr(d, \"x\", None) is None:\n",
        "        # ENZYMES έχει x, αλλά κρατάμε safe fallback\n",
        "        return d\n",
        "\n",
        "    x = d.x.clone().cpu().numpy()\n",
        "    n = x.shape[0]\n",
        "\n",
        "    if mode == \"permute_rows\":\n",
        "        perm = rng.permutation(n)\n",
        "        x = x[perm]\n",
        "    elif mode == \"shuffle_columns\":\n",
        "        for j in range(x.shape[1]):\n",
        "            rng.shuffle(x[:, j])\n",
        "    else:\n",
        "        raise ValueError(\"Unknown mode: \" + mode)\n",
        "\n",
        "    d.x = torch.from_numpy(x).to(d.x.dtype)\n",
        "    return d"
      ],
      "metadata": {
        "id": "BkvvFpgmQuAe"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_perturbed_graphs(dataset,\n",
        "                          edge_remove_pct: float,\n",
        "                          edge_add_pct: float,\n",
        "                          attr_shuffle: bool,\n",
        "                          seed: int,\n",
        "                          indices: list | None = None):\n",
        "    \"\"\"Create perturbated graphs for the given indices (or all graphs if indices=None).\"\"\"\n",
        "    if indices is None:\n",
        "      indices = list (range(len(dataset)))\n",
        "\n",
        "    perturbed = []\n",
        "    for pos,i in enumerate(indices):\n",
        "        g = dataset[i]\n",
        "        g2 = perturb_edges(g, remove_pct=edge_remove_pct, add_pct=edge_add_pct, seed=seed + pos)\n",
        "        if attr_shuffle:\n",
        "            g2 = shuffle_node_attributes(g2, seed=seed + 10_000 + pos, mode=\"permute_rows\")\n",
        "        perturbed.append(g2)\n",
        "    return perturbed"
      ],
      "metadata": {
        "id": "6yYH4M5uQxOM"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_embeddings_and_labels(method_name: str, experiment_num: str, base_dir: str = CLASS_DIR):\n",
        "    exp_dir = os.path.join(base_dir, method_name, str(experiment_num))\n",
        "    emb_path = os.path.join(exp_dir, \"embeddings.npy\")\n",
        "    labels_path = os.path.join(exp_dir, \"labels.npy\")\n",
        "\n",
        "    if not os.path.exists(emb_path):\n",
        "        raise FileNotFoundError(f\"Embeddings file not found: {emb_path}\")\n",
        "    if not os.path.exists(labels_path):\n",
        "        raise FileNotFoundError(f\"Labels file not found: {labels_path}\")\n",
        "\n",
        "    emb = np.load(emb_path)\n",
        "    y = np.load(labels_path)\n",
        "    return emb, y"
      ],
      "metadata": {
        "id": "5MlJvORWQzaU"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pyg_to_nx(data):\n",
        "    G = nx.Graph()\n",
        "    G.add_nodes_from(range(int(data.num_nodes)))\n",
        "\n",
        "    ei = data.edge_index.cpu().numpy()\n",
        "    edges = list(zip(ei[0], ei[1]))\n",
        "    G.add_edges_from(edges)\n",
        "    return G"
      ],
      "metadata": {
        "id": "aOcTCvDlQ5ZQ"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_accuracy(emb: np.ndarray, y: np.ndarray, C=10.0, gamma=\"scale\", seed: int = 42):\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        emb, y, test_size=0.2, random_state=seed, stratify=y # na do split sto classification\n",
        "    )\n",
        "    clf = make_pipeline(StandardScaler(), SVC(C=C, gamma=gamma, kernel=\"rbf\"))\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred = clf.predict(X_test)\n",
        "    return float(accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "CKNvz0qE3nsy"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# GIN (PyG) utilities\n",
        "# -------------------------\n",
        "class GINEncoderClassifier(nn.Module):\n",
        "    \"\"\"GIN encoder + graph-level pooling + classifier. Also returns graph embeddings.\"\"\"\n",
        "    def __init__(self, in_dim: int, hidden_dim: int, num_layers: int, num_classes: int, dropout: float = 0.5):\n",
        "        super().__init__()\n",
        "        self.dropout = dropout\n",
        "        self.convs = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "\n",
        "        # First layer\n",
        "        mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim)\n",
        "        )\n",
        "        self.convs.append(GINConv(mlp))\n",
        "        self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        # Remaining layers\n",
        "\n",
        "        for _ in range(num_layers -1):\n",
        "            mlp = nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(hidden_dim, hidden_dim)\n",
        "            )\n",
        "\n",
        "            self.convs.append(GINConv(mlp))\n",
        "            self.bns.append(nn.BatchNorm1d(hidden_dim))\n",
        "\n",
        "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "      for conv, bn in zip(self.convs, self.bns):\n",
        "            x = conv(x, edge_index)\n",
        "            x = bn(x)\n",
        "            x = F.relu(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "      g = global_add_pool(x, batch)  # graph embedding\n",
        "      out = self.classifier(g)\n",
        "      return out, g\n"
      ],
      "metadata": {
        "id": "4Vy-jv0h3uoH"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def _make_split_indices(n: int, seed: int = 42, test_size: float = 0.2):\n",
        "    rng = np.random.default_rng(seed)\n",
        "    idx = np.arange(n)\n",
        "    rng.shuffle(idx)\n",
        "    n_test = int(round(test_size * n))\n",
        "    test_idx = idx[:n_test]\n",
        "    train_idx = idx[n_test:]\n",
        "    return train_idx, test_idx\n",
        "\n",
        "\n",
        "def train_gin_model(model: nn.Module,\n",
        "                    train_loader: DataLoader,\n",
        "                    device: torch.device,\n",
        "                    epochs: int = 80,\n",
        "                    lr: float = 1e-3,\n",
        "                    weight_decay: float = 5e-4):\n",
        "    model.to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    start = time.time()\n",
        "    for _ in range(epochs):\n",
        "        model.train()\n",
        "        for batch in train_loader:\n",
        "            batch = batch.to(device)\n",
        "            opt.zero_grad()\n",
        "            logits, _ = model(batch.x, batch.edge_index, batch.batch)\n",
        "            loss = criterion(logits, batch.y)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    return time.time() - start\n",
        "\n",
        "\n",
        "def infer_gin_embeddings(model: nn.Module, loader: DataLoader, device: torch.device):\n",
        "    model.eval()\n",
        "    embs = []\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            _, g = model(batch.x, batch.edge_index, batch.batch)\n",
        "            embs.append(g.detach().cpu().numpy())\n",
        "    return np.vstack(embs)\n",
        "\n",
        "\n",
        "def gin_accuracy(model: nn.Module, loader: DataLoader, device: torch.device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in loader:\n",
        "            batch = batch.to(device)\n",
        "            logits, _ = model(batch.x, batch.edge_index, batch.batch)\n",
        "            pred = logits.argmax(dim=1)\n",
        "            correct += int((pred == batch.y).sum().item())\n",
        "            total += int(batch.y.size(0))\n",
        "    return float(correct / max(1, total))\n"
      ],
      "metadata": {
        "id": "Qrwpxk7M5pzZ"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_embeddings_gin(graphs_pyg: list,\n",
        "                           labels: np.ndarray,\n",
        "                           hidden_dim: int,\n",
        "                           num_layers: int,\n",
        "                           dropout: float,\n",
        "                           epochs: int,\n",
        "                           batch_size: int,\n",
        "                           lr: float,\n",
        "                           weight_decay: float,\n",
        "                           seed: int,\n",
        "                           device: torch.device):\n",
        "    \"\"\"Train a GIN on the provided graphs and return graph embeddings for ALL graphs.\"\"\"\n",
        "    # Ensure graphs have node features\n",
        "    for i, g in enumerate(graphs_pyg):\n",
        "        if getattr(g, \"x\", None) is None:\n",
        "            raise ValueError(f\"Graph {i} has no node features (x). GIN requires node features.\")\n",
        "\n",
        "    # Attach labels to graphs (PyG expects tensor y)\n",
        "    graphs = []\n",
        "    for g, y in zip(graphs_pyg, labels):\n",
        "        gg = copy.deepcopy(g)\n",
        "        gg.y = torch.tensor(int(y), dtype=torch.long)\n",
        "        graphs.append(gg)\n",
        "\n",
        "    n = len(graphs)\n",
        "    train_idx, test_idx = _make_split_indices(n, seed=seed, test_size=0.2)\n",
        "\n",
        "    train_subset = [graphs[i] for i in train_idx]\n",
        "    test_subset = [graphs[i] for i in test_idx]\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    in_dim = int(graphs[0].num_node_features)\n",
        "    num_classes = int(len(np.unique(labels)))\n",
        "\n",
        "    model = GINEncoderClassifier(\n",
        "        in_dim=in_dim,\n",
        "        hidden_dim=hidden_dim,\n",
        "        num_layers=num_layers,\n",
        "        num_classes=num_classes,\n",
        "        dropout=dropout,\n",
        "    )\n",
        "\n",
        "    train_time = train_gin_model(\n",
        "        model,\n",
        "        train_loader,\n",
        "        device=device,\n",
        "        epochs=epochs,\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay,\n",
        "    )\n",
        "\n",
        "    # Embeddings for ALL graphs\n",
        "    full_loader = DataLoader(graphs, batch_size=batch_size, shuffle=False)\n",
        "    start = time.time()\n",
        "    emb = infer_gin_embeddings(model, full_loader, device=device)\n",
        "    gen_time = time.time() - start\n",
        "\n",
        "    acc_test = gin_accuracy(model, test_loader, device=device)\n",
        "\n",
        "    info = {\n",
        "        \"gin_train_time (s)\": train_time,\n",
        "        \"gin_gen_time (s)\": gen_time,\n",
        "        \"gin_test_acc\": acc_test,\n",
        "        \"gin_hidden_dim\": hidden_dim,\n",
        "        \"gin_num_layers\": num_layers,\n",
        "        \"gin_dropout\": dropout,\n",
        "        \"gin_epochs\": epochs,\n",
        "        \"gin_batch_size\": batch_size,\n",
        "        \"gin_lr\": lr,\n",
        "        \"gin_weight_decay\": weight_decay,\n",
        "    }\n",
        "    return emb, info\n",
        "\n",
        "def compute_embeddings(method_name: str,\n",
        "                       graphs_pyg: list,\n",
        "                       labels: np.ndarray | None = None,\n",
        "                       embedding_dim: int = 128,\n",
        "                       seed: int = 42,\n",
        "                       device: torch.device = DEVICE,\n",
        "                       gin_params: dict | None = None):\n",
        "    \"\"\"Compute embeddings for supported methods: Graph2Vec, NetLSD, GIN.\"\"\"\n",
        "    m = method_name.lower()\n",
        "\n",
        "    # Unsupervised (KarateClub)\n",
        "    if m in {\"graph2vec\", \"netlsd\"}:\n",
        "        nx_graphs = [pyg_to_nx(g) for g in graphs_pyg]\n",
        "        start = time.time()\n",
        "        if m == \"graph2vec\":\n",
        "            model = Graph2Vec(dimensions=embedding_dim, seed=seed)\n",
        "            model.fit(nx_graphs)\n",
        "            emb = model.get_embedding()\n",
        "        else:  # netlsd\n",
        "            model = NetLSD()\n",
        "            model.fit(nx_graphs)\n",
        "            emb = model.get_embedding()\n",
        "        t = time.time() - start\n",
        "        return emb, {\"embedding_time (s)\": t}\n",
        "\n",
        "    # Supervised (PyG)\n",
        "    if m == \"gin\":\n",
        "        if labels is None:\n",
        "            raise ValueError(\"GIN requires labels to train.\")\n",
        "        gin_params = gin_params or {}\n",
        "        # Map embedding_dim -> hidden_dim (we keep naming consistent with saved emb dim)\n",
        "        hidden_dim = int(gin_params.get(\"hidden_dim\", embedding_dim))\n",
        "        info_emb, info = compute_embeddings_gin(\n",
        "            graphs_pyg=graphs_pyg,\n",
        "            labels=labels,\n",
        "            hidden_dim=hidden_dim,\n",
        "            num_layers=int(gin_params.get(\"num_layers\", 5)),\n",
        "            dropout=float(gin_params.get(\"dropout\", 0.5)),\n",
        "            epochs=int(gin_params.get(\"epochs\", 80)),\n",
        "            batch_size=int(gin_params.get(\"batch_size\", 128)),\n",
        "            lr=float(gin_params.get(\"lr\", 1e-3)),\n",
        "            weight_decay=float(gin_params.get(\"weight_decay\", 5e-4)),\n",
        "            seed=seed,\n",
        "            device=device,\n",
        "        )\n",
        "        return info_emb, info\n",
        "\n",
        "    raise ValueError(f\"Unsupported method: {method_name}\")\n"
      ],
      "metadata": {
        "id": "2DYYUhRR55dh"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_embedding_change(emb_orig: np.ndarray, emb_pert: np.ndarray):\n",
        "    \"\"\"\n",
        "    Returns mean cosine similarity and mean L2 distance per graph.\n",
        "    Requires same shape (N, D).\n",
        "    \"\"\"\n",
        "    if emb_orig.shape != emb_pert.shape:\n",
        "        raise ValueError(\n",
        "            f\"Embedding shapes differ: orig={emb_orig.shape}, pert={emb_pert.shape}. \"\n",
        "            \"Make sure recomputed embeddings have the SAME (N, D) as the saved ones.\"\n",
        "        )\n",
        "\n",
        "    a = emb_orig.astype(np.float64, copy=False)\n",
        "    b = emb_pert.astype(np.float64, copy=False)\n",
        "\n",
        "    a_norm = a / (np.linalg.norm(a, axis=1, keepdims=True) + 1e-12)\n",
        "    b_norm = b / (np.linalg.norm(b, axis=1, keepdims=True) + 1e-12)\n",
        "\n",
        "    cos = np.sum(a_norm * b_norm, axis=1)      # (N,)\n",
        "    l2  = np.linalg.norm(a - b, axis=1)        # (N,)\n",
        "\n",
        "    return {\n",
        "        \"mean_cosine\": float(np.mean(cos)),\n",
        "        \"std_cosine\": float(np.std(cos)),\n",
        "        \"mean_l2\": float(np.mean(l2)),\n",
        "        \"std_l2\": float(np.std(l2)),\n",
        "    }"
      ],
      "metadata": {
        "id": "eEdr0b6i7d3m"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def append_to_stability_log(method_name: str, row: dict):\n",
        "    log_path = os.path.join(RESULTS_DIR, f\"{method_name}_stability.csv\")\n",
        "    df = pd.DataFrame([row])\n",
        "    if os.path.exists(log_path):\n",
        "        df.to_csv(log_path, mode=\"a\", header=False, index=False)\n",
        "    else:\n",
        "        df.to_csv(log_path, mode=\"w\", header=True, index=False)\n",
        "    print(\"Logged:\", log_path)"
      ],
      "metadata": {
        "id": "XiCtaDOEREZR"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_stability_pipeline_enzymes(\n",
        "    method_name: str,\n",
        "    dataset: str,\n",
        "    experiment_num: str,\n",
        "    perturb_levels=(0.0, 0.05, 0.10, 0.20),\n",
        "    seeds=(42, 43, 44),\n",
        "    edge_add_equals_remove: bool = True,\n",
        "    attr_shuffle: bool = True,\n",
        "    svm_C: float = 10.0,\n",
        "    svm_gamma=\"scale\",\n",
        "):\n",
        "    ds = load_enzymes_dataset(dataset=dataset)\n",
        "    #graphs_orig = [ds[i] for i in range(len(ds))]\n",
        "    keep_indices = get_nonempty_graph_indices(ds)\n",
        "    graphs_orig = [ds[i] for i in keep_indices]\n",
        "\n",
        "    # Load original embeddings/labels from classification\n",
        "    emb_orig, y = load_embeddings_and_labels(method_name, experiment_num, base_dir=CLASS_DIR)\n",
        "\n",
        "    print(\"emb_orig rows:\", emb_orig.shape[0])\n",
        "    print(\"labels rows:\", len(y))\n",
        "\n",
        "    # VALIDATION\n",
        "    if emb_orig.ndim != 2:\n",
        "                    raise ValueError(f\"eemb_orig must be 2D array (N, D). Got shape: {emb_orig.shape}\")\n",
        "    if len(graphs_orig) != emb_orig.shape[0]:\n",
        "              raise ValueError(\n",
        "                f\"Mismatch between #graphs and emb_orig. \"\n",
        "                f\"#graphs={len(graphs_orig)} but emb_orig.shape[0]={emb_orig.shape[0]}\"\n",
        "              )\n",
        "\n",
        "    # Lock embedding_dim to saved embeddings (important!)\n",
        "    embedding_dim = emb_orig.shape[1]\n",
        "    print(f\"[INFO] Using embedding_dim from saved embeddings: {embedding_dim}\")\n",
        "\n",
        "    # Baseline accuracy on original embeddings\n",
        "    acc_orig = svm_accuracy(emb_orig, y, C=svm_C, gamma=svm_gamma, seed=42)\n",
        "\n",
        "    for lvl in perturb_levels:\n",
        "        for seed in seeds:\n",
        "            remove_pct = float(lvl)\n",
        "            add_pct = float(lvl) if edge_add_equals_remove else 0.0\n",
        "\n",
        "            # --- perturb graphs ---\n",
        "            t0 = time.time()\n",
        "            graphs_pert = make_perturbed_graphs(\n",
        "                ds,\n",
        "                edge_remove_pct=remove_pct,\n",
        "                edge_add_pct=add_pct,\n",
        "                attr_shuffle=attr_shuffle,\n",
        "                seed=seed,\n",
        "                indices = keep_indices\n",
        "            )\n",
        "            perturb_time = time.time() - t0\n",
        "\n",
        "            # --- recompute embeddings on perturbed graphs ---\n",
        "            emb_pert, emb_info = compute_embeddings(\n",
        "                method_name=method_name,\n",
        "                graphs_pyg=graphs_pert,\n",
        "                labels=y,\n",
        "                embedding_dim=embedding_dim,  # locked to emb_orig\n",
        "                seed=seed,\n",
        "                device=DEVICE,\n",
        "                gin_params= {\n",
        "                    \"hidden_dim\": embedding_dim,\n",
        "                    \"num_layers\": 5,\n",
        "                    \"dropout\": 0.5,\n",
        "                    \"epochs\": 80,\n",
        "                    \"batch_size\": 128,\n",
        "                    \"lr\": 1e-3,\n",
        "                    \"weight_decay\": 5e-4,\n",
        "                } if method_name.lower() == \"gin\" else None\n",
        "            )\n",
        "\n",
        "            # VALIDATION\n",
        "            if emb_pert.ndim != 2:\n",
        "                    raise ValueError(f\"emb_pert must be 2D array (N, D). Got shape: {emb_pert.shape}\")\n",
        "            if len(graphs_pert) != emb_pert.shape[0]:\n",
        "                  raise ValueError(\n",
        "                    f\"Mismatch between #graphs and emb_pert. \"\n",
        "                    f\"#graphs={len(graphs_pert)} but emb_pert.shape[0]={emb_pert.shape[0]}\"\n",
        "                )\n",
        "\n",
        "            # debug print\n",
        "            print(\"orig:\", emb_orig.shape, \"pert:\", emb_pert.shape)\n",
        "\n",
        "            # stability metrics\n",
        "            change = compute_embedding_change(emb_orig, emb_pert)\n",
        "\n",
        "            # accuracy on perturbed embeddings\n",
        "            acc_pert = svm_accuracy(emb_pert, y, C=svm_C, gamma=svm_gamma, seed=42)\n",
        "            acc_drop = acc_orig - acc_pert\n",
        "\n",
        "            row = {\n",
        "                \"experiment_num\": experiment_num,\n",
        "                \"dataset\": \"ENZYMES\",\n",
        "                \"embedding_type\": method_name,\n",
        "                \"seed\": seed,\n",
        "                \"edge_remove_pct\": remove_pct,\n",
        "                \"edge_add_pct\": add_pct,\n",
        "                \"attr_shuffle\": attr_shuffle,\n",
        "                \"perturb_time (s)\": perturb_time,\n",
        "                **emb_info, # embedding info includes timing for karateclub and training/gen info for GIN\n",
        "                \"acc_orig\": acc_orig,\n",
        "                \"acc_pert\": acc_pert,\n",
        "                \"acc_drop\": acc_drop,\n",
        "                **change\n",
        "            }\n",
        "\n",
        "            append_to_stability_log(method_name, row)\n",
        "\n",
        "    print(\"Stability run completed.\")"
      ],
      "metadata": {
        "id": "KlGoPTdbRKfq"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_stability_pipeline_enzymes(\n",
        "    method_name=\"GIN\",      # Graph2Vec | \"NetLSD\" | \"GIN\"\n",
        "    dataset=\"MUTAG\",\n",
        "    experiment_num=\"21112025_1511\",\n",
        "    perturb_levels=(0.0, 0.10),\n",
        "    seeds=(42,),\n",
        "    attr_shuffle=True\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mtusf1pIRPCy",
        "outputId": "16064da2-30a3-448d-e3f7-17f13641cda0"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded ENZYMES: 188 graphs | num_classes: 2 | num_node_features: 7\n",
            "emb_orig rows: 188\n",
            "labels rows: 188\n",
            "[INFO] Using embedding_dim from saved embeddings: 64\n",
            "orig: (188, 64) pert: (188, 64)\n",
            "Logged: /content/drive/MyDrive/InformationSystems/Stability/results/GIN_stability.csv\n",
            "orig: (188, 64) pert: (188, 64)\n",
            "Logged: /content/drive/MyDrive/InformationSystems/Stability/results/GIN_stability.csv\n",
            "Stability run completed.\n"
          ]
        }
      ]
    }
  ]
}