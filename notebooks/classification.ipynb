{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
        "!pip install torch-sparse  -f https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
        "!pip install torch-geometric\n",
        "\n",
        "!pip install --use-deprecated=legacy-resolver karateclub networkx numpy pandas matplotlib scikit-learn\n",
        "\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric \\\n",
        "    -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n",
        "\n",
        "\n",
        "!pip install optuna\n",
        "!pip install karateclub"
      ],
      "metadata": {
        "id": "-FzyRjdu0fpw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb60ba9e-e861-4167-bbda-3c944ede4e4a"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.12/dist-packages (2.1.2+pt22cu121)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.12/dist-packages (0.6.18+pt22cu121)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Requirement already satisfied: karateclub in /usr/local/lib/python3.12/dist-packages (1.3.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.12/dist-packages (from karateclub) (4.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from karateclub) (4.67.1)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.12/dist-packages (from karateclub) (0.16)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from karateclub) (1.16.3)\n",
            "Requirement already satisfied: pygsp in /usr/local/lib/python3.12/dist-packages (from karateclub) (0.6.1)\n",
            "Requirement already satisfied: gensim>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from karateclub) (4.4.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from karateclub) (1.17.0)\n",
            "Requirement already satisfied: python-Levenshtein in /usr/local/lib/python3.12/dist-packages (from karateclub) (0.27.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim>=4.0.0->karateclub) (7.5.0)\n",
            "Requirement already satisfied: Levenshtein==0.27.3 in /usr/local/lib/python3.12/dist-packages (from python-Levenshtein->karateclub) (0.27.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim>=4.0.0->karateclub) (2.0.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=3.9.0 in /usr/local/lib/python3.12/dist-packages (from Levenshtein==0.27.3->python-Levenshtein->karateclub) (3.14.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.8.0+cu126.html\n",
            "Requirement already satisfied: torch-scatter in /usr/local/lib/python3.12/dist-packages (2.1.2+pt22cu121)\n",
            "Requirement already satisfied: torch-sparse in /usr/local/lib/python3.12/dist-packages (0.6.18+pt22cu121)\n",
            "Requirement already satisfied: torch-cluster in /usr/local/lib/python3.12/dist-packages (1.6.3+pt28cu126)\n",
            "Requirement already satisfied: torch-spline-conv in /usr/local/lib/python3.12/dist-packages (1.2.2+pt28cu126)\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.2.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2025.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.12/dist-packages (from optuna) (6.10.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Requirement already satisfied: karateclub in /usr/local/lib/python3.12/dist-packages (1.3.3)\n",
            "Collecting numpy<1.23.0 (from karateclub)\n",
            "  Using cached numpy-1.22.4.zip (11.5 MB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d\n",
        "from torch_geometric.nn import GINConv, global_add_pool\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch_geometric.transforms import OneHotDegree\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import time, os, psutil\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from karateclub import NetLSD, Graph2Vec\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from torch_geometric.utils import to_networkx\n",
        "import networkx as nx\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ],
      "metadata": {
        "id": "luQxesBgfy04"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = \"/content/drive/MyDrive/InformationSystems/Classification\"\n",
        "RESULTS_DIR = f\"{BASE_DIR}/results\"\n",
        "MODELS_DIR = f\"{BASE_DIR}/models\"\n",
        "EMBEDDINGS_DIR = f\"{BASE_DIR}/embeddings\"\n",
        "\n",
        "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-KqwkGwOm_KI",
        "outputId": "fb4c4239-ba51-434e-d298-ecaa0c97f2ef"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sanitize_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Replace NaN/Inf values in embeddings and ensure a clean float32 array.\n",
        "    This is useful for karateclub embeddings that may occasionally produce\n",
        "    unstable values on some graphs.\n",
        "    \"\"\"\n",
        "    emb = np.asarray(embeddings, dtype=np.float32)\n",
        "    # Replace NaN and +/- Inf with 0.0\n",
        "    emb = np.nan_to_num(emb, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return emb"
      ],
      "metadata": {
        "id": "k5NN7KKnpufI"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_enzymes_graphs(graphs, labels, min_nodes: int = 3):\n",
        "    \"\"\"\n",
        "    Special handling for ENZYMES: remove very small graphs that can cause\n",
        "    numerical issues for NetLSD / Graph2Vec.\n",
        "    Returns filtered (graphs, labels) and prints how many were removed.\n",
        "    \"\"\"\n",
        "    if len(graphs) == 0:\n",
        "        return graphs, labels\n",
        "\n",
        "    mask = [g.number_of_nodes() >= min_nodes for g in graphs]\n",
        "    if not any(mask):\n",
        "        print(\"WARNING: All ENZYMES graphs would be filtered out. Skipping filtering.\")\n",
        "        return graphs, labels\n",
        "\n",
        "    filtered_graphs = [g for g, keep in zip(graphs, mask) if keep]\n",
        "    if isinstance(labels, np.ndarray):\n",
        "        filtered_labels = labels[np.array(mask)]\n",
        "    else:\n",
        "        filtered_labels = [y for y, keep in zip(labels, mask) if keep]\n",
        "\n",
        "    removed = len(graphs) - len(filtered_graphs)\n",
        "    print(f\"ENZYMES filtering: removed {removed} graphs with < {min_nodes} nodes, kept {len(filtered_graphs)} graphs.\")\n",
        "    return filtered_graphs, filtered_labels"
      ],
      "metadata": {
        "id": "BmVVRkY8pwlP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GIN Model Definition\n",
        "\n",
        "class GIN(torch.nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, num_classes, num_layers=5, dropout=0.5):\n",
        "        super(GIN, self).__init__()\n",
        "        layers = []\n",
        "        in_dim = num_features\n",
        "        for _ in range(num_layers):\n",
        "            nn = Sequential(Linear(in_dim, hidden_dim), ReLU(), Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(GINConv(nn))\n",
        "            in_dim = hidden_dim\n",
        "        self.convs = torch.nn.ModuleList(layers)\n",
        "        self.bns = torch.nn.ModuleList([BatchNorm1d(hidden_dim) for _ in range(num_layers)])\n",
        "        self.fc1 = Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = Linear(hidden_dim, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = bn(x)\n",
        "        x = global_add_pool(x, batch)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "boI0A_b4nfci"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        data = data.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    preds, labels, probs = [], [], []\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(DEVICE)\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "\n",
        "            loss = criterion(out, data.y)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            pred = out.argmax(dim=1)\n",
        "            preds.extend(pred.cpu().numpy())\n",
        "            labels.extend(data.y.cpu().numpy())\n",
        "            probs.extend(F.softmax(out, dim=1).cpu().numpy())  # probabilities for AUC\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    avg_loss = total_loss / max(1, num_batches)\n",
        "\n",
        "    try:\n",
        "        if len(np.unique(labels)) == 2:\n",
        "            auc = roc_auc_score(labels, np.array(probs)[:, 1])\n",
        "        else:\n",
        "            auc = roc_auc_score(labels, probs, multi_class='ovr')\n",
        "    except ValueError:\n",
        "        auc = np.nan  # if there are not enough samples for AUC\n",
        "\n",
        "    return acc, f1, auc, avg_loss"
      ],
      "metadata": {
        "id": "tHDMvRdhnjQm"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gin_embeddings(model, loader):\n",
        "    \"\"\"Return graph-level embeddings (after global_add_pool) and labels.\"\"\"\n",
        "    model.eval()\n",
        "    all_emb = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(DEVICE)\n",
        "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "            # forward μέχρι το pooling\n",
        "            for conv, bn in zip(model.convs, model.bns):\n",
        "                x = F.relu(conv(x, edge_index))\n",
        "                x = bn(x)\n",
        "            x = global_add_pool(x, batch)\n",
        "            all_emb.append(x.cpu().numpy())\n",
        "            all_labels.extend(data.y.cpu().numpy())\n",
        "    embeddings = np.concatenate(all_emb, axis=0)\n",
        "    labels = np.array(all_labels)\n",
        "    return embeddings, labels"
      ],
      "metadata": {
        "id": "NVzRwAdC4P2n"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_gin_pipeline(\n",
        "    dataset_name,\n",
        "    use_optuna,\n",
        "    w_acc,\n",
        "    w_f1,\n",
        "    w_auc,\n",
        "    hidden_dim,\n",
        "    epochs,\n",
        "    batch_size=32,\n",
        "    n_trials=15,\n",
        "):\n",
        "\n",
        "    # Experiment ID (used in logs and embeddings path)\n",
        "    experiment_num = time.strftime(\"%d%m%Y_%H%M\", time.localtime())\n",
        "    # Load dataset\n",
        "    dataset = TUDataset(root='data/TUDataset', name=dataset_name).shuffle()\n",
        "\n",
        "    # If no node features use one-hot degree features\n",
        "    if dataset.num_features == 0 or dataset[0].x is None:\n",
        "        print(\"Dataset has no node features. Applying OneHotDegree transform...\")\n",
        "\n",
        "        # Find maximum degree across all graphs\n",
        "        max_degree = 0\n",
        "        for data in dataset:\n",
        "            deg = torch.bincount(data.edge_index[0], minlength=data.num_nodes)\n",
        "            max_degree = max(max_degree, int(deg.max()))\n",
        "\n",
        "        # Apply transform\n",
        "        oh_transform = OneHotDegree(max_degree=max_degree)\n",
        "        dataset = TUDataset(\n",
        "            root='data/TUDataset',\n",
        "            name=dataset_name,\n",
        "            transform=oh_transform\n",
        "        ).shuffle()\n",
        "\n",
        "        num_node_features = max_degree + 1\n",
        "    else:\n",
        "        num_node_features = dataset.num_features\n",
        "\n",
        "    # Train/test split\n",
        "    train_dataset = dataset[:int(0.8 * len(dataset))]\n",
        "    test_dataset = dataset[int(0.8 * len(dataset)):]\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    print(f\"Loaded dataset {dataset_name}: {len(dataset)} graphs, {num_node_features} node features, {dataset.num_classes} classes\")\n",
        "\n",
        "    def objective(trial):\n",
        "        num_layers = trial.suggest_int(\"num_layers\", 3, 6)\n",
        "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.6)\n",
        "        lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
        "        weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
        "\n",
        "        model = GIN(num_node_features, hidden_dim, dataset.num_classes, num_layers, dropout).to(DEVICE)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(5):  # fewer epochs for fast tuning\n",
        "            train(model, train_loader, optimizer, criterion)\n",
        "\n",
        "        acc, f1, auc, _ = evaluate(model, test_loader, criterion)\n",
        "        score = (w_acc * acc) + (w_f1 * f1) + (w_auc * (0 if np.isnan(auc) else auc))\n",
        "        return score\n",
        "\n",
        "    start_generation = time.time()\n",
        "    if use_optuna:\n",
        "        print(\"Running Optuna for hyperparameter tuning...\")\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=n_trials)\n",
        "        best_params = study.best_params\n",
        "        print(f\"Best hyperparameters: {best_params}\")\n",
        "    else:\n",
        "      best_params = { \"num_layers\": 5, \"dropout\": 0.5, \"lr\": 0.001, \"weight_decay\": 1e-4}\n",
        "      print(f\"Using default hyperparameters: {best_params}\")\n",
        "\n",
        "    generation_time = time.time() - start_generation\n",
        "\n",
        "    # Final Training with best parameters\n",
        "\n",
        "    print(\"\\nRunning final training GIN...\")\n",
        "    print(best_params)\n",
        "\n",
        "    model = GIN(num_node_features, hidden_dim, dataset.num_classes,\n",
        "                num_layers=best_params[\"num_layers\"], dropout=best_params[\"dropout\"]).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"])\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    history = []\n",
        "    start_time = time.time()\n",
        "    eval_acc, eval_f1, eval_auc, eval_loss, eval_epoch = 0, 0, 0, 1e9, 0\n",
        "    best_loss_for_best_epoch = 1e9\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        loss = train(model, train_loader, optimizer, criterion)\n",
        "        acc, f1, auc, e_loss = evaluate(model, test_loader, criterion)\n",
        "        if acc > eval_acc:\n",
        "          #edo mipos to allakso na einai kai edo sindiasmos me weights poy eixe kai sto optuna\n",
        "          eval_acc, eval_f1, eval_auc, eval_loss, eval_epoch = acc, f1, auc, e_loss, epoch\n",
        "          best_loss_for_best_epoch = e_loss\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"Epoch {epoch:03d} | Loss={loss:.4f} | TestAcc={acc:.3f} | F1={f1:.3f} | AUC={auc:.3f} | Time={elapsed:.2f}s\")\n",
        "        history.append([epoch, loss, acc, f1, auc, elapsed])\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_usage = process.memory_info().rss / (1024 ** 2)  # in MB\n",
        "\n",
        "\n",
        "    # Save GIN embeddings for the whole dataset\n",
        "\n",
        "    full_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    gin_embeddings, gin_labels = get_gin_embeddings(model, full_loader)\n",
        "\n",
        "    gin_exp_dir = os.path.join(EMBEDDINGS_DIR, \"GIN\", experiment_num)\n",
        "    os.makedirs(gin_exp_dir, exist_ok=True)\n",
        "    np.save(os.path.join(gin_exp_dir, \"embeddings.npy\"), gin_embeddings)\n",
        "    np.save(os.path.join(gin_exp_dir, \"labels.npy\"), gin_labels)\n",
        "\n",
        "    # Log file save\n",
        "\n",
        "    summary_path = f\"{RESULTS_DIR}/training_log.csv\"\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "\n",
        "    summary_data = {\n",
        "        \"experiment_num\": experiment_num,\n",
        "        \"dataset\": dataset_name,\n",
        "        \"optimization_enabled\": \"yes\" if use_optuna else \"no\",\n",
        "        \"embedding_dimension\": hidden_dim,\n",
        "        \"objective_weights\": f\"({w_acc},{w_f1},{w_auc})\",\n",
        "        \"num_layers\": best_params[\"num_layers\"],\n",
        "        \"dropout\": best_params[\"dropout\"],\n",
        "        \"lr\": best_params[\"lr\"],\n",
        "        \"weight_decay\": best_params[\"weight_decay\"],\n",
        "        \"epochs\": epochs,\n",
        "        \"best_epoch\": eval_epoch,\n",
        "        \"best_loss\": round(float(best_loss_for_best_epoch), 4),\n",
        "        \"eval_loss\": round(float(eval_loss), 4),\n",
        "        \"eval_acc\": round(eval_acc, 4),\n",
        "        \"eval_f1\": round(eval_f1, 4),\n",
        "        \"eval_auc\": round(eval_auc, 4),\n",
        "        \"training_time (s)\": round(training_time, 2),\n",
        "        \"generation_time (s)\": round(generation_time, 2),\n",
        "        \"memory_usage (MB)\": round(memory_usage, 2)\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame([summary_data])\n",
        "\n",
        "    # Append mode (keep all trainings)\n",
        "    if os.path.exists(summary_path):\n",
        "        df.to_csv(summary_path, mode='a', index=False, header=False)\n",
        "    else:\n",
        "        df.to_csv(summary_path, index=False)\n",
        "\n",
        "    print(f\"\\nTraining summary stored in : {summary_path}\")\n",
        "    print(df)\n",
        "\n",
        "    # Save best model\n",
        "    torch.save(model.state_dict(), f\"{MODELS_DIR}/GIN_{dataset_name}.pth\")\n",
        "    print(f\"Saved model: results/GIN_{dataset_name}.pth\")\n"
      ],
      "metadata": {
        "id": "N7nB1kWECeeA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_graph2vec_pipeline(\n",
        "    dataset_name,\n",
        "    w_acc=0.5,\n",
        "    w_f1=0.3,\n",
        "    w_auc=0.2,\n",
        "    embedding_dim=128,\n",
        "    epochs=50,\n",
        "    test_size=0.2,\n",
        "    use_optuna=True,\n",
        "    n_trials=20,\n",
        "):\n",
        "    \"\"\"\n",
        "    Pipeline for graph classification using Graph2Vec embeddings + SVM,\n",
        "    with optional Optuna-based hyperparameter tuning and special handling\n",
        "    for ENZYMES + embedding sanitization.\n",
        "    \"\"\"\n",
        "    # Experiment ID\n",
        "    experiment_num = time.strftime(\"%d%m%Y_%H%M\", time.localtime())\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = TUDataset(root='data/TUDataset', name=dataset_name).shuffle()\n",
        "    print(f\"Loaded dataset {dataset_name} for Graph2Vec: {len(dataset)} graphs, {dataset.num_classes} classes\")\n",
        "\n",
        "    # Convert PyG graphs to NetworkX graphs\n",
        "    graphs = []\n",
        "    labels = []\n",
        "    for data in dataset:\n",
        "        g = to_networkx(data, to_undirected=True)\n",
        "        graphs.append(g)\n",
        "        labels.append(int(data.y.item()))\n",
        "\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Special handling for ENZYMES (filter very small graphs)\n",
        "    if dataset_name.upper() == \"ENZYMES\":\n",
        "        graphs, labels = filter_enzymes_graphs(graphs, labels, min_nodes=3)\n",
        "\n",
        "    # Outer train/test split on graphs\n",
        "    train_graphs, test_graphs, y_train, y_test = train_test_split(\n",
        "        graphs,\n",
        "        labels,\n",
        "        test_size=test_size,\n",
        "        random_state=42,\n",
        "        stratify=labels,\n",
        "    )\n",
        "\n",
        "    opt_time = 0.0\n",
        "\n",
        "    def objective(trial):\n",
        "        # Hyperparameters for the SVM classifier\n",
        "        C = trial.suggest_loguniform(\"C\", 1e-2, 1e2)\n",
        "        gamma = trial.suggest_loguniform(\"gamma\", 1e-4, 1e1)\n",
        "\n",
        "        # Inner train/validation split on graphs\n",
        "        inner_tr_graphs, inner_val_graphs, y_tr, y_val = train_test_split(\n",
        "            train_graphs,\n",
        "            y_train,\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=y_train,\n",
        "        )\n",
        "\n",
        "        all_graphs = inner_tr_graphs + inner_val_graphs\n",
        "\n",
        "        # Fit Graph2Vec on all (transductive setting) and slice embeddings\n",
        "        g2v = Graph2Vec(dimensions=embedding_dim, wl_iterations=2, epochs=epochs, workers=os.cpu_count())\n",
        "        g2v.fit(all_graphs)\n",
        "        emb_all = sanitize_embeddings(g2v.get_embedding())\n",
        "\n",
        "        X_tr = emb_all[:len(inner_tr_graphs)]\n",
        "        X_val = emb_all[len(inner_tr_graphs):]\n",
        "\n",
        "        clf = SVC(kernel=\"rbf\", probability=True, C=C, gamma=gamma, random_state=42)\n",
        "        clf.fit(X_tr, y_tr)\n",
        "        y_pred = clf.predict(X_val)\n",
        "        y_prob = clf.predict_proba(X_val)\n",
        "\n",
        "        acc = accuracy_score(y_val, y_pred)\n",
        "        f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
        "        try:\n",
        "            if len(np.unique(y_val)) == 2:\n",
        "                auc = roc_auc_score(y_val, y_prob[:, 1])\n",
        "            else:\n",
        "                auc = roc_auc_score(y_val, y_prob, multi_class=\"ovr\")\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "\n",
        "        score = (w_acc * acc) + (w_f1 * f1) + (w_auc * (0 if np.isnan(auc) else auc))\n",
        "        return score\n",
        "\n",
        "    if use_optuna:\n",
        "        print(\"Running Optuna for Graph2Vec+SVM hyperparameter tuning...\")\n",
        "        start_opt = time.time()\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=n_trials)\n",
        "        best_params = study.best_params\n",
        "        opt_time = time.time() - start_opt\n",
        "        print(f\"Best hyperparameters (Graph2Vec+SVM): {best_params}\")\n",
        "    else:\n",
        "        best_params = {\"C\": 1.0, \"gamma\": \"scale\"}\n",
        "        print(f\"Using default SVM hyperparameters: {best_params}\")\n",
        "\n",
        "    # Final embedding + training using best hyperparameters\n",
        "    print(\"Running final Graph2Vec embedding on train+test graphs...\")\n",
        "    all_graphs_final = train_graphs + test_graphs\n",
        "    start_embed = time.time()\n",
        "    g2v = Graph2Vec(dimensions=embedding_dim, wl_iterations=2, epochs=epochs, workers=os.cpu_count())\n",
        "    g2v.fit(all_graphs_final)\n",
        "    emb_all = sanitize_embeddings(g2v.get_embedding())\n",
        "    embed_time = time.time() - start_embed\n",
        "\n",
        "    X_train = emb_all[:len(train_graphs)]\n",
        "    X_test = emb_all[len(train_graphs):]\n",
        "\n",
        "    print(\"Training final SVM on Graph2Vec embeddings...\")\n",
        "    start_train = time.time()\n",
        "    clf = SVC(kernel=\"rbf\", probability=True, C=best_params[\"C\"], gamma=best_params[\"gamma\"], random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_train\n",
        "\n",
        "    # Evaluation on held-out test graphs\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_prob = clf.predict_proba(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "    try:\n",
        "        if len(np.unique(y_test)) == 2:\n",
        "            auc = roc_auc_score(y_test, y_prob[:, 1])\n",
        "        else:\n",
        "            auc = roc_auc_score(y_test, y_prob, multi_class=\"ovr\")\n",
        "    except ValueError:\n",
        "        auc = np.nan\n",
        "\n",
        "    score = (w_acc * acc) + (w_f1 * f1) + (w_auc * (0 if np.isnan(auc) else auc))\n",
        "\n",
        "\n",
        "    # Save Graph2Vec embeddings (for all graphs) + labels\n",
        "\n",
        "    g2v_exp_dir = os.path.join(EMBEDDINGS_DIR, \"Graph2Vec\", experiment_num)\n",
        "    os.makedirs(g2v_exp_dir, exist_ok=True)\n",
        "    np.save(os.path.join(g2v_exp_dir, \"embeddings.npy\"), emb_all)\n",
        "    np.save(os.path.join(g2v_exp_dir, \"labels.npy\"), np.array(labels))\n",
        "\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_usage = process.memory_info().rss / (1024 ** 2)  # in MB\n",
        "\n",
        "    print(f\"Graph2Vec Results on {dataset_name} -> Acc: {acc:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}, Score: {score:.3f}\")\n",
        "    print(f\"Embedding time: {embed_time:.2f}s | SVM training time: {train_time:.2f}s | Optuna time: {opt_time:.2f}s | Memory usage: {memory_usage:.2f} MB\")\n",
        "\n",
        "    # Log summary to CSV\n",
        "    summary_path = f\"{RESULTS_DIR}/graph2vec_log.csv\"\n",
        "\n",
        "    summary_data = {\n",
        "        \"experiment_num\": experiment_num,\n",
        "\n",
        "        \"dataset\": dataset_name,\n",
        "        \"embedding_type\": \"Graph2Vec\",\n",
        "        \"embedding_dimension\": embedding_dim,\n",
        "        \"optuna_enabled\": \"yes\" if use_optuna else \"no\",\n",
        "        \"C\": best_params[\"C\"],\n",
        "        \"gamma\": best_params[\"gamma\"],\n",
        "        \"acc\": round(float(acc), 4),\n",
        "        \"f1\": round(float(f1), 4),\n",
        "        \"auc\": round(float(auc) if not np.isnan(auc) else -1, 4),\n",
        "        \"score\": round(float(score), 4),\n",
        "        \"embedding_time (s)\": round(embed_time, 2),\n",
        "        \"svm_training_time (s)\": round(train_time, 2),\n",
        "        \"optuna_time (s)\": round(opt_time, 2),\n",
        "        \"memory_usage (MB)\": round(memory_usage, 2),\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame([summary_data])\n",
        "    if os.path.exists(summary_path):\n",
        "        df.to_csv(summary_path, mode='a', index=False, header=False)\n",
        "    else:\n",
        "        df.to_csv(summary_path, index=False)\n",
        "\n",
        "    print(f\"Graph2Vec summary stored in: {summary_path}\")\n"
      ],
      "metadata": {
        "id": "vrpvs2N3qJHQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_netlsd_pipeline(\n",
        "    dataset_name,\n",
        "    w_acc=0.5,\n",
        "    w_f1=0.3,\n",
        "    w_auc=0.2,\n",
        "    test_size=0.2,\n",
        "    use_optuna=True,\n",
        "    n_trials=20,\n",
        "):\n",
        "    \"\"\"\n",
        "    Pipeline for graph classification using NetLSD embeddings + SVM,\n",
        "    with optional Optuna-based hyperparameter tuning and ENZYMES filtering.\n",
        "    \"\"\"\n",
        "    # Experiment ID\n",
        "    experiment_num = time.strftime(\"%d%m%Y_%H%M\", time.localtime())\n",
        "    # Load dataset\n",
        "    dataset = TUDataset(root='data/TUDataset', name=dataset_name).shuffle()\n",
        "    print(f\"Loaded dataset {dataset_name} for NetLSD: {len(dataset)} graphs, {dataset.num_classes} classes\")\n",
        "\n",
        "    # Convert PyG graphs to NetworkX graphs\n",
        "    graphs = []\n",
        "    labels = []\n",
        "    for data in dataset:\n",
        "        g = to_networkx(data, to_undirected=True)\n",
        "        graphs.append(g)\n",
        "        labels.append(int(data.y.item()))\n",
        "\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Special handling for ENZYMES\n",
        "    if dataset_name.upper() == \"ENZYMES\":\n",
        "        graphs, labels = filter_enzymes_graphs(graphs, labels, min_nodes=3)\n",
        "\n",
        "    # Outer train/test split on graphs\n",
        "    train_graphs, test_graphs, y_train, y_test = train_test_split(\n",
        "        graphs,\n",
        "        labels,\n",
        "        test_size=test_size,\n",
        "        random_state=42,\n",
        "        stratify=labels,\n",
        "    )\n",
        "\n",
        "    opt_time = 0.0\n",
        "\n",
        "    def objective(trial):\n",
        "        # Hyperparameters for the SVM classifier\n",
        "        C = trial.suggest_loguniform(\"C\", 1e-2, 1e2)\n",
        "        gamma = trial.suggest_loguniform(\"gamma\", 1e-4, 1e1)\n",
        "\n",
        "        # Inner train/validation split on graphs\n",
        "        inner_tr_graphs, inner_val_graphs, y_tr, y_val = train_test_split(\n",
        "            train_graphs,\n",
        "            y_train,\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=y_train,\n",
        "        )\n",
        "\n",
        "        all_graphs = inner_tr_graphs + inner_val_graphs\n",
        "\n",
        "        # Fit NetLSD on all and slice embeddings\n",
        "        netlsd = NetLSD()\n",
        "        netlsd.fit(all_graphs)\n",
        "        emb_all = sanitize_embeddings(netlsd.get_embedding())\n",
        "\n",
        "        X_tr = emb_all[:len(inner_tr_graphs)]\n",
        "        X_val = emb_all[len(inner_tr_graphs):]\n",
        "\n",
        "        clf = SVC(kernel=\"rbf\", probability=True, C=C, gamma=gamma, random_state=42)\n",
        "        clf.fit(X_tr, y_tr)\n",
        "        y_pred = clf.predict(X_val)\n",
        "        y_prob = clf.predict_proba(X_val)\n",
        "\n",
        "        acc = accuracy_score(y_val, y_pred)\n",
        "        f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
        "        try:\n",
        "            if len(np.unique(y_val)) == 2:\n",
        "                auc = roc_auc_score(y_val, y_prob[:, 1])\n",
        "            else:\n",
        "                auc = roc_auc_score(y_val, y_prob, multi_class=\"ovr\")\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "\n",
        "        score = (w_acc * acc) + (w_f1 * f1) + (w_auc * (0 if np.isnan(auc) else auc))\n",
        "        return score\n",
        "\n",
        "    if use_optuna:\n",
        "        print(\"Running Optuna for NetLSD+SVM hyperparameter tuning...\")\n",
        "        start_opt = time.time()\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=n_trials)\n",
        "        best_params = study.best_params\n",
        "        opt_time = time.time() - start_opt\n",
        "        print(f\"Best hyperparameters (NetLSD+SVM): {best_params}\")\n",
        "    else:\n",
        "        best_params = {\"C\": 1.0, \"gamma\": \"scale\"}\n",
        "        print(f\"Using default SVM hyperparameters: {best_params}\")\n",
        "\n",
        "    # Final embedding + training using best hyperparameters\n",
        "    print(\"Running final NetLSD embedding on train+test graphs...\")\n",
        "    all_graphs_final = train_graphs + test_graphs\n",
        "    start_embed = time.time()\n",
        "    netlsd = NetLSD()\n",
        "    netlsd.fit(all_graphs_final)\n",
        "    emb_all = sanitize_embeddings(netlsd.get_embedding())\n",
        "    embed_time = time.time() - start_embed\n",
        "\n",
        "    X_train = emb_all[:len(train_graphs)]\n",
        "    X_test = emb_all[len(train_graphs):]\n",
        "\n",
        "    print(\"Training final SVM on NetLSD embeddings...\")\n",
        "    start_train = time.time()\n",
        "    clf = SVC(kernel=\"rbf\", probability=True, C=best_params[\"C\"], gamma=best_params[\"gamma\"], random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_train\n",
        "\n",
        "    # Evaluation on held-out test graphs\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_prob = clf.predict_proba(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "    try:\n",
        "        if len(np.unique(y_test)) == 2:\n",
        "            auc = roc_auc_score(y_test, y_prob[:, 1])\n",
        "        else:\n",
        "            auc = roc_auc_score(y_test, y_prob, multi_class=\"ovr\")\n",
        "    except ValueError:\n",
        "        auc = np.nan\n",
        "\n",
        "    score = (w_acc * acc) + (w_f1 * f1) + (w_auc * (0 if np.isnan(auc) else auc))\n",
        "\n",
        "\n",
        "    # Save NetLSD embeddings (all graphs) + labels\n",
        "\n",
        "    netlsd_exp_dir = os.path.join(EMBEDDINGS_DIR, \"NetLSD\", experiment_num)\n",
        "    os.makedirs(netlsd_exp_dir, exist_ok=True)\n",
        "    np.save(os.path.join(netlsd_exp_dir, \"embeddings.npy\"), emb_all)\n",
        "    np.save(os.path.join(netlsd_exp_dir, \"labels.npy\"), np.array(labels))\n",
        "\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_usage = process.memory_info().rss / (1024 ** 2)  # in MB\n",
        "\n",
        "    print(f\"NetLSD Results on {dataset_name} -> Acc: {acc:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}, Score: {score:.3f}\")\n",
        "    print(f\"Embedding time: {embed_time:.2f}s | SVM training time: {train_time:.2f}s | Optuna time: {opt_time:.2f}s | Memory usage: {memory_usage:.2f} MB\")\n",
        "\n",
        "    # Log summary to CSV\n",
        "    summary_path = f\"{RESULTS_DIR}/netlsd_log.csv\"\n",
        "\n",
        "    summary_data = {\n",
        "        \"experiment_num\": experiment_num,\n",
        "        \"dataset\": dataset_name,\n",
        "        \"embedding_type\": \"NetLSD\",\n",
        "        \"optuna_enabled\": \"yes\" if use_optuna else \"no\",\n",
        "        \"C\": best_params[\"C\"],\n",
        "        \"gamma\": best_params[\"gamma\"],\n",
        "        \"acc\": round(float(acc), 4),\n",
        "        \"f1\": round(float(f1), 4),\n",
        "        \"auc\": round(float(auc) if not np.isnan(auc) else -1, 4),\n",
        "        \"score\": round(float(score), 4),\n",
        "        \"embedding_time (s)\": round(embed_time, 2),\n",
        "        \"svm_training_time (s)\": round(train_time, 2),\n",
        "        \"optuna_time (s)\": round(opt_time, 2),\n",
        "        \"memory_usage (MB)\": round(memory_usage, 2),\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame([summary_data])\n",
        "    if os.path.exists(summary_path):\n",
        "        df.to_csv(summary_path, mode='a', index=False, header=False)\n",
        "    else:\n",
        "        df.to_csv(summary_path, index=False)\n",
        "\n",
        "    print(f\"NetLSD summary stored in: {summary_path}\")\n"
      ],
      "metadata": {
        "id": "GCpM6Q5cqOwL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_gin_pipeline(\n",
        "    dataset_name=\"MUTAG\",\n",
        "    use_optuna=True,\n",
        "    w_acc=0.5,\n",
        "    w_f1=0.3,\n",
        "    w_auc=0.2,\n",
        "    hidden_dim=64,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    n_trials=3,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5dGElCoqdAr",
        "outputId": "d55037eb-7488-4ec8-9d5d-967ad5411973"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-21 15:11:30,729] A new study created in memory with name: no-name-12706f1b-64ec-4aa5-b75b-3f6dc60a4d7b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset MUTAG: 188 graphs, 7 node features, 2 classes\n",
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-21 15:11:31,080] Trial 0 finished with value: 0.8847282347282348 and parameters: {'num_layers': 3, 'dropout': 0.05841567693412568, 'lr': 0.006407982415764923, 'weight_decay': 8.217058221938571e-06}. Best is trial 0 with value: 0.8847282347282348.\n",
            "[I 2025-11-21 15:11:31,400] Trial 1 finished with value: 0.2847372760906596 and parameters: {'num_layers': 3, 'dropout': 0.15278766282830905, 'lr': 0.0012825057091342817, 'weight_decay': 4.1179903887272965e-05}. Best is trial 0 with value: 0.8847282347282348.\n",
            "[I 2025-11-21 15:11:31,906] Trial 2 finished with value: 0.3426497340031175 and parameters: {'num_layers': 6, 'dropout': 0.4172555685589557, 'lr': 0.0036455932756505483, 'weight_decay': 0.0008042309870004285}. Best is trial 0 with value: 0.8847282347282348.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'num_layers': 3, 'dropout': 0.05841567693412568, 'lr': 0.006407982415764923, 'weight_decay': 8.217058221938571e-06}\n",
            "\n",
            "Running final training GIN...\n",
            "{'num_layers': 3, 'dropout': 0.05841567693412568, 'lr': 0.006407982415764923, 'weight_decay': 8.217058221938571e-06}\n",
            "Epoch 001 | Loss=0.6883 | TestAcc=0.711 | F1=0.590 | AUC=0.902 | Time=0.07s\n",
            "Epoch 002 | Loss=0.3776 | TestAcc=0.842 | F1=0.837 | AUC=0.929 | Time=0.14s\n",
            "Epoch 003 | Loss=0.3448 | TestAcc=0.868 | F1=0.861 | AUC=0.956 | Time=0.21s\n",
            "Epoch 004 | Loss=0.3301 | TestAcc=0.763 | F1=0.721 | AUC=0.904 | Time=0.28s\n",
            "Epoch 005 | Loss=0.2680 | TestAcc=0.789 | F1=0.761 | AUC=0.875 | Time=0.37s\n",
            "Epoch 006 | Loss=0.3046 | TestAcc=0.763 | F1=0.721 | AUC=0.941 | Time=0.44s\n",
            "Epoch 007 | Loss=0.5430 | TestAcc=0.789 | F1=0.761 | AUC=0.949 | Time=0.51s\n",
            "Epoch 008 | Loss=0.3017 | TestAcc=0.842 | F1=0.842 | AUC=0.919 | Time=0.58s\n",
            "Epoch 009 | Loss=0.2417 | TestAcc=0.868 | F1=0.866 | AUC=0.939 | Time=0.66s\n",
            "Epoch 010 | Loss=0.1973 | TestAcc=0.868 | F1=0.861 | AUC=0.946 | Time=0.73s\n",
            "\n",
            "Training summary stored in : /content/drive/MyDrive/InformationSystems/Classification/results/training_log.csv\n",
            "  experiment_num dataset optimization_enabled  embedding_dimension  \\\n",
            "0  21112025_1511   MUTAG                  yes                   64   \n",
            "\n",
            "  objective_weights  num_layers   dropout        lr  weight_decay  epochs  \\\n",
            "0     (0.5,0.3,0.2)           3  0.058416  0.006408      0.000008      10   \n",
            "\n",
            "   best_epoch  best_loss  eval_loss  eval_acc  eval_f1  eval_auc  \\\n",
            "0           3     0.1767     0.1767    0.8684   0.8615    0.9562   \n",
            "\n",
            "   training_time (s)  generation_time (s)  memory_usage (MB)  \n",
            "0               0.73                 1.18             959.85  \n",
            "Saved model: results/GIN_MUTAG.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_graph2vec_pipeline(\n",
        "    dataset_name=\"ENZYMES\",\n",
        "    w_acc=0.5, w_f1=0.3, w_auc=0.2,\n",
        "    embedding_dim=128,\n",
        "    epochs=10,\n",
        "    test_size=0.2,\n",
        "    use_optuna=True,\n",
        "    n_trials=3,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUWkRRajqf-k",
        "outputId": "90e9f80a-1b83-4b2a-c54e-165c17355596"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset ENZYMES for Graph2Vec: 600 graphs, 6 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-21 15:11:33,071] A new study created in memory with name: no-name-969b983e-1db5-4996-a612-1b2995dcb44c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES filtering: removed 1 graphs with < 3 nodes, kept 599 graphs.\n",
            "Running Optuna for Graph2Vec+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-21 15:11:33,867] Trial 0 finished with value: 0.21122012120801734 and parameters: {'C': 5.230738733974349, 'gamma': 1.2705242995137496}. Best is trial 0 with value: 0.21122012120801734.\n",
            "[I 2025-11-21 15:11:34,681] Trial 1 finished with value: 0.23571003181974545 and parameters: {'C': 0.12616462870858278, 'gamma': 0.00013262503528969994}. Best is trial 1 with value: 0.23571003181974545.\n",
            "[I 2025-11-21 15:11:35,433] Trial 2 finished with value: 0.2229684943626405 and parameters: {'C': 0.3051874850456814, 'gamma': 1.6141692229754927}. Best is trial 1 with value: 0.23571003181974545.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (Graph2Vec+SVM): {'C': 0.12616462870858278, 'gamma': 0.00013262503528969994}\n",
            "Running final Graph2Vec embedding on train+test graphs...\n",
            "Training final SVM on Graph2Vec embeddings...\n",
            "Graph2Vec Results on ENZYMES -> Acc: 0.167, F1: 0.106, AUC: 0.447, Score: 0.205\n",
            "Embedding time: 0.84s | SVM training time: 0.15s | Optuna time: 2.36s | Memory usage: 971.66 MB\n",
            "Graph2Vec summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/graph2vec_log.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run_netlsd_pipeline(\n",
        "    dataset_name=\"ENZYMES\",\n",
        "    w_acc=0.5, w_f1=0.3, w_auc=0.2,\n",
        "    test_size=0.2,\n",
        "    use_optuna=True,\n",
        "    n_trials=3,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6az0J-SNqi11",
        "outputId": "03028707-0e30-4266-f7e3-6cbefec0afbe"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset ENZYMES for NetLSD: 600 graphs, 6 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-21 15:11:36,803] A new study created in memory with name: no-name-e251be39-d5e6-4d2f-841f-4fd7807e2413\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES filtering: removed 1 graphs with < 3 nodes, kept 599 graphs.\n",
            "Running Optuna for NetLSD+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-11-21 15:11:40,244] Trial 0 finished with value: 0.21250000000000002 and parameters: {'C': 0.04593304718208167, 'gamma': 0.00029408622785242013}. Best is trial 0 with value: 0.21250000000000002.\n",
            "[I 2025-11-21 15:11:43,582] Trial 1 finished with value: 0.21278645833333337 and parameters: {'C': 0.020546034101263242, 'gamma': 0.0005022386467944024}. Best is trial 1 with value: 0.21278645833333337.\n",
            "[I 2025-11-21 15:11:45,982] Trial 2 finished with value: 0.21523437500000003 and parameters: {'C': 0.030472976931070834, 'gamma': 0.0066796320792242144}. Best is trial 2 with value: 0.21523437500000003.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (NetLSD+SVM): {'C': 0.030472976931070834, 'gamma': 0.0066796320792242144}\n",
            "Running final NetLSD embedding on train+test graphs...\n",
            "Training final SVM on NetLSD embeddings...\n",
            "NetLSD Results on ENZYMES -> Acc: 0.267, F1: 0.186, AUC: 0.388, Score: 0.267\n",
            "Embedding time: 5.66s | SVM training time: 0.51s | Optuna time: 9.18s | Memory usage: 982.07 MB\n",
            "NetLSD summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/netlsd_log.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}