{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FzyRjdu0fpw",
        "outputId": "b2339b33-8fc3-4b35-b1a9-b5ab50b25364",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_scatter-2.1.2%2Bpt22cu121-cp312-cp312-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m97.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt22cu121\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_sparse-0.6.18%2Bpt22cu121-cp312-cp312-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt22cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2026.1.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0\n",
            "Collecting karateclub\n",
            "  Downloading karateclub-1.3.3.tar.gz (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.12/dist-packages (from karateclub) (4.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from karateclub) (4.67.1)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.12/dist-packages (from karateclub) (0.16)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from karateclub) (1.16.3)\n",
            "Collecting pygsp (from karateclub)\n",
            "  Downloading pygsp-0.6.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gensim>=4.0.0 (from karateclub)\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from karateclub) (1.17.0)\n",
            "Collecting python-Levenshtein (from karateclub)\n",
            "  Downloading python_levenshtein-0.27.3-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim>=4.0.0->karateclub) (7.5.0)\n",
            "Collecting Levenshtein==0.27.3 (from python-Levenshtein->karateclub)\n",
            "  Downloading levenshtein-0.27.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim>=4.0.0->karateclub) (2.0.1)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.3->python-Levenshtein->karateclub)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: karateclub\n",
            "  Building wheel for karateclub (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for karateclub: filename=karateclub-1.3.3-py3-none-any.whl size=101979 sha256=399f7aec64d38bca319351294daaaebfaff9b1ed435379b3f321e68ad78e5e08\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/59/5b/cec587a448c281393eeed3604826bc3e3460970d69b23f7fe4\n",
            "Successfully built karateclub\n",
            "Installing collected packages: pygsp, gensim, rapidfuzz, Levenshtein, python-Levenshtein, karateclub\n",
            "\u001b[31mERROR: pip's legacy dependency resolver does not consider dependency conflicts when selecting packages. This behaviour is the source of the following dependency conflicts.\n",
            "karateclub 1.3.3 requires networkx<2.7, but you'll have networkx 3.6.1 which is incompatible.\n",
            "karateclub 1.3.3 requires numpy<1.23.0, but you'll have numpy 2.0.2 which is incompatible.\n",
            "karateclub 1.3.3 requires pandas<=1.3.5, but you'll have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Levenshtein-0.27.3 gensim-4.4.0 karateclub-1.3.3 pygsp-0.6.1 python-Levenshtein-0.27.3 rapidfuzz-3.14.3\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.9.0+cu126.html\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2026.1.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.18.1)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.7.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.9/413.9 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.7.0\n",
            "Requirement already satisfied: karateclub in /usr/local/lib/python3.12/dist-packages (1.3.3)\n",
            "Collecting numpy<1.23.0 (from karateclub)\n",
            "  Downloading numpy-1.22.4.zip (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m119.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
        "!pip install torch-sparse  -f https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
        "!pip install torch-geometric\n",
        "\n",
        "!pip install --use-deprecated=legacy-resolver karateclub networkx numpy pandas matplotlib scikit-learn\n",
        "\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch-geometric \\\n",
        "    -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n",
        "\n",
        "\n",
        "!pip install optuna\n",
        "!pip install karateclub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luQxesBgfy04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "086fba47-fc92-4fe5-ffaa-8f6c6554485d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_scatter/_version_cuda.so\n",
            "  import torch_geometric.typing\n",
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_sparse/_version_cuda.so\n",
            "  import torch_geometric.typing\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import joblib\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d\n",
        "from torch_geometric.nn import GINConv, global_add_pool\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch_geometric.transforms import OneHotDegree\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import time, os, psutil\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from karateclub import NetLSD, Graph2Vec\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from torch_geometric.utils import to_networkx\n",
        "import networkx as nx\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-KqwkGwOm_KI",
        "outputId": "ef117f2f-ab65-4a1d-85cd-c493a8379820"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = \"/content/drive/MyDrive/InformationSystems/Classification\"\n",
        "RESULTS_DIR = f\"{BASE_DIR}/results\"\n",
        "MODELS_DIR = f\"{BASE_DIR}/models\"\n",
        "EMBEDDINGS_DIR = f\"{BASE_DIR}/embeddings\"\n",
        "\n",
        "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5NN7KKnpufI"
      },
      "outputs": [],
      "source": [
        "def sanitize_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Replace NaN/Inf values in embeddings and ensure a clean float32 array.\n",
        "    This is useful for karateclub embeddings that may occasionally produce\n",
        "    unstable values on some graphs.\n",
        "    \"\"\"\n",
        "    emb = np.asarray(embeddings, dtype=np.float32)\n",
        "    # Replace NaN and +/- Inf with 0.0\n",
        "    emb = np.nan_to_num(emb, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BmVVRkY8pwlP"
      },
      "outputs": [],
      "source": [
        "def filter_enzymes_graphs(graphs, labels, min_nodes: int = 3):\n",
        "    \"\"\"\n",
        "    Special handling for ENZYMES: remove very small graphs that can cause\n",
        "    numerical issues for NetLSD / Graph2Vec.\n",
        "    Returns filtered (graphs, labels) and prints how many were removed.\n",
        "    \"\"\"\n",
        "    if len(graphs) == 0:\n",
        "        return graphs, labels\n",
        "\n",
        "    mask = [g.number_of_nodes() >= min_nodes for g in graphs]\n",
        "    if not any(mask):\n",
        "        print(\"WARNING: All ENZYMES graphs would be filtered out. Skipping filtering.\")\n",
        "        return graphs, labels\n",
        "\n",
        "    filtered_graphs = [g for g, keep in zip(graphs, mask) if keep]\n",
        "    if isinstance(labels, np.ndarray):\n",
        "       filtered_labels = labels[np.array(mask)]\n",
        "    else:\n",
        "        filtered_labels = [y for y, keep in zip(labels, mask) if keep]\n",
        "\n",
        "    removed = len(graphs) - len(filtered_graphs)\n",
        "    print(f\"ENZYMES filtering: removed {removed} graphs with < {min_nodes} nodes, kept {len(filtered_graphs)} graphs.\")\n",
        "    return filtered_graphs, filtered_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boI0A_b4nfci"
      },
      "outputs": [],
      "source": [
        "# GIN Model Definition\n",
        "\n",
        "class GIN(torch.nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, num_classes, num_layers=5, dropout=0.5):\n",
        "        super(GIN, self).__init__()\n",
        "        layers = []\n",
        "        in_dim = num_features\n",
        "        for _ in range(num_layers):\n",
        "            nn = Sequential(Linear(in_dim, hidden_dim), ReLU(), Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(GINConv(nn))\n",
        "            in_dim = hidden_dim\n",
        "        self.convs = torch.nn.ModuleList(layers)\n",
        "        self.bns = torch.nn.ModuleList([BatchNorm1d(hidden_dim) for _ in range(num_layers)])\n",
        "        self.fc1 = Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = Linear(hidden_dim, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = bn(x)\n",
        "        x = global_add_pool(x, batch)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tHDMvRdhnjQm"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        data = data.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    preds, labels, probs = [], [], []\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(DEVICE)\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "\n",
        "            loss = criterion(out, data.y)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            pred = out.argmax(dim=1)\n",
        "            preds.extend(pred.cpu().numpy())\n",
        "            labels.extend(data.y.cpu().numpy())\n",
        "            probs.extend(F.softmax(out, dim=1).cpu().numpy())  # probabilities for AUC\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    avg_loss = total_loss / max(1, num_batches)\n",
        "\n",
        "    try:\n",
        "        if len(np.unique(labels)) == 2:\n",
        "            auc = roc_auc_score(labels, np.array(probs)[:, 1])\n",
        "        else:\n",
        "            auc = roc_auc_score(labels, probs, multi_class='ovr')\n",
        "    except ValueError:\n",
        "        auc = np.nan  # if there are not enough samples for AUC\n",
        "\n",
        "    return acc, f1, auc, avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVzRwAdC4P2n"
      },
      "outputs": [],
      "source": [
        "def get_gin_embeddings(model, loader):\n",
        "    \"\"\"Return graph-level embeddings (after global_add_pool) and labels.\"\"\"\n",
        "    model.eval()\n",
        "    all_emb = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(DEVICE)\n",
        "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "            # forward μέχρι το pooling\n",
        "            for conv, bn in zip(model.convs, model.bns):\n",
        "                x = F.relu(conv(x, edge_index))\n",
        "                x = bn(x)\n",
        "            x = global_add_pool(x, batch)\n",
        "            all_emb.append(x.cpu().numpy())\n",
        "            all_labels.extend(data.y.cpu().numpy())\n",
        "    embeddings = np.concatenate(all_emb, axis=0)\n",
        "    labels = np.array(all_labels)\n",
        "    return embeddings, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7nB1kWECeeA"
      },
      "outputs": [],
      "source": [
        "def run_gin_pipeline(\n",
        "    dataset_name,\n",
        "    use_optuna,\n",
        "    w_acc,\n",
        "    w_f1,\n",
        "    w_auc,\n",
        "    hidden_dim,\n",
        "    epochs,\n",
        "    batch_size=32,\n",
        "    n_trials=15,\n",
        "):\n",
        "\n",
        "    # Experiment ID (used in logs and embeddings path)\n",
        "    experiment_num = time.strftime(\"%d%m%Y_%H%M\", time.localtime())\n",
        "    # Load dataset\n",
        "    dataset = TUDataset(root='data/TUDataset', name=dataset_name).shuffle()\n",
        "\n",
        "    # If no node features use one-hot degree features\n",
        "    if dataset.num_features == 0 or dataset[0].x is None:\n",
        "        print(\"Dataset has no node features. Applying OneHotDegree transform...\")\n",
        "\n",
        "        # Find maximum degree across all graphs\n",
        "        max_degree = 0\n",
        "        for data in dataset:\n",
        "            deg = torch.bincount(data.edge_index[0], minlength=data.num_nodes)\n",
        "            max_degree = max(max_degree, int(deg.max()))\n",
        "\n",
        "        # Apply transform\n",
        "        oh_transform = OneHotDegree(max_degree=max_degree)\n",
        "        dataset = TUDataset(\n",
        "            root='data/TUDataset',\n",
        "            name=dataset_name,\n",
        "            transform=oh_transform\n",
        "        ).shuffle()\n",
        "\n",
        "        num_node_features = max_degree + 1\n",
        "    else:\n",
        "        num_node_features = dataset.num_features\n",
        "\n",
        "    # Train/test split\n",
        "    train_dataset = dataset[:int(0.8 * len(dataset))]\n",
        "    test_dataset = dataset[int(0.8 * len(dataset)):]\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    print(f\"Loaded dataset {dataset_name}: {len(dataset)} graphs, {num_node_features} node features, {dataset.num_classes} classes\")\n",
        "\n",
        "    def objective(trial):\n",
        "        num_layers = trial.suggest_int(\"num_layers\", 3, 6)\n",
        "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.6)\n",
        "        lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
        "        weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
        "\n",
        "        model = GIN(num_node_features, hidden_dim, dataset.num_classes, num_layers, dropout).to(DEVICE)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(5):  # fewer epochs for fast tuning\n",
        "            train(model, train_loader, optimizer, criterion)\n",
        "\n",
        "        acc, f1, auc, _ = evaluate(model, test_loader, criterion)\n",
        "        score = (w_acc * acc) + (w_f1 * f1) + (w_auc * (0 if np.isnan(auc) else auc))\n",
        "        return score\n",
        "\n",
        "    start_generation = time.time()\n",
        "    if use_optuna:\n",
        "        print(\"Running Optuna for hyperparameter tuning...\")\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=n_trials)\n",
        "        best_params = study.best_params\n",
        "        print(f\"Best hyperparameters: {best_params}\")\n",
        "    else:\n",
        "      best_params = { \"num_layers\": 5, \"dropout\": 0.5, \"lr\": 0.001, \"weight_decay\": 1e-4}\n",
        "      print(f\"Using default hyperparameters: {best_params}\")\n",
        "\n",
        "    generation_time = time.time() - start_generation\n",
        "\n",
        "    # Final Training with best parameters\n",
        "\n",
        "    print(\"\\nRunning final training GIN...\")\n",
        "    print(best_params)\n",
        "\n",
        "    model = GIN(num_node_features, hidden_dim, dataset.num_classes,\n",
        "                num_layers=best_params[\"num_layers\"], dropout=best_params[\"dropout\"]).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"])\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    history = []\n",
        "    start_time = time.time()\n",
        "    eval_acc, eval_f1, eval_auc, eval_loss, eval_epoch = 0, 0, 0, 1e9, 0\n",
        "    best_loss_for_best_epoch = 1e9\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        loss = train(model, train_loader, optimizer, criterion)\n",
        "        acc, f1, auc, e_loss = evaluate(model, test_loader, criterion)\n",
        "        if acc > eval_acc:\n",
        "          #edo mipos to allakso na einai kai edo sindiasmos me weights poy eixe kai sto optuna\n",
        "          eval_acc, eval_f1, eval_auc, eval_loss, eval_epoch = acc, f1, auc, e_loss, epoch\n",
        "          best_loss_for_best_epoch = e_loss\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"Epoch {epoch:03d} | Loss={loss:.4f} | TestAcc={acc:.3f} | F1={f1:.3f} | AUC={auc:.3f} | Time={elapsed:.2f}s\")\n",
        "        history.append([epoch, loss, acc, f1, auc, elapsed])\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_usage = process.memory_info().rss / (1024 ** 2)  # in MB\n",
        "\n",
        "\n",
        "    # Save GIN embeddings for the whole dataset\n",
        "\n",
        "    full_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    gin_embeddings, gin_labels = get_gin_embeddings(model, full_loader)\n",
        "\n",
        "    gin_exp_dir = os.path.join(EMBEDDINGS_DIR, \"GIN\", dataset_name, experiment_num)\n",
        "    os.makedirs(gin_exp_dir, exist_ok=True)\n",
        "    np.save(os.path.join(gin_exp_dir, \"embeddings.npy\"), gin_embeddings)\n",
        "    np.save(os.path.join(gin_exp_dir, \"labels.npy\"), gin_labels)\n",
        "\n",
        "    # Log file save\n",
        "\n",
        "    summary_path = f\"{RESULTS_DIR}/gin_log.csv\"\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "\n",
        "    summary_data = {\n",
        "        \"experiment_num\": experiment_num,\n",
        "        \"dataset\": dataset_name,\n",
        "        \"optimization_enabled\": \"yes\" if use_optuna else \"no\",\n",
        "        \"embedding_dimension\": hidden_dim,\n",
        "        \"objective_weights\": f\"({w_acc},{w_f1},{w_auc})\",\n",
        "        \"num_layers\": best_params[\"num_layers\"],\n",
        "        \"dropout\": best_params[\"dropout\"],\n",
        "        \"lr\": best_params[\"lr\"],\n",
        "        \"weight_decay\": best_params[\"weight_decay\"],\n",
        "        \"epochs\": epochs,\n",
        "        \"best_epoch\": eval_epoch,\n",
        "        \"best_loss\": round(float(best_loss_for_best_epoch), 4),\n",
        "        \"eval_loss\": round(float(eval_loss), 4),\n",
        "        \"eval_acc\": round(eval_acc, 4),\n",
        "        \"eval_f1\": round(eval_f1, 4),\n",
        "        \"eval_auc\": round(eval_auc, 4),\n",
        "        \"training_time (s)\": round(training_time, 2),\n",
        "        \"generation_time (s)\": round(generation_time, 2),\n",
        "        \"memory_usage (MB)\": round(memory_usage, 2)\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame([summary_data])\n",
        "\n",
        "    # Append mode (keep all trainings)\n",
        "    if os.path.exists(summary_path):\n",
        "        df.to_csv(summary_path, mode='a', index=False, header=False)\n",
        "    else:\n",
        "        df.to_csv(summary_path, index=False)\n",
        "\n",
        "    print(f\"\\nTraining summary stored in : {summary_path}\")\n",
        "    print(df)\n",
        "\n",
        "    # Save best model (weights + metadata)\n",
        "    gin_ckpt = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"num_node_features\": num_node_features,\n",
        "        \"hidden_dim\": hidden_dim,\n",
        "        \"num_classes\": dataset.num_classes,\n",
        "        \"num_layers\": best_params[\"num_layers\"],\n",
        "        \"dropout\": best_params[\"dropout\"],\n",
        "    }\n",
        "    torch.save(gin_ckpt, f\"{MODELS_DIR}/GIN_{dataset_name}_{experiment_num}.pth\")\n",
        "    print(f\"Saved model: {MODELS_DIR}/GIN_{dataset_name}_{experiment_num}.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vrpvs2N3qJHQ"
      },
      "outputs": [],
      "source": [
        "def run_graph2vec_pipeline(\n",
        "    dataset_name,\n",
        "    w_acc=0.5,\n",
        "    w_f1=0.3,\n",
        "    w_auc=0.2,\n",
        "    embedding_dim=128,\n",
        "    epochs=50,\n",
        "    test_size=0.2,\n",
        "    use_optuna=True,\n",
        "    n_trials=20,\n",
        "):\n",
        "    \"\"\"\n",
        "    Pipeline for graph classification using Graph2Vec embeddings + SVM,\n",
        "    with optional Optuna-based hyperparameter tuning and special handling\n",
        "    for ENZYMES + embedding sanitization.\n",
        "    \"\"\"\n",
        "    # Experiment ID\n",
        "    experiment_num = time.strftime(\"%d%m%Y_%H%M\", time.localtime())\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = TUDataset(root='data/TUDataset', name=dataset_name).shuffle()\n",
        "    print(f\"Loaded dataset {dataset_name} for Graph2Vec: {len(dataset)} graphs, {dataset.num_classes} classes\")\n",
        "\n",
        "    # Convert PyG graphs to NetworkX graphs\n",
        "    graphs = []\n",
        "    labels = []\n",
        "    for data in dataset:\n",
        "        g = to_networkx(data, to_undirected=True)\n",
        "        graphs.append(g)\n",
        "        labels.append(int(data.y.item()))\n",
        "\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Special handling for ENZYMES (filter very small graphs)\n",
        "    if dataset_name.upper() == \"ENZYMES\":\n",
        "        graphs, labels = filter_enzymes_graphs(graphs, labels, min_nodes=3)\n",
        "\n",
        "    # Outer train/test split on graphs\n",
        "    train_graphs, test_graphs, y_train, y_test = train_test_split(\n",
        "        graphs,\n",
        "        labels,\n",
        "        test_size=test_size,\n",
        "        random_state=42,\n",
        "        stratify=labels,\n",
        "    )\n",
        "\n",
        "    opt_time = 0.0\n",
        "\n",
        "    def objective(trial):\n",
        "        # Hyperparameters for the SVM classifier\n",
        "        C = trial.suggest_loguniform(\"C\", 1e-2, 1e2)\n",
        "        gamma = trial.suggest_loguniform(\"gamma\", 1e-4, 1e1)\n",
        "\n",
        "        # Inner train/validation split on graphs\n",
        "        inner_tr_graphs, inner_val_graphs, y_tr, y_val = train_test_split(\n",
        "            train_graphs,\n",
        "            y_train,\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=y_train,\n",
        "        )\n",
        "\n",
        "        all_graphs = inner_tr_graphs + inner_val_graphs\n",
        "\n",
        "        # Fit Graph2Vec on all (transductive setting) and slice embeddings\n",
        "        g2v = Graph2Vec(dimensions=embedding_dim, wl_iterations=2, epochs=epochs, workers=os.cpu_count())\n",
        "        g2v.fit(all_graphs)\n",
        "        emb_all = sanitize_embeddings(g2v.get_embedding())\n",
        "\n",
        "        X_tr = emb_all[:len(inner_tr_graphs)]\n",
        "        X_val = emb_all[len(inner_tr_graphs):]\n",
        "\n",
        "        clf = SVC(kernel=\"rbf\", probability=True, C=C, gamma=gamma, random_state=42)\n",
        "        clf.fit(X_tr, y_tr)\n",
        "        y_pred = clf.predict(X_val)\n",
        "        y_prob = clf.predict_proba(X_val)\n",
        "\n",
        "        acc = accuracy_score(y_val, y_pred)\n",
        "        f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
        "        try:\n",
        "            if len(np.unique(y_val)) == 2:\n",
        "                auc = roc_auc_score(y_val, y_prob[:, 1])\n",
        "            else:\n",
        "                auc = roc_auc_score(y_val, y_prob, multi_class=\"ovr\")\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "\n",
        "        score = (w_acc * acc) + (w_f1 * f1) + (w_auc * (0 if np.isnan(auc) else auc))\n",
        "        return score\n",
        "\n",
        "    if use_optuna:\n",
        "        print(\"Running Optuna for Graph2Vec+SVM hyperparameter tuning...\")\n",
        "        start_opt = time.time()\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=n_trials)\n",
        "        best_params = study.best_params\n",
        "        opt_time = time.time() - start_opt\n",
        "        print(f\"Best hyperparameters (Graph2Vec+SVM): {best_params}\")\n",
        "    else:\n",
        "        best_params = {\"C\": 1.0, \"gamma\": \"scale\"}\n",
        "        print(f\"Using default SVM hyperparameters: {best_params}\")\n",
        "\n",
        "    # Final embedding + training using best hyperparameters\n",
        "    print(\"Running final Graph2Vec embedding on train+test graphs...\")\n",
        "    all_graphs_final = train_graphs + test_graphs\n",
        "    start_embed = time.time()\n",
        "    g2v = Graph2Vec(dimensions=embedding_dim, wl_iterations=2, epochs=epochs, workers=os.cpu_count())\n",
        "    g2v.fit(all_graphs_final)\n",
        "    emb_all = sanitize_embeddings(g2v.get_embedding())\n",
        "    embed_time = time.time() - start_embed\n",
        "\n",
        "    X_train = emb_all[:len(train_graphs)]\n",
        "    X_test = emb_all[len(train_graphs):]\n",
        "\n",
        "    print(\"Training final SVM on Graph2Vec embeddings...\")\n",
        "    start_train = time.time()\n",
        "    clf = SVC(kernel=\"rbf\", probability=True, C=best_params[\"C\"], gamma=best_params[\"gamma\"], random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_train\n",
        "\n",
        "    # Evaluation on held-out test graphs\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_prob = clf.predict_proba(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "    try:\n",
        "        if len(np.unique(y_test)) == 2:\n",
        "            auc = roc_auc_score(y_test, y_prob[:, 1])\n",
        "        else:\n",
        "            auc = roc_auc_score(y_test, y_prob, multi_class=\"ovr\")\n",
        "    except ValueError:\n",
        "        auc = np.nan\n",
        "\n",
        "    score = (w_acc * acc) + (w_f1 * f1) + (w_auc * (0 if np.isnan(auc) else auc))\n",
        "\n",
        "\n",
        "    # Save Graph2Vec embeddings (for all graphs) + labels\n",
        "\n",
        "    g2v_exp_dir = os.path.join(EMBEDDINGS_DIR, \"Graph2Vec\", dataset_name, experiment_num, \"baseline\")\n",
        "    os.makedirs(g2v_exp_dir, exist_ok=True)\n",
        "    np.save(os.path.join(g2v_exp_dir, \"embeddings.npy\"), emb_all)\n",
        "    np.save(os.path.join(g2v_exp_dir, \"labels.npy\"), np.array(labels))\n",
        "\n",
        "    # Save fitted Graph2Vec model\n",
        "    g2v_model_path = f\"{MODELS_DIR}/Graph2Vec_{dataset_name}_{experiment_num}.joblib\"\n",
        "    joblib.dump(g2v, g2v_model_path)\n",
        "\n",
        "    # Save trained SVM classifier\n",
        "    svm_model_path = f\"{MODELS_DIR}/Graph2Vec_SVM_{dataset_name}_{experiment_num}.joblib\"\n",
        "    joblib.dump(clf, svm_model_path)\n",
        "\n",
        "\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_usage = process.memory_info().rss / (1024 ** 2)  # in MB\n",
        "\n",
        "    print(f\"Graph2Vec Results on {dataset_name} -> Acc: {acc:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}, Score: {score:.3f}\")\n",
        "    print(f\"Embedding time: {embed_time:.2f}s | SVM training time: {train_time:.2f}s | Optuna time: {opt_time:.2f}s | Memory usage: {memory_usage:.2f} MB\")\n",
        "\n",
        "    # Log summary to CSV\n",
        "    summary_path = f\"{RESULTS_DIR}/graph2vec_log.csv\"\n",
        "\n",
        "    summary_data = {\n",
        "        \"experiment_num\": experiment_num,\n",
        "\n",
        "        \"dataset\": dataset_name,\n",
        "        \"embedding_type\": \"Graph2Vec\",\n",
        "        \"embedding_dimension\": embedding_dim,\n",
        "        \"optuna_enabled\": \"yes\" if use_optuna else \"no\",\n",
        "        \"C\": best_params[\"C\"],\n",
        "        \"gamma\": best_params[\"gamma\"],\n",
        "        \"acc\": round(float(acc), 4),\n",
        "        \"f1\": round(float(f1), 4),\n",
        "        \"auc\": round(float(auc) if not np.isnan(auc) else -1, 4),\n",
        "        \"score\": round(float(score), 4),\n",
        "        \"embedding_time (s)\": round(embed_time, 2),\n",
        "        \"svm_training_time (s)\": round(train_time, 2),\n",
        "        \"optuna_time (s)\": round(opt_time, 2),\n",
        "        \"memory_usage (MB)\": round(memory_usage, 2),\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame([summary_data])\n",
        "    if os.path.exists(summary_path):\n",
        "        df.to_csv(summary_path, mode='a', index=False, header=False)\n",
        "    else:\n",
        "        df.to_csv(summary_path, index=False)\n",
        "\n",
        "    print(f\"Graph2Vec summary stored in: {summary_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCpM6Q5cqOwL"
      },
      "outputs": [],
      "source": [
        "def run_netlsd_pipeline(\n",
        "    dataset_name,\n",
        "    w_acc=0.5,\n",
        "    w_f1=0.3,\n",
        "    w_auc=0.2,\n",
        "    test_size=0.2,\n",
        "    use_optuna=True,\n",
        "    n_trials=20,\n",
        "):\n",
        "    \"\"\"\n",
        "    Pipeline for graph classification using NetLSD embeddings + SVM,\n",
        "    with optional Optuna-based hyperparameter tuning and ENZYMES filtering.\n",
        "    \"\"\"\n",
        "    # Experiment ID\n",
        "    experiment_num = time.strftime(\"%d%m%Y_%H%M\", time.localtime())\n",
        "    # Load dataset\n",
        "    dataset = TUDataset(root='data/TUDataset', name=dataset_name).shuffle()\n",
        "    print(f\"Loaded dataset {dataset_name} for NetLSD: {len(dataset)} graphs, {dataset.num_classes} classes\")\n",
        "\n",
        "    # Convert PyG graphs to NetworkX graphs\n",
        "    graphs = []\n",
        "    labels = []\n",
        "    for data in dataset:\n",
        "        g = to_networkx(data, to_undirected=True)\n",
        "        graphs.append(g)\n",
        "        labels.append(int(data.y.item()))\n",
        "\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Special handling for ENZYMES\n",
        "    if dataset_name.upper() == \"ENZYMES\":\n",
        "        graphs, labels = filter_enzymes_graphs(graphs, labels, min_nodes=3)\n",
        "\n",
        "    # Outer train/test split on graphs\n",
        "    train_graphs, test_graphs, y_train, y_test = train_test_split(\n",
        "        graphs,\n",
        "        labels,\n",
        "        test_size=test_size,\n",
        "        random_state=42,\n",
        "        stratify=labels,\n",
        "    )\n",
        "\n",
        "    opt_time = 0.0\n",
        "\n",
        "    def objective(trial):\n",
        "        # Hyperparameters for the SVM classifier\n",
        "        C = trial.suggest_loguniform(\"C\", 1e-2, 1e2)\n",
        "        gamma = trial.suggest_loguniform(\"gamma\", 1e-4, 1e1)\n",
        "\n",
        "        # Inner train/validation split on graphs\n",
        "        inner_tr_graphs, inner_val_graphs, y_tr, y_val = train_test_split(\n",
        "            train_graphs,\n",
        "            y_train,\n",
        "            test_size=0.2,\n",
        "            random_state=42,\n",
        "            stratify=y_train,\n",
        "        )\n",
        "\n",
        "        all_graphs = inner_tr_graphs + inner_val_graphs\n",
        "\n",
        "        # Fit NetLSD on all and slice embeddings\n",
        "        netlsd = NetLSD()\n",
        "        netlsd.fit(all_graphs)\n",
        "        emb_all = sanitize_embeddings(netlsd.get_embedding())\n",
        "\n",
        "        X_tr = emb_all[:len(inner_tr_graphs)]\n",
        "        X_val = emb_all[len(inner_tr_graphs):]\n",
        "\n",
        "        clf = SVC(kernel=\"rbf\", probability=True, C=C, gamma=gamma, random_state=42)\n",
        "        clf.fit(X_tr, y_tr)\n",
        "        y_pred = clf.predict(X_val)\n",
        "        y_prob = clf.predict_proba(X_val)\n",
        "\n",
        "        acc = accuracy_score(y_val, y_pred)\n",
        "        f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
        "        try:\n",
        "            if len(np.unique(y_val)) == 2:\n",
        "                auc = roc_auc_score(y_val, y_prob[:, 1])\n",
        "            else:\n",
        "                auc = roc_auc_score(y_val, y_prob, multi_class=\"ovr\")\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "\n",
        "        score = (w_acc * acc) + (w_f1 * f1) + (w_auc * (0 if np.isnan(auc) else auc))\n",
        "        return score\n",
        "\n",
        "    if use_optuna:\n",
        "        print(\"Running Optuna for NetLSD+SVM hyperparameter tuning...\")\n",
        "        start_opt = time.time()\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=n_trials)\n",
        "        best_params = study.best_params\n",
        "        opt_time = time.time() - start_opt\n",
        "        print(f\"Best hyperparameters (NetLSD+SVM): {best_params}\")\n",
        "    else:\n",
        "        best_params = {\"C\": 1.0, \"gamma\": \"scale\"}\n",
        "        print(f\"Using default SVM hyperparameters: {best_params}\")\n",
        "\n",
        "    # Final embedding + training using best hyperparameters\n",
        "    print(\"Running final NetLSD embedding on train+test graphs...\")\n",
        "    all_graphs_final = train_graphs + test_graphs\n",
        "    start_embed = time.time()\n",
        "    netlsd = NetLSD()\n",
        "    netlsd.fit(all_graphs_final)\n",
        "    emb_all = sanitize_embeddings(netlsd.get_embedding())\n",
        "    embed_time = time.time() - start_embed\n",
        "\n",
        "    X_train = emb_all[:len(train_graphs)]\n",
        "    X_test = emb_all[len(train_graphs):]\n",
        "\n",
        "    print(\"Training final SVM on NetLSD embeddings...\")\n",
        "    start_train = time.time()\n",
        "    clf = SVC(kernel=\"rbf\", probability=True, C=best_params[\"C\"], gamma=best_params[\"gamma\"], random_state=42)\n",
        "    clf.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_train\n",
        "\n",
        "    # Evaluation on held-out test graphs\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_prob = clf.predict_proba(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "    try:\n",
        "        if len(np.unique(y_test)) == 2:\n",
        "            auc = roc_auc_score(y_test, y_prob[:, 1])\n",
        "        else:\n",
        "            auc = roc_auc_score(y_test, y_prob, multi_class=\"ovr\")\n",
        "    except ValueError:\n",
        "        auc = np.nan\n",
        "\n",
        "    score = (w_acc * acc) + (w_f1 * f1) + (w_auc * (0 if np.isnan(auc) else auc))\n",
        "\n",
        "\n",
        "    # Save NetLSD embeddings (all graphs) + labels\n",
        "\n",
        "    netlsd_exp_dir = os.path.join(EMBEDDINGS_DIR, \"NetLSD\",dataset_name, experiment_num, \"baseline\")\n",
        "    os.makedirs(netlsd_exp_dir, exist_ok=True)\n",
        "    np.save(os.path.join(netlsd_exp_dir, \"embeddings.npy\"), emb_all)\n",
        "    np.save(os.path.join(netlsd_exp_dir, \"labels.npy\"), np.array(labels))\n",
        "\n",
        "    # Save fitted NetLSD model\n",
        "    netlsd_model_path = f\"{MODELS_DIR}/NetLSD_{dataset_name}_{experiment_num}.joblib\"\n",
        "    joblib.dump(netlsd, netlsd_model_path)\n",
        "\n",
        "    # Save trained SVM classifier\n",
        "    svm_model_path = f\"{MODELS_DIR}/NetLSD_SVM_{dataset_name}_{experiment_num}.joblib\"\n",
        "    joblib.dump(clf, svm_model_path)\n",
        "\n",
        "\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_usage = process.memory_info().rss / (1024 ** 2)  # in MB\n",
        "\n",
        "    print(f\"NetLSD Results on {dataset_name} -> Acc: {acc:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}, Score: {score:.3f}\")\n",
        "    print(f\"Embedding time: {embed_time:.2f}s | SVM training time: {train_time:.2f}s | Optuna time: {opt_time:.2f}s | Memory usage: {memory_usage:.2f} MB\")\n",
        "\n",
        "    # Log summary to CSV\n",
        "    summary_path = f\"{RESULTS_DIR}/netlsd_log.csv\"\n",
        "\n",
        "    summary_data = {\n",
        "        \"experiment_num\": experiment_num,\n",
        "        \"dataset\": dataset_name,\n",
        "        \"embedding_type\": \"NetLSD\",\n",
        "        \"optuna_enabled\": \"yes\" if use_optuna else \"no\",\n",
        "        \"C\": best_params[\"C\"],\n",
        "        \"gamma\": best_params[\"gamma\"],\n",
        "        \"acc\": round(float(acc), 4),\n",
        "        \"f1\": round(float(f1), 4),\n",
        "        \"auc\": round(float(auc) if not np.isnan(auc) else -1, 4),\n",
        "        \"score\": round(float(score), 4),\n",
        "        \"embedding_time (s)\": round(embed_time, 2),\n",
        "        \"svm_training_time (s)\": round(train_time, 2),\n",
        "        \"optuna_time (s)\": round(opt_time, 2),\n",
        "        \"memory_usage (MB)\": round(memory_usage, 2),\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame([summary_data])\n",
        "    if os.path.exists(summary_path):\n",
        "        df.to_csv(summary_path, mode='a', index=False, header=False)\n",
        "    else:\n",
        "        df.to_csv(summary_path, index=False)\n",
        "\n",
        "    print(f\"NetLSD summary stored in: {summary_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5dGElCoqdAr",
        "outputId": "a29c5a39-da62-416f-8a43-0cdf6f454833"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/IMDB-MULTI.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset has no node features. Applying OneHotDegree transform...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-21 17:15:28,667] A new study created in memory with name: no-name-551eb7ba-ea0c-47c9-81df-43d97cb168ee\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset IMDB-MULTI: 1500 graphs, 89 node features, 3 classes\n",
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-21 17:15:31,020] Trial 0 finished with value: 0.4896705728336562 and parameters: {'num_layers': 3, 'dropout': 0.33024362611278424, 'lr': 0.007561096109374824, 'weight_decay': 0.00024619810519407225}. Best is trial 0 with value: 0.4896705728336562.\n",
            "[I 2026-01-21 17:15:34,576] Trial 1 finished with value: 0.42317957778728843 and parameters: {'num_layers': 6, 'dropout': 0.41023004762139387, 'lr': 0.00751813417784414, 'weight_decay': 3.36407680124545e-06}. Best is trial 0 with value: 0.4896705728336562.\n",
            "[I 2026-01-21 17:15:39,377] Trial 2 finished with value: 0.401749112142021 and parameters: {'num_layers': 6, 'dropout': 0.014760444066226496, 'lr': 0.0005801657852015326, 'weight_decay': 2.328667592373646e-06}. Best is trial 0 with value: 0.4896705728336562.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'num_layers': 3, 'dropout': 0.33024362611278424, 'lr': 0.007561096109374824, 'weight_decay': 0.00024619810519407225}\n",
            "\n",
            "Running final training GIN...\n",
            "{'num_layers': 3, 'dropout': 0.33024362611278424, 'lr': 0.007561096109374824, 'weight_decay': 0.00024619810519407225}\n",
            "Epoch 001 | Loss=2.1970 | TestAcc=0.397 | F1=0.333 | AUC=0.635 | Time=0.34s\n",
            "Epoch 002 | Loss=1.3248 | TestAcc=0.467 | F1=0.385 | AUC=0.640 | Time=0.68s\n",
            "Epoch 003 | Loss=1.1306 | TestAcc=0.437 | F1=0.416 | AUC=0.648 | Time=1.04s\n",
            "Epoch 004 | Loss=1.1206 | TestAcc=0.450 | F1=0.372 | AUC=0.689 | Time=1.37s\n",
            "Epoch 005 | Loss=1.0737 | TestAcc=0.487 | F1=0.471 | AUC=0.692 | Time=1.71s\n",
            "Epoch 006 | Loss=1.0650 | TestAcc=0.453 | F1=0.401 | AUC=0.666 | Time=2.06s\n",
            "Epoch 007 | Loss=1.0401 | TestAcc=0.470 | F1=0.414 | AUC=0.694 | Time=2.39s\n",
            "Epoch 008 | Loss=1.0385 | TestAcc=0.457 | F1=0.406 | AUC=0.675 | Time=2.73s\n",
            "Epoch 009 | Loss=1.0145 | TestAcc=0.457 | F1=0.417 | AUC=0.679 | Time=3.08s\n",
            "Epoch 010 | Loss=1.0265 | TestAcc=0.483 | F1=0.478 | AUC=0.675 | Time=3.41s\n",
            "\n",
            "Training summary stored in : /content/drive/MyDrive/InformationSystems/Classification/results/training_log.csv\n",
            "  experiment_num     dataset optimization_enabled  embedding_dimension  \\\n",
            "0  21012026_1715  IMDB-MULTI                  yes                   64   \n",
            "\n",
            "  objective_weights  num_layers   dropout        lr  weight_decay  epochs  \\\n",
            "0     (0.5,0.3,0.2)           3  0.330244  0.007561      0.000246      10   \n",
            "\n",
            "   best_epoch  best_loss  eval_loss  eval_acc  eval_f1  eval_auc  \\\n",
            "0           5     1.0082     1.0082    0.4867   0.4713    0.6919   \n",
            "\n",
            "   training_time (s)  generation_time (s)  memory_usage (MB)  \n",
            "0               3.41                10.71            1467.04  \n",
            "Saved model: /content/drive/MyDrive/InformationSystems/Classification/models/GIN_IMDB-MULTI_21012026_1715.pth\n"
          ]
        }
      ],
      "source": [
        "run_gin_pipeline(\n",
        "    dataset_name=\"IMDB-MULTI\",\n",
        "    use_optuna=True,\n",
        "    w_acc=0.5,\n",
        "    w_f1=0.3,\n",
        "    w_auc=0.2,\n",
        "    hidden_dim=64,\n",
        "    epochs=10,\n",
        "    batch_size=32,\n",
        "    n_trials=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUWkRRajqf-k",
        "outputId": "4411b0e0-2049-4a2c-a14b-f2772f2a16ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset IMDB-MULTI for Graph2Vec: 1500 graphs, 3 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-21 17:15:51,803] A new study created in memory with name: no-name-9f8bf705-e878-47d8-a126-6f33c18d2784\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Optuna for Graph2Vec+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-21 17:15:53,548] Trial 0 finished with value: 0.39769086104269297 and parameters: {'C': 0.5807437287748115, 'gamma': 0.9307045678121256}. Best is trial 0 with value: 0.39769086104269297.\n",
            "[I 2026-01-21 17:15:54,900] Trial 1 finished with value: 0.34453202736318406 and parameters: {'C': 1.2646221781582017, 'gamma': 0.004222426502423212}. Best is trial 0 with value: 0.39769086104269297.\n",
            "[I 2026-01-21 17:15:56,251] Trial 2 finished with value: 0.49879246354606926 and parameters: {'C': 2.531775871559638, 'gamma': 7.007815458608908}. Best is trial 2 with value: 0.49879246354606926.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (Graph2Vec+SVM): {'C': 2.531775871559638, 'gamma': 7.007815458608908}\n",
            "Running final Graph2Vec embedding on train+test graphs...\n",
            "Training final SVM on Graph2Vec embeddings...\n",
            "Graph2Vec Results on IMDB-MULTI -> Acc: 0.400, F1: 0.398, AUC: 0.579, Score: 0.435\n",
            "Embedding time: 0.94s | SVM training time: 0.62s | Optuna time: 4.45s | Memory usage: 1473.72 MB\n",
            "Graph2Vec summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/graph2vec_log.csv\n"
          ]
        }
      ],
      "source": [
        "run_graph2vec_pipeline(\n",
        "    dataset_name=\"IMDB-MULTI\",\n",
        "    w_acc=0.5, w_f1=0.3, w_auc=0.2,\n",
        "    embedding_dim=128,\n",
        "    epochs=10,\n",
        "    test_size=0.2,\n",
        "    use_optuna=True,\n",
        "    n_trials=3,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6az0J-SNqi11",
        "outputId": "da664c67-d024-4b5b-c665-f3501b770add"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset IMDB-MULTI for NetLSD: 1500 graphs, 3 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-21 17:16:03,866] A new study created in memory with name: no-name-2d428ce8-48c9-48eb-982b-587351d0b5c6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Optuna for NetLSD+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-21 17:16:08,104] Trial 0 finished with value: 0.4482480352813431 and parameters: {'C': 1.2522186962934112, 'gamma': 0.00027665690953069405}. Best is trial 0 with value: 0.4482480352813431.\n",
            "[I 2026-01-21 17:16:12,781] Trial 1 finished with value: 0.41900049559638686 and parameters: {'C': 0.03845138598342104, 'gamma': 2.114970270324145}. Best is trial 0 with value: 0.4482480352813431.\n",
            "[I 2026-01-21 17:16:17,745] Trial 2 finished with value: 0.5241373984421526 and parameters: {'C': 44.69023343763088, 'gamma': 0.05715357526654174}. Best is trial 2 with value: 0.5241373984421526.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (NetLSD+SVM): {'C': 44.69023343763088, 'gamma': 0.05715357526654174}\n",
            "Running final NetLSD embedding on train+test graphs...\n",
            "Training final SVM on NetLSD embeddings...\n",
            "NetLSD Results on IMDB-MULTI -> Acc: 0.427, F1: 0.392, AUC: 0.583, Score: 0.447\n",
            "Embedding time: 4.80s | SVM training time: 1.24s | Optuna time: 13.88s | Memory usage: 1493.59 MB\n",
            "NetLSD summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/netlsd_log.csv\n"
          ]
        }
      ],
      "source": [
        "run_netlsd_pipeline(\n",
        "    dataset_name=\"IMDB-MULTI\",\n",
        "    w_acc=0.5, w_f1=0.3, w_auc=0.2,\n",
        "    test_size=0.2,\n",
        "    use_optuna=True,\n",
        "    n_trials=3,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}