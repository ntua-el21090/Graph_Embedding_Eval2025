{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FzyRjdu0fpw",
        "outputId": "1c29c5b7-7a21-40c1-8209-e202848da778",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_scatter-2.1.2%2Bpt22cu121-cp312-cp312-linux_x86_64.whl (10.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.1.2+pt22cu121\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-2.2.0%2Bcu121/torch_sparse-0.6.18%2Bpt22cu121-cp312-cp312-linux_x86_64.whl (5.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from torch-sparse) (1.16.3)\n",
            "Requirement already satisfied: numpy<2.6,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from scipy->torch-sparse) (2.0.2)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.18+pt22cu121\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.7.0-py3-none-any.whl.metadata (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2026.1.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Downloading torch_geometric-2.7.0-py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.7.0\n",
            "Collecting karateclub\n",
            "  Downloading karateclub-1.3.3.tar.gz (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (3.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: decorator==4.4.2 in /usr/local/lib/python3.12/dist-packages (from karateclub) (4.4.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from karateclub) (4.67.1)\n",
            "Requirement already satisfied: python-louvain in /usr/local/lib/python3.12/dist-packages (from karateclub) (0.16)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from karateclub) (1.16.3)\n",
            "Collecting pygsp (from karateclub)\n",
            "  Downloading pygsp-0.6.1-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gensim>=4.0.0 (from karateclub)\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from karateclub) (1.17.0)\n",
            "Collecting python-Levenshtein (from karateclub)\n",
            "  Downloading python_levenshtein-0.27.3-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.3.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim>=4.0.0->karateclub) (7.5.0)\n",
            "Collecting Levenshtein==0.27.3 (from python-Levenshtein->karateclub)\n",
            "  Downloading levenshtein-0.27.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.3/153.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim>=4.0.0->karateclub) (2.0.1)\n",
            "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.3->python-Levenshtein->karateclub)\n",
            "  Downloading rapidfuzz-3.14.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (3.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m125.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: karateclub\n",
            "  Building wheel for karateclub (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for karateclub: filename=karateclub-1.3.3-py3-none-any.whl size=101979 sha256=ff1bf582ff973e290a2e3a31040e9392c2e83933393aef4ef9b7dd5423158ba9\n",
            "  Stored in directory: /root/.cache/pip/wheels/8c/59/5b/cec587a448c281393eeed3604826bc3e3460970d69b23f7fe4\n",
            "Successfully built karateclub\n",
            "Installing collected packages: pygsp, gensim, rapidfuzz, Levenshtein, python-Levenshtein, karateclub\n",
            "\u001b[31mERROR: pip's legacy dependency resolver does not consider dependency conflicts when selecting packages. This behaviour is the source of the following dependency conflicts.\n",
            "karateclub 1.3.3 requires networkx<2.7, but you'll have networkx 3.6.1 which is incompatible.\n",
            "karateclub 1.3.3 requires numpy<1.23.0, but you'll have numpy 2.0.2 which is incompatible.\n",
            "karateclub 1.3.3 requires pandas<=1.3.5, but you'll have pandas 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Levenshtein-0.27.3 gensim-4.4.0 karateclub-1.3.3 pygsp-0.6.1 python-Levenshtein-0.27.3 rapidfuzz-3.14.3\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Looking in links: https://data.pyg.org/whl/torch-2.9.0+cu126.html\n",
            "Requirement already satisfied: torch-geometric in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.13.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2025.3.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.1.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.0.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (5.9.5)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.3.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (2.32.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from torch-geometric) (3.6.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp->torch-geometric) (1.22.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch-geometric) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->torch-geometric) (2026.1.4)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp->torch-geometric) (4.15.0)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.7.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.18.1)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.45)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.7.0-py3-none-any.whl (413 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m413.9/413.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.7.0\n",
            "Requirement already satisfied: karateclub in /usr/local/lib/python3.12/dist-packages (1.3.3)\n",
            "Collecting numpy<1.23.0 (from karateclub)\n",
            "  Downloading numpy-1.22.4.zip (11.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m142.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ],
      "source": [
        "!pip install torch-scatter -f https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
        "!pip install torch-sparse  -f https://data.pyg.org/whl/torch-2.2.0+cu121.html\n",
        "!pip install torch-geometric\n",
        "\n",
        "!pip install --use-deprecated=legacy-resolver karateclub networkx numpy pandas matplotlib scikit-learn\n",
        "\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install torch-geometric \\\n",
        "    -f https://data.pyg.org/whl/torch-$(python -c \"import torch; print(torch.__version__)\").html\n",
        "\n",
        "\n",
        "!pip install optuna\n",
        "!pip install karateclub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "luQxesBgfy04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0ed2bf8-2b82-48cb-90bf-ea1b037a5bd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_scatter/_version_cuda.so\n",
            "  import torch_geometric.typing\n",
            "/usr/local/lib/python3.12/dist-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: Could not load this library: /usr/local/lib/python3.12/dist-packages/torch_sparse/_version_cuda.so\n",
            "  import torch_geometric.typing\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import joblib\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear, Sequential, ReLU, BatchNorm1d\n",
        "from torch_geometric.nn import GINConv, global_add_pool\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from torch_geometric.transforms import OneHotDegree\n",
        "import optuna\n",
        "import pandas as pd\n",
        "import time, os, psutil\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "from karateclub import NetLSD, Graph2Vec\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from torch_geometric.utils import to_networkx\n",
        "import networkx as nx\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-KqwkGwOm_KI",
        "outputId": "329fe2a0-7d11-42b3-a038-05952a05aec3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Connect to google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "BASE_DIR = \"/content/drive/MyDrive/InformationSystems/Classification\"\n",
        "RESULTS_DIR = f\"{BASE_DIR}/results\"\n",
        "MODELS_DIR = f\"{BASE_DIR}/models\"\n",
        "EMBEDDINGS_DIR = f\"{BASE_DIR}/embeddings\"\n",
        "CLASSIF_RESULTS_DIR = f\"{BASE_DIR}/results/classification\"\n",
        "\n",
        "os.makedirs(EMBEDDINGS_DIR, exist_ok=True)\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "os.makedirs(MODELS_DIR, exist_ok=True)\n",
        "os.makedirs(CLASSIF_RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "k5NN7KKnpufI"
      },
      "outputs": [],
      "source": [
        "def sanitize_embeddings(embeddings: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Replace NaN/Inf values in embeddings and ensure a clean float32 array.\n",
        "    This is useful for karateclub embeddings that may occasionally produce\n",
        "    unstable values on some graphs.\n",
        "    \"\"\"\n",
        "    emb = np.asarray(embeddings, dtype=np.float32)\n",
        "    # Replace NaN and +/- Inf with 0.0\n",
        "    emb = np.nan_to_num(emb, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "    return emb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "BmVVRkY8pwlP"
      },
      "outputs": [],
      "source": [
        "def filter_enzymes_graphs(graphs, labels, min_nodes: int = 3):\n",
        "    \"\"\"\n",
        "    Special handling for ENZYMES: remove very small graphs that can cause\n",
        "    numerical issues for NetLSD / Graph2Vec.\n",
        "    Returns filtered (graphs, labels) and prints how many were removed.\n",
        "    \"\"\"\n",
        "    if len(graphs) == 0:\n",
        "        return graphs, labels\n",
        "\n",
        "    mask = [g.number_of_nodes() >= min_nodes for g in graphs]\n",
        "    if not any(mask):\n",
        "        print(\"WARNING: All ENZYMES graphs would be filtered out. Skipping filtering.\")\n",
        "        return graphs, labels\n",
        "\n",
        "    filtered_graphs = [g for g, keep in zip(graphs, mask) if keep]\n",
        "    if isinstance(labels, np.ndarray):\n",
        "       filtered_labels = labels[np.array(mask)]\n",
        "    else:\n",
        "        filtered_labels = [y for y, keep in zip(labels, mask) if keep]\n",
        "\n",
        "    removed = len(graphs) - len(filtered_graphs)\n",
        "    print(f\"ENZYMES filtering: removed {removed} graphs with < {min_nodes} nodes, kept {len(filtered_graphs)} graphs.\")\n",
        "    return filtered_graphs, filtered_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "boI0A_b4nfci"
      },
      "outputs": [],
      "source": [
        "# GIN Model Definition\n",
        "\n",
        "class GIN(torch.nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, num_classes, num_layers=5, dropout=0.5):\n",
        "        super(GIN, self).__init__()\n",
        "        layers = []\n",
        "        in_dim = num_features\n",
        "        for _ in range(num_layers):\n",
        "            nn = Sequential(Linear(in_dim, hidden_dim), ReLU(), Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(GINConv(nn))\n",
        "            in_dim = hidden_dim\n",
        "        self.convs = torch.nn.ModuleList(layers)\n",
        "        self.bns = torch.nn.ModuleList([BatchNorm1d(hidden_dim) for _ in range(num_layers)])\n",
        "        self.fc1 = Linear(hidden_dim, hidden_dim)\n",
        "        self.fc2 = Linear(hidden_dim, num_classes)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        for conv, bn in zip(self.convs, self.bns):\n",
        "            x = F.relu(conv(x, edge_index))\n",
        "            x = bn(x)\n",
        "        x = global_add_pool(x, batch)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tHDMvRdhnjQm"
      },
      "outputs": [],
      "source": [
        "# Training\n",
        "\n",
        "def train(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for data in loader:\n",
        "        data = data.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(data.x, data.edge_index, data.batch)\n",
        "        loss = criterion(out, data.y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# Evaluation\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    preds, labels, probs = [], [], []\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(DEVICE)\n",
        "            out = model(data.x, data.edge_index, data.batch)\n",
        "\n",
        "            loss = criterion(out, data.y)\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            pred = out.argmax(dim=1)\n",
        "            preds.extend(pred.cpu().numpy())\n",
        "            labels.extend(data.y.cpu().numpy())\n",
        "            probs.extend(F.softmax(out, dim=1).cpu().numpy())  # probabilities for AUC\n",
        "\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average='weighted')\n",
        "    avg_loss = total_loss / max(1, num_batches)\n",
        "\n",
        "    try:\n",
        "        if len(np.unique(labels)) == 2:\n",
        "            auc = roc_auc_score(labels, np.array(probs)[:, 1])\n",
        "        else:\n",
        "            auc = roc_auc_score(labels, probs, multi_class='ovr')\n",
        "    except ValueError:\n",
        "        auc = np.nan  # if there are not enough samples for AUC\n",
        "\n",
        "    return acc, f1, auc, avg_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NVzRwAdC4P2n"
      },
      "outputs": [],
      "source": [
        "def get_gin_embeddings(model, loader):\n",
        "    \"\"\"Return graph-level embeddings (after global_add_pool) and labels.\"\"\"\n",
        "    model.eval()\n",
        "    all_emb = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            data = data.to(DEVICE)\n",
        "            x, edge_index, batch = data.x, data.edge_index, data.batch\n",
        "            # forward μέχρι το pooling\n",
        "            for conv, bn in zip(model.convs, model.bns):\n",
        "                x = F.relu(conv(x, edge_index))\n",
        "                x = bn(x)\n",
        "            x = global_add_pool(x, batch)\n",
        "            all_emb.append(x.cpu().numpy())\n",
        "            all_labels.extend(data.y.cpu().numpy())\n",
        "    embeddings = np.concatenate(all_emb, axis=0)\n",
        "    labels = np.array(all_labels)\n",
        "    return embeddings, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "N7nB1kWECeeA"
      },
      "outputs": [],
      "source": [
        "def run_gin_pipeline(\n",
        "    dataset_name,\n",
        "    seed,\n",
        "    use_optuna,\n",
        "    w_acc,\n",
        "    w_f1,\n",
        "    w_auc,\n",
        "    hidden_dim,\n",
        "    epochs,\n",
        "    batch_size=32,\n",
        "    n_trials=15,\n",
        "):\n",
        "\n",
        "    # Experiment ID (used in logs and embeddings path)\n",
        "    experiment_id = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    # Load dataset\n",
        "    dataset = TUDataset(root='data/TUDataset', name=dataset_name).shuffle()\n",
        "\n",
        "    # If no node features use one-hot degree features\n",
        "    if dataset.num_features == 0 or dataset[0].x is None:\n",
        "        print(\"Dataset has no node features. Applying OneHotDegree transform...\")\n",
        "\n",
        "        # Find maximum degree across all graphs\n",
        "        max_degree = 0\n",
        "        for data in dataset:\n",
        "            deg = torch.bincount(data.edge_index[0], minlength=data.num_nodes)\n",
        "            max_degree = max(max_degree, int(deg.max()))\n",
        "\n",
        "        # Apply transform\n",
        "        oh_transform = OneHotDegree(max_degree=max_degree)\n",
        "        dataset = TUDataset(\n",
        "            root='data/TUDataset',\n",
        "            name=dataset_name,\n",
        "            transform=oh_transform\n",
        "        ).shuffle()\n",
        "\n",
        "        num_node_features = max_degree + 1\n",
        "    else:\n",
        "        num_node_features = dataset.num_features\n",
        "\n",
        "    # Train/test split\n",
        "    train_dataset = dataset[:int(0.8 * len(dataset))]\n",
        "    test_dataset = dataset[int(0.8 * len(dataset)):]\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
        "\n",
        "    print(f\"Loaded dataset {dataset_name}: {len(dataset)} graphs, {num_node_features} node features, {dataset.num_classes} classes\")\n",
        "\n",
        "    def objective(trial):\n",
        "        num_layers = trial.suggest_int(\"num_layers\", 3, 6)\n",
        "        dropout = trial.suggest_float(\"dropout\", 0.0, 0.6)\n",
        "        lr = trial.suggest_loguniform(\"lr\", 1e-4, 1e-2)\n",
        "        weight_decay = trial.suggest_loguniform(\"weight_decay\", 1e-6, 1e-3)\n",
        "\n",
        "        model = GIN(num_node_features, hidden_dim, dataset.num_classes, num_layers, dropout).to(DEVICE)\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "        for epoch in range(5):  # fewer epochs for fast tuning\n",
        "            train(model, train_loader, optimizer, criterion)\n",
        "\n",
        "        acc, f1, auc, _ = evaluate(model, test_loader, criterion)\n",
        "        score = (w_acc * acc) + (w_f1 * f1) + (w_auc * (0 if np.isnan(auc) else auc))\n",
        "        return score\n",
        "\n",
        "    start_generation = time.time()\n",
        "    if use_optuna:\n",
        "        print(\"Running Optuna for hyperparameter tuning...\")\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=n_trials)\n",
        "        best_params = study.best_params\n",
        "        print(f\"Best hyperparameters: {best_params}\")\n",
        "    else:\n",
        "      best_params = { \"num_layers\": 5, \"dropout\": 0.5, \"lr\": 0.001, \"weight_decay\": 1e-4}\n",
        "      print(f\"Using default hyperparameters: {best_params}\")\n",
        "\n",
        "    generation_time = time.time() - start_generation\n",
        "\n",
        "    # Final Training with best parameters\n",
        "\n",
        "    print(\"\\nRunning final training GIN...\")\n",
        "    print(best_params)\n",
        "\n",
        "    model = GIN(num_node_features, hidden_dim, dataset.num_classes,\n",
        "                num_layers=best_params[\"num_layers\"], dropout=best_params[\"dropout\"]).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=best_params[\"lr\"], weight_decay=best_params[\"weight_decay\"])\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    history = []\n",
        "    start_time = time.time()\n",
        "    eval_acc, eval_f1, eval_auc, eval_loss, eval_epoch = 0, 0, 0, 1e9, 0\n",
        "    best_loss_for_best_epoch = 1e9\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        loss = train(model, train_loader, optimizer, criterion)\n",
        "        acc, f1, auc, e_loss = evaluate(model, test_loader, criterion)\n",
        "        if acc > eval_acc:\n",
        "          #edo mipos to allakso na einai kai edo sindiasmos me weights poy eixe kai sto optuna\n",
        "          eval_acc, eval_f1, eval_auc, eval_loss, eval_epoch = acc, f1, auc, e_loss, epoch\n",
        "          best_loss_for_best_epoch = e_loss\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"Epoch {epoch:03d} | Loss={loss:.4f} | TestAcc={acc:.3f} | F1={f1:.3f} | AUC={auc:.3f} | Time={elapsed:.2f}s\")\n",
        "        history.append([epoch, loss, acc, f1, auc, elapsed])\n",
        "\n",
        "    training_time = time.time() - start_time\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_usage = process.memory_info().rss / (1024 ** 2)  # in MB\n",
        "\n",
        "\n",
        "    # Save GIN embeddings for the whole dataset\n",
        "\n",
        "    full_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
        "    gin_embeddings, gin_labels = get_gin_embeddings(model, full_loader)\n",
        "\n",
        "    gin_exp_dir = os.path.join(EMBEDDINGS_DIR, \"GIN\", dataset_name, experiment_id)\n",
        "    os.makedirs(gin_exp_dir, exist_ok=True)\n",
        "    np.save(os.path.join(gin_exp_dir, \"embeddings.npy\"), gin_embeddings)\n",
        "    np.save(os.path.join(gin_exp_dir, \"labels.npy\"), gin_labels)\n",
        "\n",
        "    # Log file save\n",
        "\n",
        "    summary_path = f\"{CLASSIF_RESULTS_DIR}/gin.csv\"\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "\n",
        "    summary_data = {\n",
        "        \"method\": \"GIN\",\n",
        "        \"seed\": seed,\n",
        "        \"dataset\": dataset_name,\n",
        "        \"optimization_enabled\": \"yes\" if use_optuna else \"no\",\n",
        "        \"embedding_dimension\": hidden_dim,\n",
        "        \"objective_weights\": f\"({w_acc},{w_f1},{w_auc})\",\n",
        "        \"num_layers\": best_params[\"num_layers\"],\n",
        "        \"dropout\": best_params[\"dropout\"],\n",
        "        \"lr\": best_params[\"lr\"],\n",
        "        \"weight_decay\": best_params[\"weight_decay\"],\n",
        "        \"epochs\": epochs,\n",
        "        \"best_epoch\": eval_epoch,\n",
        "        \"best_loss\": round(float(best_loss_for_best_epoch), 4),\n",
        "        \"eval_loss\": round(float(eval_loss), 4),\n",
        "        \"eval_acc\": round(eval_acc, 4),\n",
        "        \"eval_f1\": round(eval_f1, 4),\n",
        "        \"eval_auc\": round(eval_auc, 4),\n",
        "        \"training_time (s)\": round(training_time, 2),\n",
        "        \"generation_time (s)\": round(generation_time, 2),\n",
        "        \"memory_usage (MB)\": round(memory_usage, 2)\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame([summary_data])\n",
        "\n",
        "    # Append mode (keep all trainings)\n",
        "    if os.path.exists(summary_path):\n",
        "        df.to_csv(summary_path, mode='a', index=False, header=False)\n",
        "    else:\n",
        "        df.to_csv(summary_path, index=False)\n",
        "\n",
        "    print(f\"\\nTraining summary stored in : {summary_path}\")\n",
        "    print(df)\n",
        "\n",
        "    # Save best model (weights + metadata)\n",
        "    gin_ckpt = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"num_node_features\": num_node_features,\n",
        "        \"hidden_dim\": hidden_dim,\n",
        "        \"num_classes\": dataset.num_classes,\n",
        "        \"num_layers\": best_params[\"num_layers\"],\n",
        "        \"dropout\": best_params[\"dropout\"],\n",
        "    }\n",
        "    torch.save(gin_ckpt, f\"{MODELS_DIR}/GIN_{dataset_name}_{experiment_id}.pth\")\n",
        "    print(f\"Saved model: {MODELS_DIR}/GIN_{dataset_name}_{experiment_id}.pth\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "vrpvs2N3qJHQ"
      },
      "outputs": [],
      "source": [
        "def run_graph2vec_pipeline(\n",
        "    dataset_name,\n",
        "    seed,\n",
        "    w_acc=0.5,\n",
        "    w_f1=0.3,\n",
        "    w_auc=0.2,\n",
        "    embedding_dim=128,\n",
        "    epochs=50,\n",
        "    test_size=0.2,\n",
        "    use_optuna=True,\n",
        "    n_trials=20,\n",
        "):\n",
        "    \"\"\"\n",
        "    Pipeline for graph classification using Graph2Vec embeddings + SVM,\n",
        "    with optional Optuna-based hyperparameter tuning and special handling\n",
        "    for ENZYMES + embedding sanitization.\n",
        "    \"\"\"\n",
        "    # Experiment ID\n",
        "    experiment_id = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "    # Load dataset\n",
        "    dataset = TUDataset(root='data/TUDataset', name=dataset_name).shuffle()\n",
        "    print(f\"Loaded dataset {dataset_name} for Graph2Vec: {len(dataset)} graphs, {dataset.num_classes} classes\")\n",
        "\n",
        "    # Convert PyG graphs to NetworkX graphs\n",
        "    graphs = []\n",
        "    labels = []\n",
        "    for data in dataset:\n",
        "        g = to_networkx(data, to_undirected=True)\n",
        "        graphs.append(g)\n",
        "        labels.append(int(data.y.item()))\n",
        "\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Special handling for ENZYMES (filter very small graphs)\n",
        "    if dataset_name.upper() == \"ENZYMES\":\n",
        "        graphs, labels = filter_enzymes_graphs(graphs, labels, min_nodes=3)\n",
        "\n",
        "    # Outer train/test split on graphs\n",
        "    train_graphs, test_graphs, y_train, y_test = train_test_split(\n",
        "        graphs,\n",
        "        labels,\n",
        "        test_size=test_size,\n",
        "        random_state=seed,\n",
        "        stratify=labels,\n",
        "    )\n",
        "\n",
        "    opt_time = 0.0\n",
        "\n",
        "    def objective(trial):\n",
        "        # Hyperparameters for the SVM classifier\n",
        "        C = trial.suggest_loguniform(\"C\", 1e-2, 1e2)\n",
        "        gamma = trial.suggest_loguniform(\"gamma\", 1e-4, 1e1)\n",
        "\n",
        "        # Inner train/validation split on graphs\n",
        "        inner_tr_graphs, inner_val_graphs, y_tr, y_val = train_test_split(\n",
        "            train_graphs,\n",
        "            y_train,\n",
        "            test_size=0.2,\n",
        "            random_state=seed,\n",
        "            stratify=y_train,\n",
        "        )\n",
        "\n",
        "        all_graphs = inner_tr_graphs + inner_val_graphs\n",
        "\n",
        "        # Fit Graph2Vec on all (transductive setting) and slice embeddings\n",
        "        g2v = Graph2Vec(dimensions=embedding_dim, wl_iterations=2, epochs=epochs, workers=os.cpu_count())\n",
        "        g2v.fit(all_graphs)\n",
        "        emb_all = sanitize_embeddings(g2v.get_embedding())\n",
        "\n",
        "        X_tr = emb_all[:len(inner_tr_graphs)]\n",
        "        X_val = emb_all[len(inner_tr_graphs):]\n",
        "\n",
        "        clf = SVC(kernel=\"rbf\", probability=True, C=C, gamma=gamma, random_state=seed)\n",
        "        clf.fit(X_tr, y_tr)\n",
        "        y_pred = clf.predict(X_val)\n",
        "        y_prob = clf.predict_proba(X_val)\n",
        "\n",
        "        acc = accuracy_score(y_val, y_pred)\n",
        "        f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
        "        try:\n",
        "            if len(np.unique(y_val)) == 2:\n",
        "                auc = roc_auc_score(y_val, y_prob[:, 1])\n",
        "            else:\n",
        "                auc = roc_auc_score(y_val, y_prob, multi_class=\"ovr\")\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "\n",
        "        score = (w_acc * acc) + (w_f1 * f1) + (w_auc * (0 if np.isnan(auc) else auc))\n",
        "        return score\n",
        "\n",
        "    if use_optuna:\n",
        "        print(\"Running Optuna for Graph2Vec+SVM hyperparameter tuning...\")\n",
        "        start_opt = time.time()\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=n_trials)\n",
        "        best_params = study.best_params\n",
        "        opt_time = time.time() - start_opt\n",
        "        print(f\"Best hyperparameters (Graph2Vec+SVM): {best_params}\")\n",
        "    else:\n",
        "        best_params = {\"C\": 1.0, \"gamma\": \"scale\"}\n",
        "        print(f\"Using default SVM hyperparameters: {best_params}\")\n",
        "\n",
        "    # Final embedding + training using best hyperparameters\n",
        "    print(\"Running final Graph2Vec embedding on train+test graphs...\")\n",
        "    all_graphs_final = train_graphs + test_graphs\n",
        "    start_embed = time.time()\n",
        "    g2v = Graph2Vec(dimensions=embedding_dim, wl_iterations=2, epochs=epochs, workers=os.cpu_count())\n",
        "    g2v.fit(all_graphs_final)\n",
        "    emb_all = sanitize_embeddings(g2v.get_embedding())\n",
        "    embed_time = time.time() - start_embed\n",
        "\n",
        "    X_train = emb_all[:len(train_graphs)]\n",
        "    X_test = emb_all[len(train_graphs):]\n",
        "\n",
        "    print(\"Training final SVM on Graph2Vec embeddings...\")\n",
        "    start_train = time.time()\n",
        "    clf = SVC(kernel=\"rbf\", probability=True, C=best_params[\"C\"], gamma=best_params[\"gamma\"], random_state=seed)\n",
        "    clf.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_train\n",
        "\n",
        "    # Evaluation on held-out test graphs\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_prob = clf.predict_proba(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "    try:\n",
        "        if len(np.unique(y_test)) == 2:\n",
        "            auc = roc_auc_score(y_test, y_prob[:, 1])\n",
        "        else:\n",
        "            auc = roc_auc_score(y_test, y_prob, multi_class=\"ovr\")\n",
        "    except ValueError:\n",
        "        auc = np.nan\n",
        "\n",
        "    score = (w_acc * acc) + (w_f1 * f1) + (w_auc * (0 if np.isnan(auc) else auc))\n",
        "\n",
        "\n",
        "    # Save Graph2Vec embeddings (for all graphs) + labels\n",
        "\n",
        "    g2v_exp_dir = os.path.join(EMBEDDINGS_DIR, \"Graph2Vec\", dataset_name, experiment_id)\n",
        "    os.makedirs(g2v_exp_dir, exist_ok=True)\n",
        "    np.save(os.path.join(g2v_exp_dir, \"embeddings.npy\"), emb_all)\n",
        "    np.save(os.path.join(g2v_exp_dir, \"labels.npy\"), np.array(labels))\n",
        "\n",
        "    # Save fitted Graph2Vec model\n",
        "    g2v_model_path = f\"{MODELS_DIR}/Graph2Vec_{dataset_name}_{experiment_id}.joblib\"\n",
        "    joblib.dump(g2v, g2v_model_path)\n",
        "\n",
        "    # Save trained SVM classifier\n",
        "    svm_model_path = f\"{MODELS_DIR}/Graph2Vec_SVM_{dataset_name}_{experiment_id}.joblib\"\n",
        "    joblib.dump(clf, svm_model_path)\n",
        "\n",
        "\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_usage = process.memory_info().rss / (1024 ** 2)  # in MB\n",
        "\n",
        "    print(f\"Graph2Vec Results on {dataset_name} -> Acc: {acc:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}, Score: {score:.3f}\")\n",
        "    print(f\"Embedding time: {embed_time:.2f}s | SVM training time: {train_time:.2f}s | Optuna time: {opt_time:.2f}s | Memory usage: {memory_usage:.2f} MB\")\n",
        "\n",
        "    # Log summary to CSV\n",
        "    summary_path = f\"{CLASSIF_RESULTS_DIR}/g2v.csv\"\n",
        "\n",
        "    summary_data = {\n",
        "        \"method\": \"Graph2Vec\",\n",
        "        \"seed\": seed,\n",
        "        \"dataset\": dataset_name,\n",
        "        \"embedding_dimension\": embedding_dim,\n",
        "        \"optuna_enabled\": \"yes\" if use_optuna else \"no\",\n",
        "        \"C\": best_params[\"C\"],\n",
        "        \"gamma\": best_params[\"gamma\"],\n",
        "        \"acc\": round(float(acc), 4),\n",
        "        \"f1\": round(float(f1), 4),\n",
        "        \"auc\": round(float(auc) if not np.isnan(auc) else -1, 4),\n",
        "        \"score\": round(float(score), 4),\n",
        "        \"embedding_time (s)\": round(embed_time, 2),\n",
        "        \"svm_training_time (s)\": round(train_time, 2),\n",
        "        \"optuna_time (s)\": round(opt_time, 2),\n",
        "        \"memory_usage (MB)\": round(memory_usage, 2),\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame([summary_data])\n",
        "    if os.path.exists(summary_path):\n",
        "        df.to_csv(summary_path, mode='a', index=False, header=False)\n",
        "    else:\n",
        "        df.to_csv(summary_path, index=False)\n",
        "\n",
        "    print(f\"Graph2Vec summary stored in: {summary_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GCpM6Q5cqOwL"
      },
      "outputs": [],
      "source": [
        "def run_netlsd_pipeline(\n",
        "    dataset_name,\n",
        "    seed,\n",
        "    w_acc=0.5,\n",
        "    w_f1=0.3,\n",
        "    w_auc=0.2,\n",
        "    test_size=0.2,\n",
        "    use_optuna=True,\n",
        "    n_trials=20,\n",
        "):\n",
        "    \"\"\"\n",
        "    Pipeline for graph classification using NetLSD embeddings + SVM,\n",
        "    with optional Optuna-based hyperparameter tuning and ENZYMES filtering.\n",
        "    \"\"\"\n",
        "    # Experiment ID\n",
        "    experiment_id = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    # Load dataset\n",
        "    dataset = TUDataset(root='data/TUDataset', name=dataset_name).shuffle()\n",
        "    print(f\"Loaded dataset {dataset_name} for NetLSD: {len(dataset)} graphs, {dataset.num_classes} classes\")\n",
        "\n",
        "    # Convert PyG graphs to NetworkX graphs\n",
        "    graphs = []\n",
        "    labels = []\n",
        "    for data in dataset:\n",
        "        g = to_networkx(data, to_undirected=True)\n",
        "        graphs.append(g)\n",
        "        labels.append(int(data.y.item()))\n",
        "\n",
        "    labels = np.array(labels)\n",
        "\n",
        "    # Special handling for ENZYMES\n",
        "    if dataset_name.upper() == \"ENZYMES\":\n",
        "        graphs, labels = filter_enzymes_graphs(graphs, labels, min_nodes=3)\n",
        "\n",
        "    # Outer train/test split on graphs\n",
        "    train_graphs, test_graphs, y_train, y_test = train_test_split(\n",
        "        graphs,\n",
        "        labels,\n",
        "        test_size=test_size,\n",
        "        random_state=seed,\n",
        "        stratify=labels,\n",
        "    )\n",
        "\n",
        "    opt_time = 0.0\n",
        "\n",
        "    def objective(trial):\n",
        "        # Hyperparameters for the SVM classifier\n",
        "        C = trial.suggest_loguniform(\"C\", 1e-2, 1e2)\n",
        "        gamma = trial.suggest_loguniform(\"gamma\", 1e-4, 1e1)\n",
        "\n",
        "        # Inner train/validation split on graphs\n",
        "        inner_tr_graphs, inner_val_graphs, y_tr, y_val = train_test_split(\n",
        "            train_graphs,\n",
        "            y_train,\n",
        "            test_size=0.2,\n",
        "            random_state=seed,\n",
        "            stratify=y_train,\n",
        "        )\n",
        "\n",
        "        all_graphs = inner_tr_graphs + inner_val_graphs\n",
        "\n",
        "        # Fit NetLSD on all and slice embeddings\n",
        "        netlsd = NetLSD()\n",
        "        netlsd.fit(all_graphs)\n",
        "        emb_all = sanitize_embeddings(netlsd.get_embedding())\n",
        "\n",
        "        X_tr = emb_all[:len(inner_tr_graphs)]\n",
        "        X_val = emb_all[len(inner_tr_graphs):]\n",
        "\n",
        "        clf = SVC(kernel=\"rbf\", probability=True, C=C, gamma=gamma, random_state=seed)\n",
        "        clf.fit(X_tr, y_tr)\n",
        "        y_pred = clf.predict(X_val)\n",
        "        y_prob = clf.predict_proba(X_val)\n",
        "\n",
        "        acc = accuracy_score(y_val, y_pred)\n",
        "        f1 = f1_score(y_val, y_pred, average=\"weighted\")\n",
        "        try:\n",
        "            if len(np.unique(y_val)) == 2:\n",
        "                auc = roc_auc_score(y_val, y_prob[:, 1])\n",
        "            else:\n",
        "                auc = roc_auc_score(y_val, y_prob, multi_class=\"ovr\")\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "\n",
        "        score = (w_acc * acc) + (w_f1 * f1) + (w_auc * (0 if np.isnan(auc) else auc))\n",
        "        return score\n",
        "\n",
        "    if use_optuna:\n",
        "        print(\"Running Optuna for NetLSD+SVM hyperparameter tuning...\")\n",
        "        start_opt = time.time()\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=n_trials)\n",
        "        best_params = study.best_params\n",
        "        opt_time = time.time() - start_opt\n",
        "        print(f\"Best hyperparameters (NetLSD+SVM): {best_params}\")\n",
        "    else:\n",
        "        best_params = {\"C\": 1.0, \"gamma\": \"scale\"}\n",
        "        print(f\"Using default SVM hyperparameters: {best_params}\")\n",
        "\n",
        "    # Final embedding + training using best hyperparameters\n",
        "    print(\"Running final NetLSD embedding on train+test graphs...\")\n",
        "    all_graphs_final = train_graphs + test_graphs\n",
        "    start_embed = time.time()\n",
        "    netlsd = NetLSD()\n",
        "    netlsd.fit(all_graphs_final)\n",
        "    emb_all = sanitize_embeddings(netlsd.get_embedding())\n",
        "    embed_time = time.time() - start_embed\n",
        "\n",
        "    X_train = emb_all[:len(train_graphs)]\n",
        "    X_test = emb_all[len(train_graphs):]\n",
        "\n",
        "    print(\"Training final SVM on NetLSD embeddings...\")\n",
        "    start_train = time.time()\n",
        "    clf = SVC(kernel=\"rbf\", probability=True, C=best_params[\"C\"], gamma=best_params[\"gamma\"], random_state=seed)\n",
        "    clf.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_train\n",
        "\n",
        "    # Evaluation on held-out test graphs\n",
        "    y_pred = clf.predict(X_test)\n",
        "    y_prob = clf.predict_proba(X_test)\n",
        "\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average=\"weighted\")\n",
        "    try:\n",
        "        if len(np.unique(y_test)) == 2:\n",
        "            auc = roc_auc_score(y_test, y_prob[:, 1])\n",
        "        else:\n",
        "            auc = roc_auc_score(y_test, y_prob, multi_class=\"ovr\")\n",
        "    except ValueError:\n",
        "        auc = np.nan\n",
        "\n",
        "    score = (w_acc * acc) + (w_f1 * f1) + (w_auc * (0 if np.isnan(auc) else auc))\n",
        "\n",
        "\n",
        "    # Save NetLSD embeddings (all graphs) + labels\n",
        "\n",
        "    netlsd_exp_dir = os.path.join(EMBEDDINGS_DIR, \"NetLSD\",dataset_name, experiment_id)\n",
        "    os.makedirs(netlsd_exp_dir, exist_ok=True)\n",
        "    np.save(os.path.join(netlsd_exp_dir, \"embeddings.npy\"), emb_all)\n",
        "    np.save(os.path.join(netlsd_exp_dir, \"labels.npy\"), np.array(labels))\n",
        "\n",
        "    # Save fitted NetLSD model\n",
        "    netlsd_model_path = f\"{MODELS_DIR}/NetLSD_{dataset_name}_{experiment_id}.joblib\"\n",
        "    joblib.dump(netlsd, netlsd_model_path)\n",
        "\n",
        "    # Save trained SVM classifier\n",
        "    svm_model_path = f\"{MODELS_DIR}/NetLSD_SVM_{dataset_name}_{experiment_id}.joblib\"\n",
        "    joblib.dump(clf, svm_model_path)\n",
        "\n",
        "\n",
        "    process = psutil.Process(os.getpid())\n",
        "    memory_usage = process.memory_info().rss / (1024 ** 2)  # in MB\n",
        "\n",
        "    print(f\"NetLSD Results on {dataset_name} -> Acc: {acc:.3f}, F1: {f1:.3f}, AUC: {auc:.3f}, Score: {score:.3f}\")\n",
        "    print(f\"Embedding time: {embed_time:.2f}s | SVM training time: {train_time:.2f}s | Optuna time: {opt_time:.2f}s | Memory usage: {memory_usage:.2f} MB\")\n",
        "\n",
        "    # Log summary to CSV\n",
        "    summary_path = f\"{CLASSIF_RESULTS_DIR}/netlsd.csv\"\n",
        "\n",
        "    summary_data = {\n",
        "        \"method\": \"NetLSD\",\n",
        "        \"seed\": seed,\n",
        "        \"optuna_enabled\": \"yes\" if use_optuna else \"no\",\n",
        "        \"C\": best_params[\"C\"],\n",
        "        \"gamma\": best_params[\"gamma\"],\n",
        "        \"acc\": round(float(acc), 4),\n",
        "        \"f1\": round(float(f1), 4),\n",
        "        \"auc\": round(float(auc) if not np.isnan(auc) else -1, 4),\n",
        "        \"score\": round(float(score), 4),\n",
        "        \"embedding_time (s)\": round(embed_time, 2),\n",
        "        \"svm_training_time (s)\": round(train_time, 2),\n",
        "        \"optuna_time (s)\": round(opt_time, 2),\n",
        "        \"memory_usage (MB)\": round(memory_usage, 2),\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame([summary_data])\n",
        "    if os.path.exists(summary_path):\n",
        "        df.to_csv(summary_path, mode='a', index=False, header=False)\n",
        "    else:\n",
        "        df.to_csv(summary_path, index=False)\n",
        "\n",
        "    print(f\"NetLSD summary stored in: {summary_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5dGElCoqdAr",
        "outputId": "70ece1d9-d611-4248-c250-072a64e64622"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/IMDB-MULTI.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset has no node features. Applying OneHotDegree transform...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:05:31,897] A new study created in memory with name: no-name-867cc728-6507-4353-b4f6-cd54306609a9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset IMDB-MULTI: 1500 graphs, 89 node features, 3 classes\n",
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:05:35,388] Trial 0 finished with value: 0.3808476513749058 and parameters: {'num_layers': 6, 'dropout': 0.33434670268252814, 'lr': 0.0013089563985149541, 'weight_decay': 0.0007165338468342581}. Best is trial 0 with value: 0.3808476513749058.\n",
            "[I 2026-01-22 11:05:37,669] Trial 1 finished with value: 0.459772480997108 and parameters: {'num_layers': 3, 'dropout': 0.11505963633519778, 'lr': 0.00046598131355125703, 'weight_decay': 0.00020963436185342127}. Best is trial 1 with value: 0.459772480997108.\n",
            "[I 2026-01-22 11:05:39,718] Trial 2 finished with value: 0.3821035737484786 and parameters: {'num_layers': 4, 'dropout': 0.2245986286091399, 'lr': 0.000114634640762874, 'weight_decay': 0.00010749554972788053}. Best is trial 1 with value: 0.459772480997108.\n",
            "[I 2026-01-22 11:05:41,682] Trial 3 finished with value: 0.46198086202559363 and parameters: {'num_layers': 4, 'dropout': 0.0988857215282207, 'lr': 0.00042478568601898034, 'weight_decay': 3.2466981984519287e-06}. Best is trial 3 with value: 0.46198086202559363.\n",
            "[I 2026-01-22 11:05:43,951] Trial 4 finished with value: 0.4085452600296522 and parameters: {'num_layers': 5, 'dropout': 0.24107030660792053, 'lr': 0.0067326925817169465, 'weight_decay': 1.3520481679391775e-06}. Best is trial 3 with value: 0.46198086202559363.\n",
            "[I 2026-01-22 11:05:46,242] Trial 5 finished with value: 0.4299594161952911 and parameters: {'num_layers': 5, 'dropout': 0.0675315806697232, 'lr': 0.0063504715384030035, 'weight_decay': 2.6072961238479143e-06}. Best is trial 3 with value: 0.46198086202559363.\n",
            "[I 2026-01-22 11:05:48,062] Trial 6 finished with value: 0.4648166052993292 and parameters: {'num_layers': 4, 'dropout': 0.2277921620647513, 'lr': 0.0017734904164030364, 'weight_decay': 2.9227798555112167e-06}. Best is trial 6 with value: 0.4648166052993292.\n",
            "[I 2026-01-22 11:05:51,132] Trial 7 finished with value: 0.44066822374919734 and parameters: {'num_layers': 6, 'dropout': 0.35397157372268456, 'lr': 0.002732881444555734, 'weight_decay': 9.187209596347665e-06}. Best is trial 6 with value: 0.4648166052993292.\n",
            "[I 2026-01-22 11:05:53,166] Trial 8 finished with value: 0.42672755919109095 and parameters: {'num_layers': 5, 'dropout': 0.22845239913949408, 'lr': 0.0004918883936788902, 'weight_decay': 3.635169817922722e-05}. Best is trial 6 with value: 0.4648166052993292.\n",
            "[I 2026-01-22 11:05:55,177] Trial 9 finished with value: 0.41668547556270263 and parameters: {'num_layers': 5, 'dropout': 0.37988164467968505, 'lr': 0.00025391379673289036, 'weight_decay': 6.822522962128393e-05}. Best is trial 6 with value: 0.4648166052993292.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'num_layers': 4, 'dropout': 0.2277921620647513, 'lr': 0.0017734904164030364, 'weight_decay': 2.9227798555112167e-06}\n",
            "\n",
            "Running final training GIN...\n",
            "{'num_layers': 4, 'dropout': 0.2277921620647513, 'lr': 0.0017734904164030364, 'weight_decay': 2.9227798555112167e-06}\n",
            "Epoch 001 | Loss=1.9787 | TestAcc=0.403 | F1=0.355 | AUC=0.610 | Time=0.39s\n",
            "Epoch 002 | Loss=1.5308 | TestAcc=0.440 | F1=0.426 | AUC=0.627 | Time=0.76s\n",
            "Epoch 003 | Loss=1.2920 | TestAcc=0.377 | F1=0.376 | AUC=0.589 | Time=1.13s\n",
            "Epoch 004 | Loss=1.2098 | TestAcc=0.433 | F1=0.374 | AUC=0.630 | Time=1.51s\n",
            "Epoch 005 | Loss=1.1238 | TestAcc=0.410 | F1=0.406 | AUC=0.611 | Time=1.89s\n",
            "Epoch 006 | Loss=1.0652 | TestAcc=0.473 | F1=0.463 | AUC=0.657 | Time=2.27s\n",
            "Epoch 007 | Loss=1.0755 | TestAcc=0.453 | F1=0.406 | AUC=0.656 | Time=2.64s\n",
            "Epoch 008 | Loss=1.0572 | TestAcc=0.430 | F1=0.428 | AUC=0.630 | Time=3.02s\n",
            "Epoch 009 | Loss=1.0300 | TestAcc=0.397 | F1=0.375 | AUC=0.627 | Time=3.40s\n",
            "Epoch 010 | Loss=1.0328 | TestAcc=0.443 | F1=0.439 | AUC=0.641 | Time=3.77s\n",
            "Epoch 011 | Loss=0.9972 | TestAcc=0.427 | F1=0.413 | AUC=0.626 | Time=4.19s\n",
            "Epoch 012 | Loss=1.0174 | TestAcc=0.467 | F1=0.458 | AUC=0.664 | Time=4.58s\n",
            "Epoch 013 | Loss=1.0101 | TestAcc=0.443 | F1=0.424 | AUC=0.668 | Time=4.96s\n",
            "Epoch 014 | Loss=1.0012 | TestAcc=0.467 | F1=0.455 | AUC=0.675 | Time=5.34s\n",
            "Epoch 015 | Loss=0.9871 | TestAcc=0.457 | F1=0.451 | AUC=0.656 | Time=5.72s\n",
            "Epoch 016 | Loss=1.0051 | TestAcc=0.433 | F1=0.428 | AUC=0.645 | Time=6.25s\n",
            "Epoch 017 | Loss=1.0022 | TestAcc=0.413 | F1=0.395 | AUC=0.637 | Time=6.76s\n",
            "Epoch 018 | Loss=0.9896 | TestAcc=0.473 | F1=0.465 | AUC=0.665 | Time=7.24s\n",
            "Epoch 019 | Loss=0.9655 | TestAcc=0.453 | F1=0.449 | AUC=0.671 | Time=7.74s\n",
            "Epoch 020 | Loss=0.9674 | TestAcc=0.470 | F1=0.470 | AUC=0.677 | Time=8.33s\n",
            "Epoch 021 | Loss=0.9692 | TestAcc=0.467 | F1=0.452 | AUC=0.679 | Time=8.89s\n",
            "Epoch 022 | Loss=0.9733 | TestAcc=0.483 | F1=0.459 | AUC=0.673 | Time=9.26s\n",
            "Epoch 023 | Loss=0.9723 | TestAcc=0.447 | F1=0.427 | AUC=0.670 | Time=9.66s\n",
            "Epoch 024 | Loss=0.9705 | TestAcc=0.447 | F1=0.432 | AUC=0.667 | Time=10.03s\n",
            "Epoch 025 | Loss=0.9712 | TestAcc=0.447 | F1=0.409 | AUC=0.678 | Time=10.40s\n",
            "Epoch 026 | Loss=0.9810 | TestAcc=0.457 | F1=0.453 | AUC=0.671 | Time=10.78s\n",
            "Epoch 027 | Loss=0.9830 | TestAcc=0.490 | F1=0.473 | AUC=0.694 | Time=11.16s\n",
            "Epoch 028 | Loss=0.9845 | TestAcc=0.477 | F1=0.430 | AUC=0.698 | Time=11.53s\n",
            "Epoch 029 | Loss=0.9641 | TestAcc=0.480 | F1=0.455 | AUC=0.672 | Time=11.92s\n",
            "Epoch 030 | Loss=0.9610 | TestAcc=0.463 | F1=0.454 | AUC=0.678 | Time=12.29s\n",
            "Epoch 031 | Loss=0.9576 | TestAcc=0.463 | F1=0.459 | AUC=0.675 | Time=12.68s\n",
            "Epoch 032 | Loss=0.9527 | TestAcc=0.473 | F1=0.430 | AUC=0.682 | Time=13.05s\n",
            "Epoch 033 | Loss=0.9462 | TestAcc=0.467 | F1=0.451 | AUC=0.678 | Time=13.42s\n",
            "Epoch 034 | Loss=0.9587 | TestAcc=0.457 | F1=0.426 | AUC=0.697 | Time=13.79s\n",
            "Epoch 035 | Loss=0.9588 | TestAcc=0.477 | F1=0.461 | AUC=0.683 | Time=14.17s\n",
            "Epoch 036 | Loss=0.9450 | TestAcc=0.460 | F1=0.450 | AUC=0.685 | Time=14.54s\n",
            "Epoch 037 | Loss=0.9331 | TestAcc=0.480 | F1=0.474 | AUC=0.688 | Time=14.93s\n",
            "Epoch 038 | Loss=0.9639 | TestAcc=0.447 | F1=0.427 | AUC=0.654 | Time=15.29s\n",
            "Epoch 039 | Loss=0.9530 | TestAcc=0.477 | F1=0.453 | AUC=0.682 | Time=15.67s\n",
            "Epoch 040 | Loss=0.9355 | TestAcc=0.443 | F1=0.438 | AUC=0.650 | Time=16.06s\n",
            "Epoch 041 | Loss=0.9337 | TestAcc=0.460 | F1=0.436 | AUC=0.665 | Time=16.43s\n",
            "Epoch 042 | Loss=0.9484 | TestAcc=0.477 | F1=0.468 | AUC=0.673 | Time=16.82s\n",
            "Epoch 043 | Loss=0.9341 | TestAcc=0.450 | F1=0.438 | AUC=0.662 | Time=17.20s\n",
            "Epoch 044 | Loss=0.9427 | TestAcc=0.443 | F1=0.395 | AUC=0.671 | Time=17.59s\n",
            "Epoch 045 | Loss=0.9452 | TestAcc=0.463 | F1=0.443 | AUC=0.670 | Time=17.98s\n",
            "Epoch 046 | Loss=0.9589 | TestAcc=0.457 | F1=0.437 | AUC=0.666 | Time=18.36s\n",
            "Epoch 047 | Loss=0.9486 | TestAcc=0.450 | F1=0.434 | AUC=0.661 | Time=18.74s\n",
            "Epoch 048 | Loss=0.9524 | TestAcc=0.450 | F1=0.447 | AUC=0.653 | Time=19.27s\n",
            "Epoch 049 | Loss=0.9308 | TestAcc=0.460 | F1=0.449 | AUC=0.674 | Time=19.76s\n",
            "Epoch 050 | Loss=0.9293 | TestAcc=0.433 | F1=0.432 | AUC=0.647 | Time=20.27s\n",
            "Epoch 051 | Loss=0.9239 | TestAcc=0.450 | F1=0.444 | AUC=0.651 | Time=20.75s\n",
            "Epoch 052 | Loss=0.9323 | TestAcc=0.473 | F1=0.470 | AUC=0.653 | Time=21.33s\n",
            "Epoch 053 | Loss=0.9314 | TestAcc=0.480 | F1=0.467 | AUC=0.654 | Time=21.85s\n",
            "Epoch 054 | Loss=0.9365 | TestAcc=0.443 | F1=0.426 | AUC=0.662 | Time=22.23s\n",
            "Epoch 055 | Loss=0.9224 | TestAcc=0.433 | F1=0.424 | AUC=0.659 | Time=22.61s\n",
            "Epoch 056 | Loss=0.9141 | TestAcc=0.433 | F1=0.410 | AUC=0.662 | Time=22.98s\n",
            "Epoch 057 | Loss=0.9140 | TestAcc=0.470 | F1=0.458 | AUC=0.665 | Time=23.37s\n",
            "Epoch 058 | Loss=0.9154 | TestAcc=0.473 | F1=0.456 | AUC=0.674 | Time=23.94s\n",
            "Epoch 059 | Loss=0.9329 | TestAcc=0.450 | F1=0.423 | AUC=0.678 | Time=24.97s\n",
            "Epoch 060 | Loss=0.9109 | TestAcc=0.460 | F1=0.442 | AUC=0.654 | Time=25.98s\n",
            "Epoch 061 | Loss=0.9139 | TestAcc=0.457 | F1=0.447 | AUC=0.661 | Time=26.82s\n",
            "Epoch 062 | Loss=0.9187 | TestAcc=0.453 | F1=0.435 | AUC=0.674 | Time=27.62s\n",
            "Epoch 063 | Loss=0.9024 | TestAcc=0.447 | F1=0.439 | AUC=0.666 | Time=28.42s\n",
            "Epoch 064 | Loss=0.9234 | TestAcc=0.447 | F1=0.431 | AUC=0.650 | Time=29.27s\n",
            "Epoch 065 | Loss=0.9054 | TestAcc=0.473 | F1=0.470 | AUC=0.660 | Time=30.40s\n",
            "Epoch 066 | Loss=0.9144 | TestAcc=0.457 | F1=0.419 | AUC=0.666 | Time=31.54s\n",
            "Epoch 067 | Loss=0.9205 | TestAcc=0.473 | F1=0.455 | AUC=0.695 | Time=33.31s\n",
            "Epoch 068 | Loss=0.8955 | TestAcc=0.463 | F1=0.456 | AUC=0.670 | Time=35.27s\n",
            "Epoch 069 | Loss=0.9257 | TestAcc=0.423 | F1=0.403 | AUC=0.632 | Time=36.22s\n",
            "Epoch 070 | Loss=0.9433 | TestAcc=0.440 | F1=0.429 | AUC=0.648 | Time=37.57s\n",
            "Epoch 071 | Loss=0.9171 | TestAcc=0.440 | F1=0.408 | AUC=0.676 | Time=38.94s\n",
            "Epoch 072 | Loss=0.9259 | TestAcc=0.460 | F1=0.434 | AUC=0.665 | Time=40.26s\n",
            "Epoch 073 | Loss=0.9157 | TestAcc=0.480 | F1=0.459 | AUC=0.675 | Time=40.99s\n",
            "Epoch 074 | Loss=0.9078 | TestAcc=0.490 | F1=0.458 | AUC=0.699 | Time=41.37s\n",
            "Epoch 075 | Loss=0.9085 | TestAcc=0.450 | F1=0.419 | AUC=0.676 | Time=41.74s\n",
            "Epoch 076 | Loss=0.9111 | TestAcc=0.483 | F1=0.463 | AUC=0.685 | Time=42.13s\n",
            "Epoch 077 | Loss=0.9046 | TestAcc=0.450 | F1=0.443 | AUC=0.663 | Time=42.50s\n",
            "Epoch 078 | Loss=0.9066 | TestAcc=0.453 | F1=0.411 | AUC=0.662 | Time=42.87s\n",
            "Epoch 079 | Loss=0.9289 | TestAcc=0.490 | F1=0.490 | AUC=0.663 | Time=43.26s\n",
            "Epoch 080 | Loss=0.9187 | TestAcc=0.480 | F1=0.446 | AUC=0.662 | Time=43.63s\n",
            "Epoch 081 | Loss=0.9080 | TestAcc=0.450 | F1=0.437 | AUC=0.681 | Time=44.00s\n",
            "Epoch 082 | Loss=0.9086 | TestAcc=0.433 | F1=0.419 | AUC=0.649 | Time=44.39s\n",
            "Epoch 083 | Loss=0.8947 | TestAcc=0.460 | F1=0.441 | AUC=0.673 | Time=44.76s\n",
            "Epoch 084 | Loss=0.9071 | TestAcc=0.480 | F1=0.435 | AUC=0.683 | Time=45.14s\n",
            "Epoch 085 | Loss=0.9104 | TestAcc=0.460 | F1=0.446 | AUC=0.666 | Time=45.60s\n",
            "Epoch 086 | Loss=0.9182 | TestAcc=0.470 | F1=0.427 | AUC=0.666 | Time=46.09s\n",
            "Epoch 087 | Loss=0.9064 | TestAcc=0.497 | F1=0.481 | AUC=0.696 | Time=46.58s\n",
            "Epoch 088 | Loss=0.8920 | TestAcc=0.493 | F1=0.479 | AUC=0.672 | Time=47.07s\n",
            "Epoch 089 | Loss=0.9272 | TestAcc=0.467 | F1=0.457 | AUC=0.670 | Time=47.60s\n",
            "Epoch 090 | Loss=0.9053 | TestAcc=0.483 | F1=0.458 | AUC=0.682 | Time=48.17s\n",
            "Epoch 091 | Loss=0.8964 | TestAcc=0.483 | F1=0.473 | AUC=0.675 | Time=48.67s\n",
            "Epoch 092 | Loss=0.9149 | TestAcc=0.447 | F1=0.442 | AUC=0.649 | Time=49.04s\n",
            "Epoch 093 | Loss=0.8791 | TestAcc=0.447 | F1=0.430 | AUC=0.660 | Time=49.41s\n",
            "Epoch 094 | Loss=0.9082 | TestAcc=0.487 | F1=0.460 | AUC=0.676 | Time=49.80s\n",
            "Epoch 095 | Loss=0.9043 | TestAcc=0.503 | F1=0.473 | AUC=0.693 | Time=50.17s\n",
            "Epoch 096 | Loss=0.9019 | TestAcc=0.450 | F1=0.428 | AUC=0.648 | Time=50.56s\n",
            "Epoch 097 | Loss=0.9090 | TestAcc=0.490 | F1=0.459 | AUC=0.664 | Time=50.93s\n",
            "Epoch 098 | Loss=0.9182 | TestAcc=0.483 | F1=0.464 | AUC=0.679 | Time=51.31s\n",
            "Epoch 099 | Loss=0.8927 | TestAcc=0.513 | F1=0.487 | AUC=0.677 | Time=51.70s\n",
            "Epoch 100 | Loss=0.8817 | TestAcc=0.497 | F1=0.495 | AUC=0.678 | Time=52.08s\n",
            "Epoch 101 | Loss=0.9221 | TestAcc=0.487 | F1=0.472 | AUC=0.669 | Time=52.45s\n",
            "Epoch 102 | Loss=0.9031 | TestAcc=0.467 | F1=0.455 | AUC=0.654 | Time=52.83s\n",
            "Epoch 103 | Loss=0.9034 | TestAcc=0.460 | F1=0.460 | AUC=0.654 | Time=53.21s\n",
            "Epoch 104 | Loss=0.8958 | TestAcc=0.460 | F1=0.459 | AUC=0.669 | Time=53.59s\n",
            "Epoch 105 | Loss=0.8803 | TestAcc=0.463 | F1=0.451 | AUC=0.650 | Time=53.97s\n",
            "Epoch 106 | Loss=0.9138 | TestAcc=0.473 | F1=0.472 | AUC=0.665 | Time=54.35s\n",
            "Epoch 107 | Loss=0.8961 | TestAcc=0.447 | F1=0.420 | AUC=0.664 | Time=54.73s\n",
            "Epoch 108 | Loss=0.9214 | TestAcc=0.437 | F1=0.416 | AUC=0.668 | Time=55.11s\n",
            "Epoch 109 | Loss=0.8991 | TestAcc=0.450 | F1=0.434 | AUC=0.681 | Time=55.48s\n",
            "Epoch 110 | Loss=0.8962 | TestAcc=0.470 | F1=0.451 | AUC=0.662 | Time=55.87s\n",
            "Epoch 111 | Loss=0.8958 | TestAcc=0.490 | F1=0.480 | AUC=0.671 | Time=56.24s\n",
            "Epoch 112 | Loss=0.8856 | TestAcc=0.450 | F1=0.422 | AUC=0.661 | Time=56.61s\n",
            "Epoch 113 | Loss=0.8846 | TestAcc=0.450 | F1=0.417 | AUC=0.647 | Time=57.00s\n",
            "Epoch 114 | Loss=0.9140 | TestAcc=0.470 | F1=0.460 | AUC=0.665 | Time=57.37s\n",
            "Epoch 115 | Loss=0.8983 | TestAcc=0.440 | F1=0.425 | AUC=0.672 | Time=57.75s\n",
            "Epoch 116 | Loss=0.8805 | TestAcc=0.457 | F1=0.454 | AUC=0.665 | Time=58.14s\n",
            "Epoch 117 | Loss=0.8878 | TestAcc=0.467 | F1=0.453 | AUC=0.674 | Time=58.53s\n",
            "Epoch 118 | Loss=0.8845 | TestAcc=0.443 | F1=0.434 | AUC=0.657 | Time=59.08s\n",
            "Epoch 119 | Loss=0.8758 | TestAcc=0.483 | F1=0.474 | AUC=0.668 | Time=59.56s\n",
            "Epoch 120 | Loss=0.8787 | TestAcc=0.447 | F1=0.438 | AUC=0.654 | Time=60.08s\n",
            "Epoch 121 | Loss=0.8893 | TestAcc=0.440 | F1=0.439 | AUC=0.665 | Time=60.56s\n",
            "Epoch 122 | Loss=0.8846 | TestAcc=0.457 | F1=0.428 | AUC=0.669 | Time=61.12s\n",
            "Epoch 123 | Loss=0.8925 | TestAcc=0.467 | F1=0.438 | AUC=0.675 | Time=61.63s\n",
            "Epoch 124 | Loss=0.8914 | TestAcc=0.487 | F1=0.472 | AUC=0.672 | Time=62.02s\n",
            "Epoch 125 | Loss=0.8925 | TestAcc=0.450 | F1=0.444 | AUC=0.663 | Time=62.39s\n",
            "Epoch 126 | Loss=0.8934 | TestAcc=0.460 | F1=0.452 | AUC=0.652 | Time=62.76s\n",
            "Epoch 127 | Loss=0.9018 | TestAcc=0.470 | F1=0.460 | AUC=0.660 | Time=63.15s\n",
            "Epoch 128 | Loss=0.8829 | TestAcc=0.457 | F1=0.443 | AUC=0.640 | Time=63.52s\n",
            "Epoch 129 | Loss=0.8645 | TestAcc=0.467 | F1=0.457 | AUC=0.647 | Time=63.90s\n",
            "Epoch 130 | Loss=0.8611 | TestAcc=0.480 | F1=0.465 | AUC=0.671 | Time=64.28s\n",
            "Epoch 131 | Loss=0.8906 | TestAcc=0.453 | F1=0.445 | AUC=0.668 | Time=64.65s\n",
            "Epoch 132 | Loss=0.8645 | TestAcc=0.460 | F1=0.454 | AUC=0.639 | Time=65.04s\n",
            "Epoch 133 | Loss=0.8615 | TestAcc=0.497 | F1=0.482 | AUC=0.659 | Time=65.41s\n",
            "Epoch 134 | Loss=0.8784 | TestAcc=0.447 | F1=0.440 | AUC=0.649 | Time=65.78s\n",
            "Epoch 135 | Loss=0.8659 | TestAcc=0.477 | F1=0.468 | AUC=0.672 | Time=66.18s\n",
            "Epoch 136 | Loss=0.8742 | TestAcc=0.467 | F1=0.459 | AUC=0.662 | Time=66.55s\n",
            "Epoch 137 | Loss=0.8658 | TestAcc=0.470 | F1=0.442 | AUC=0.670 | Time=66.92s\n",
            "Epoch 138 | Loss=0.8569 | TestAcc=0.450 | F1=0.444 | AUC=0.649 | Time=67.31s\n",
            "Epoch 139 | Loss=0.8538 | TestAcc=0.450 | F1=0.427 | AUC=0.648 | Time=67.68s\n",
            "Epoch 140 | Loss=0.8593 | TestAcc=0.467 | F1=0.446 | AUC=0.650 | Time=68.06s\n",
            "Epoch 141 | Loss=0.8478 | TestAcc=0.457 | F1=0.439 | AUC=0.649 | Time=68.46s\n",
            "Epoch 142 | Loss=0.8391 | TestAcc=0.460 | F1=0.457 | AUC=0.661 | Time=68.83s\n",
            "Epoch 143 | Loss=0.8876 | TestAcc=0.483 | F1=0.472 | AUC=0.663 | Time=69.22s\n",
            "Epoch 144 | Loss=0.8748 | TestAcc=0.460 | F1=0.451 | AUC=0.656 | Time=69.59s\n",
            "Epoch 145 | Loss=0.8635 | TestAcc=0.470 | F1=0.439 | AUC=0.675 | Time=69.97s\n",
            "Epoch 146 | Loss=0.8593 | TestAcc=0.447 | F1=0.420 | AUC=0.645 | Time=70.34s\n",
            "Epoch 147 | Loss=0.8654 | TestAcc=0.443 | F1=0.421 | AUC=0.655 | Time=70.71s\n",
            "Epoch 148 | Loss=0.8550 | TestAcc=0.457 | F1=0.436 | AUC=0.660 | Time=71.09s\n",
            "Epoch 149 | Loss=0.8702 | TestAcc=0.433 | F1=0.423 | AUC=0.646 | Time=71.48s\n",
            "Epoch 150 | Loss=0.8835 | TestAcc=0.450 | F1=0.436 | AUC=0.646 | Time=72.02s\n",
            "Epoch 151 | Loss=0.8639 | TestAcc=0.480 | F1=0.446 | AUC=0.658 | Time=72.52s\n",
            "Epoch 152 | Loss=0.8609 | TestAcc=0.443 | F1=0.424 | AUC=0.636 | Time=73.01s\n",
            "Epoch 153 | Loss=0.8701 | TestAcc=0.460 | F1=0.447 | AUC=0.654 | Time=73.50s\n",
            "Epoch 154 | Loss=0.8686 | TestAcc=0.447 | F1=0.423 | AUC=0.661 | Time=74.04s\n",
            "Epoch 155 | Loss=0.8778 | TestAcc=0.450 | F1=0.442 | AUC=0.654 | Time=74.59s\n",
            "Epoch 156 | Loss=0.8886 | TestAcc=0.463 | F1=0.438 | AUC=0.633 | Time=74.98s\n",
            "Epoch 157 | Loss=0.9005 | TestAcc=0.453 | F1=0.442 | AUC=0.656 | Time=75.35s\n",
            "Epoch 158 | Loss=0.8590 | TestAcc=0.470 | F1=0.458 | AUC=0.662 | Time=75.73s\n",
            "Epoch 159 | Loss=0.8856 | TestAcc=0.450 | F1=0.421 | AUC=0.654 | Time=76.11s\n",
            "Epoch 160 | Loss=0.8716 | TestAcc=0.440 | F1=0.430 | AUC=0.650 | Time=76.49s\n",
            "Epoch 161 | Loss=0.8579 | TestAcc=0.487 | F1=0.466 | AUC=0.657 | Time=76.87s\n",
            "Epoch 162 | Loss=0.8424 | TestAcc=0.473 | F1=0.461 | AUC=0.648 | Time=77.24s\n",
            "Epoch 163 | Loss=0.8726 | TestAcc=0.477 | F1=0.454 | AUC=0.657 | Time=77.65s\n",
            "Epoch 164 | Loss=0.8595 | TestAcc=0.450 | F1=0.438 | AUC=0.656 | Time=78.03s\n",
            "Epoch 165 | Loss=0.8826 | TestAcc=0.450 | F1=0.413 | AUC=0.646 | Time=78.41s\n",
            "Epoch 166 | Loss=0.8577 | TestAcc=0.443 | F1=0.439 | AUC=0.656 | Time=78.81s\n",
            "Epoch 167 | Loss=0.8450 | TestAcc=0.440 | F1=0.416 | AUC=0.650 | Time=79.18s\n",
            "Epoch 168 | Loss=0.8698 | TestAcc=0.467 | F1=0.441 | AUC=0.646 | Time=79.55s\n",
            "Epoch 169 | Loss=0.8521 | TestAcc=0.440 | F1=0.435 | AUC=0.659 | Time=79.95s\n",
            "Epoch 170 | Loss=0.8408 | TestAcc=0.440 | F1=0.425 | AUC=0.655 | Time=80.32s\n",
            "Epoch 171 | Loss=0.8604 | TestAcc=0.477 | F1=0.451 | AUC=0.666 | Time=80.70s\n",
            "Epoch 172 | Loss=0.8503 | TestAcc=0.427 | F1=0.406 | AUC=0.637 | Time=81.08s\n",
            "Epoch 173 | Loss=0.8631 | TestAcc=0.420 | F1=0.408 | AUC=0.641 | Time=81.45s\n",
            "Epoch 174 | Loss=0.8636 | TestAcc=0.443 | F1=0.424 | AUC=0.660 | Time=81.83s\n",
            "Epoch 175 | Loss=0.8579 | TestAcc=0.423 | F1=0.413 | AUC=0.624 | Time=82.22s\n",
            "Epoch 176 | Loss=0.8594 | TestAcc=0.450 | F1=0.441 | AUC=0.665 | Time=82.59s\n",
            "Epoch 177 | Loss=0.8378 | TestAcc=0.447 | F1=0.442 | AUC=0.645 | Time=82.99s\n",
            "Epoch 178 | Loss=0.8435 | TestAcc=0.430 | F1=0.412 | AUC=0.647 | Time=83.36s\n",
            "Epoch 179 | Loss=0.8388 | TestAcc=0.443 | F1=0.423 | AUC=0.650 | Time=83.74s\n",
            "Epoch 180 | Loss=0.8267 | TestAcc=0.447 | F1=0.432 | AUC=0.649 | Time=84.12s\n",
            "Epoch 181 | Loss=0.8443 | TestAcc=0.450 | F1=0.432 | AUC=0.651 | Time=84.48s\n",
            "Epoch 182 | Loss=0.8379 | TestAcc=0.470 | F1=0.455 | AUC=0.643 | Time=85.02s\n",
            "Epoch 183 | Loss=0.8592 | TestAcc=0.450 | F1=0.428 | AUC=0.644 | Time=85.52s\n",
            "Epoch 184 | Loss=0.8412 | TestAcc=0.440 | F1=0.428 | AUC=0.639 | Time=86.03s\n",
            "Epoch 185 | Loss=0.8297 | TestAcc=0.443 | F1=0.431 | AUC=0.649 | Time=86.51s\n",
            "Epoch 186 | Loss=0.8430 | TestAcc=0.423 | F1=0.411 | AUC=0.649 | Time=87.07s\n",
            "Epoch 187 | Loss=0.8470 | TestAcc=0.457 | F1=0.433 | AUC=0.626 | Time=87.58s\n",
            "Epoch 188 | Loss=0.9080 | TestAcc=0.447 | F1=0.432 | AUC=0.649 | Time=87.97s\n",
            "Epoch 189 | Loss=0.8635 | TestAcc=0.437 | F1=0.418 | AUC=0.653 | Time=88.34s\n",
            "Epoch 190 | Loss=0.8499 | TestAcc=0.453 | F1=0.444 | AUC=0.635 | Time=88.71s\n",
            "Epoch 191 | Loss=0.8399 | TestAcc=0.453 | F1=0.431 | AUC=0.656 | Time=89.13s\n",
            "Epoch 192 | Loss=0.8541 | TestAcc=0.433 | F1=0.415 | AUC=0.644 | Time=89.51s\n",
            "Epoch 193 | Loss=0.8433 | TestAcc=0.453 | F1=0.440 | AUC=0.659 | Time=89.90s\n",
            "Epoch 194 | Loss=0.8631 | TestAcc=0.467 | F1=0.450 | AUC=0.656 | Time=90.29s\n",
            "Epoch 195 | Loss=0.8632 | TestAcc=0.463 | F1=0.450 | AUC=0.660 | Time=90.66s\n",
            "Epoch 196 | Loss=0.8527 | TestAcc=0.473 | F1=0.457 | AUC=0.654 | Time=91.05s\n",
            "Epoch 197 | Loss=0.8831 | TestAcc=0.430 | F1=0.408 | AUC=0.605 | Time=91.42s\n",
            "Epoch 198 | Loss=0.8960 | TestAcc=0.453 | F1=0.433 | AUC=0.633 | Time=91.79s\n",
            "Epoch 199 | Loss=0.8654 | TestAcc=0.440 | F1=0.430 | AUC=0.639 | Time=92.18s\n",
            "Epoch 200 | Loss=0.8762 | TestAcc=0.460 | F1=0.438 | AUC=0.657 | Time=92.55s\n",
            "\n",
            "Training summary stored in : /content/drive/MyDrive/InformationSystems/Classification/results/classification/gin.csv\n",
            "  method  seed     dataset optimization_enabled  embedding_dimension  \\\n",
            "0    GIN    42  IMDB-MULTI                  yes                   64   \n",
            "\n",
            "  objective_weights  num_layers   dropout        lr  weight_decay  epochs  \\\n",
            "0     (0.5,0.3,0.2)           4  0.227792  0.001773      0.000003     200   \n",
            "\n",
            "   best_epoch  best_loss  eval_loss  eval_acc  eval_f1  eval_auc  \\\n",
            "0          99     1.0223     1.0223    0.5133   0.4875    0.6772   \n",
            "\n",
            "   training_time (s)  generation_time (s)  memory_usage (MB)  \n",
            "0              92.55                23.28            1449.03  \n",
            "Saved model: /content/drive/MyDrive/InformationSystems/Classification/models/GIN_IMDB-MULTI_42.pth\n",
            "Dataset has no node features. Applying OneHotDegree transform...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:07:28,949] A new study created in memory with name: no-name-53b0f0ec-e58a-4fb1-97de-58e8cb457ccd\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset IMDB-MULTI: 1500 graphs, 89 node features, 3 classes\n",
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:07:31,061] Trial 0 finished with value: 0.45066491083935656 and parameters: {'num_layers': 5, 'dropout': 0.3968406347303555, 'lr': 0.0016025687592155459, 'weight_decay': 0.00025006238832684426}. Best is trial 0 with value: 0.45066491083935656.\n",
            "[I 2026-01-22 11:07:32,976] Trial 1 finished with value: 0.4838049306385075 and parameters: {'num_layers': 4, 'dropout': 0.47625284744213514, 'lr': 0.005019519413100507, 'weight_decay': 2.7569098100337688e-05}. Best is trial 1 with value: 0.4838049306385075.\n",
            "[I 2026-01-22 11:07:35,510] Trial 2 finished with value: 0.48527842963668855 and parameters: {'num_layers': 4, 'dropout': 0.3681266750412476, 'lr': 0.0028644583057078434, 'weight_decay': 0.0007793022994105935}. Best is trial 2 with value: 0.48527842963668855.\n",
            "[I 2026-01-22 11:07:37,876] Trial 3 finished with value: 0.4799451935566936 and parameters: {'num_layers': 6, 'dropout': 0.5152821577956808, 'lr': 0.00530329580116467, 'weight_decay': 1.7083268121101567e-06}. Best is trial 2 with value: 0.48527842963668855.\n",
            "[I 2026-01-22 11:07:39,710] Trial 4 finished with value: 0.5064280727726486 and parameters: {'num_layers': 4, 'dropout': 0.27715096923033883, 'lr': 0.003204457311949126, 'weight_decay': 2.2977853799793026e-05}. Best is trial 4 with value: 0.5064280727726486.\n",
            "[I 2026-01-22 11:07:41,378] Trial 5 finished with value: 0.48906828483620246 and parameters: {'num_layers': 3, 'dropout': 0.5066085651630807, 'lr': 0.007688654128934974, 'weight_decay': 0.00019646047145209267}. Best is trial 4 with value: 0.5064280727726486.\n",
            "[I 2026-01-22 11:07:43,035] Trial 6 finished with value: 0.5109864409700388 and parameters: {'num_layers': 3, 'dropout': 0.0345838540600349, 'lr': 0.0022279585847517817, 'weight_decay': 2.20402473240747e-05}. Best is trial 6 with value: 0.5109864409700388.\n",
            "[I 2026-01-22 11:07:45,529] Trial 7 finished with value: 0.49106183656200375 and parameters: {'num_layers': 6, 'dropout': 0.538462169060877, 'lr': 0.009672807499989616, 'weight_decay': 1.7328738752160933e-05}. Best is trial 6 with value: 0.5109864409700388.\n",
            "[I 2026-01-22 11:07:47,649] Trial 8 finished with value: 0.4510334766687232 and parameters: {'num_layers': 3, 'dropout': 0.3838034030569043, 'lr': 0.00029451674963230635, 'weight_decay': 2.320503241694264e-05}. Best is trial 6 with value: 0.5109864409700388.\n",
            "[I 2026-01-22 11:07:50,238] Trial 9 finished with value: 0.4381033661023732 and parameters: {'num_layers': 6, 'dropout': 0.08684225002149808, 'lr': 0.002901965921033013, 'weight_decay': 0.0006004166037957332}. Best is trial 6 with value: 0.5109864409700388.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'num_layers': 3, 'dropout': 0.0345838540600349, 'lr': 0.0022279585847517817, 'weight_decay': 2.20402473240747e-05}\n",
            "\n",
            "Running final training GIN...\n",
            "{'num_layers': 3, 'dropout': 0.0345838540600349, 'lr': 0.0022279585847517817, 'weight_decay': 2.20402473240747e-05}\n",
            "Epoch 001 | Loss=1.6252 | TestAcc=0.460 | F1=0.392 | AUC=0.624 | Time=0.35s\n",
            "Epoch 002 | Loss=1.1829 | TestAcc=0.440 | F1=0.410 | AUC=0.638 | Time=0.69s\n",
            "Epoch 003 | Loss=1.1192 | TestAcc=0.467 | F1=0.409 | AUC=0.658 | Time=1.04s\n",
            "Epoch 004 | Loss=1.1168 | TestAcc=0.460 | F1=0.459 | AUC=0.638 | Time=1.37s\n",
            "Epoch 005 | Loss=1.1023 | TestAcc=0.453 | F1=0.403 | AUC=0.661 | Time=1.70s\n",
            "Epoch 006 | Loss=1.0267 | TestAcc=0.463 | F1=0.405 | AUC=0.664 | Time=2.05s\n",
            "Epoch 007 | Loss=1.0248 | TestAcc=0.457 | F1=0.393 | AUC=0.671 | Time=2.38s\n",
            "Epoch 008 | Loss=1.0287 | TestAcc=0.443 | F1=0.412 | AUC=0.676 | Time=2.72s\n",
            "Epoch 009 | Loss=0.9985 | TestAcc=0.467 | F1=0.465 | AUC=0.681 | Time=3.08s\n",
            "Epoch 010 | Loss=0.9925 | TestAcc=0.493 | F1=0.461 | AUC=0.684 | Time=3.42s\n",
            "Epoch 011 | Loss=0.9786 | TestAcc=0.423 | F1=0.407 | AUC=0.656 | Time=3.76s\n",
            "Epoch 012 | Loss=1.0056 | TestAcc=0.497 | F1=0.479 | AUC=0.686 | Time=4.09s\n",
            "Epoch 013 | Loss=0.9716 | TestAcc=0.480 | F1=0.459 | AUC=0.684 | Time=4.43s\n",
            "Epoch 014 | Loss=0.9541 | TestAcc=0.480 | F1=0.471 | AUC=0.671 | Time=4.77s\n",
            "Epoch 015 | Loss=0.9896 | TestAcc=0.443 | F1=0.428 | AUC=0.665 | Time=5.10s\n",
            "Epoch 016 | Loss=0.9658 | TestAcc=0.480 | F1=0.460 | AUC=0.683 | Time=5.47s\n",
            "Epoch 017 | Loss=0.9519 | TestAcc=0.457 | F1=0.424 | AUC=0.701 | Time=5.81s\n",
            "Epoch 018 | Loss=0.9664 | TestAcc=0.483 | F1=0.461 | AUC=0.692 | Time=6.15s\n",
            "Epoch 019 | Loss=0.9602 | TestAcc=0.483 | F1=0.483 | AUC=0.659 | Time=6.49s\n",
            "Epoch 020 | Loss=0.9481 | TestAcc=0.510 | F1=0.499 | AUC=0.697 | Time=6.83s\n",
            "Epoch 021 | Loss=0.9502 | TestAcc=0.480 | F1=0.466 | AUC=0.674 | Time=7.17s\n",
            "Epoch 022 | Loss=0.9416 | TestAcc=0.483 | F1=0.475 | AUC=0.677 | Time=7.52s\n",
            "Epoch 023 | Loss=0.9391 | TestAcc=0.513 | F1=0.509 | AUC=0.703 | Time=7.86s\n",
            "Epoch 024 | Loss=0.9370 | TestAcc=0.467 | F1=0.437 | AUC=0.680 | Time=8.20s\n",
            "Epoch 025 | Loss=0.9177 | TestAcc=0.473 | F1=0.458 | AUC=0.667 | Time=8.64s\n",
            "Epoch 026 | Loss=0.9319 | TestAcc=0.460 | F1=0.413 | AUC=0.660 | Time=9.08s\n",
            "Epoch 027 | Loss=0.9438 | TestAcc=0.480 | F1=0.472 | AUC=0.675 | Time=9.52s\n",
            "Epoch 028 | Loss=0.9352 | TestAcc=0.480 | F1=0.464 | AUC=0.690 | Time=9.97s\n",
            "Epoch 029 | Loss=0.9085 | TestAcc=0.503 | F1=0.495 | AUC=0.689 | Time=10.41s\n",
            "Epoch 030 | Loss=0.9271 | TestAcc=0.460 | F1=0.440 | AUC=0.647 | Time=10.90s\n",
            "Epoch 031 | Loss=0.9374 | TestAcc=0.473 | F1=0.449 | AUC=0.680 | Time=11.42s\n",
            "Epoch 032 | Loss=0.9311 | TestAcc=0.467 | F1=0.446 | AUC=0.651 | Time=11.76s\n",
            "Epoch 033 | Loss=0.9118 | TestAcc=0.477 | F1=0.442 | AUC=0.675 | Time=12.10s\n",
            "Epoch 034 | Loss=0.9010 | TestAcc=0.493 | F1=0.459 | AUC=0.666 | Time=12.45s\n",
            "Epoch 035 | Loss=0.9135 | TestAcc=0.463 | F1=0.434 | AUC=0.659 | Time=12.80s\n",
            "Epoch 036 | Loss=0.9218 | TestAcc=0.487 | F1=0.448 | AUC=0.668 | Time=13.13s\n",
            "Epoch 037 | Loss=0.9097 | TestAcc=0.477 | F1=0.456 | AUC=0.663 | Time=13.47s\n",
            "Epoch 038 | Loss=0.9037 | TestAcc=0.473 | F1=0.455 | AUC=0.653 | Time=13.82s\n",
            "Epoch 039 | Loss=0.8901 | TestAcc=0.483 | F1=0.460 | AUC=0.660 | Time=14.15s\n",
            "Epoch 040 | Loss=0.8815 | TestAcc=0.463 | F1=0.445 | AUC=0.658 | Time=14.48s\n",
            "Epoch 041 | Loss=0.8814 | TestAcc=0.460 | F1=0.445 | AUC=0.653 | Time=14.85s\n",
            "Epoch 042 | Loss=0.8805 | TestAcc=0.447 | F1=0.433 | AUC=0.644 | Time=15.18s\n",
            "Epoch 043 | Loss=0.8870 | TestAcc=0.477 | F1=0.459 | AUC=0.655 | Time=15.53s\n",
            "Epoch 044 | Loss=0.9141 | TestAcc=0.463 | F1=0.432 | AUC=0.652 | Time=15.89s\n",
            "Epoch 045 | Loss=0.9138 | TestAcc=0.450 | F1=0.426 | AUC=0.637 | Time=16.22s\n",
            "Epoch 046 | Loss=0.8974 | TestAcc=0.460 | F1=0.434 | AUC=0.660 | Time=16.55s\n",
            "Epoch 047 | Loss=0.8987 | TestAcc=0.463 | F1=0.442 | AUC=0.658 | Time=16.91s\n",
            "Epoch 048 | Loss=0.8804 | TestAcc=0.457 | F1=0.419 | AUC=0.658 | Time=17.24s\n",
            "Epoch 049 | Loss=0.9143 | TestAcc=0.467 | F1=0.439 | AUC=0.652 | Time=17.58s\n",
            "Epoch 050 | Loss=0.9160 | TestAcc=0.473 | F1=0.445 | AUC=0.669 | Time=17.93s\n",
            "Epoch 051 | Loss=0.8827 | TestAcc=0.477 | F1=0.453 | AUC=0.646 | Time=18.27s\n",
            "Epoch 052 | Loss=0.8917 | TestAcc=0.480 | F1=0.450 | AUC=0.650 | Time=18.60s\n",
            "Epoch 053 | Loss=0.8782 | TestAcc=0.460 | F1=0.442 | AUC=0.660 | Time=18.95s\n",
            "Epoch 054 | Loss=0.8868 | TestAcc=0.477 | F1=0.449 | AUC=0.659 | Time=19.28s\n",
            "Epoch 055 | Loss=0.8820 | TestAcc=0.473 | F1=0.451 | AUC=0.659 | Time=19.61s\n",
            "Epoch 056 | Loss=0.8882 | TestAcc=0.470 | F1=0.442 | AUC=0.676 | Time=19.96s\n",
            "Epoch 057 | Loss=0.8862 | TestAcc=0.480 | F1=0.465 | AUC=0.666 | Time=20.29s\n",
            "Epoch 058 | Loss=0.8680 | TestAcc=0.483 | F1=0.473 | AUC=0.670 | Time=20.62s\n",
            "Epoch 059 | Loss=0.8802 | TestAcc=0.503 | F1=0.487 | AUC=0.678 | Time=20.97s\n",
            "Epoch 060 | Loss=0.8749 | TestAcc=0.473 | F1=0.439 | AUC=0.674 | Time=21.31s\n",
            "Epoch 061 | Loss=0.8681 | TestAcc=0.473 | F1=0.443 | AUC=0.658 | Time=21.75s\n",
            "Epoch 062 | Loss=0.8801 | TestAcc=0.483 | F1=0.471 | AUC=0.666 | Time=22.21s\n",
            "Epoch 063 | Loss=0.8745 | TestAcc=0.487 | F1=0.477 | AUC=0.686 | Time=22.65s\n",
            "Epoch 064 | Loss=0.8807 | TestAcc=0.440 | F1=0.417 | AUC=0.651 | Time=23.73s\n",
            "Epoch 065 | Loss=0.8607 | TestAcc=0.487 | F1=0.464 | AUC=0.675 | Time=24.60s\n",
            "Epoch 066 | Loss=0.8788 | TestAcc=0.480 | F1=0.443 | AUC=0.678 | Time=25.24s\n",
            "Epoch 067 | Loss=0.8818 | TestAcc=0.493 | F1=0.459 | AUC=0.665 | Time=25.69s\n",
            "Epoch 068 | Loss=0.8886 | TestAcc=0.487 | F1=0.464 | AUC=0.680 | Time=26.28s\n",
            "Epoch 069 | Loss=0.8630 | TestAcc=0.483 | F1=0.467 | AUC=0.658 | Time=26.61s\n",
            "Epoch 070 | Loss=0.8793 | TestAcc=0.490 | F1=0.484 | AUC=0.673 | Time=26.95s\n",
            "Epoch 071 | Loss=0.8666 | TestAcc=0.490 | F1=0.475 | AUC=0.662 | Time=27.30s\n",
            "Epoch 072 | Loss=0.8839 | TestAcc=0.487 | F1=0.451 | AUC=0.672 | Time=27.63s\n",
            "Epoch 073 | Loss=0.8563 | TestAcc=0.500 | F1=0.479 | AUC=0.676 | Time=27.98s\n",
            "Epoch 074 | Loss=0.8609 | TestAcc=0.490 | F1=0.461 | AUC=0.656 | Time=28.34s\n",
            "Epoch 075 | Loss=0.8600 | TestAcc=0.497 | F1=0.487 | AUC=0.675 | Time=28.69s\n",
            "Epoch 076 | Loss=0.8463 | TestAcc=0.483 | F1=0.461 | AUC=0.667 | Time=29.03s\n",
            "Epoch 077 | Loss=0.8730 | TestAcc=0.497 | F1=0.476 | AUC=0.675 | Time=29.38s\n",
            "Epoch 078 | Loss=0.8523 | TestAcc=0.497 | F1=0.464 | AUC=0.673 | Time=29.72s\n",
            "Epoch 079 | Loss=0.8404 | TestAcc=0.493 | F1=0.470 | AUC=0.674 | Time=30.06s\n",
            "Epoch 080 | Loss=0.8560 | TestAcc=0.450 | F1=0.417 | AUC=0.659 | Time=30.40s\n",
            "Epoch 081 | Loss=0.8738 | TestAcc=0.473 | F1=0.451 | AUC=0.652 | Time=30.75s\n",
            "Epoch 082 | Loss=0.8799 | TestAcc=0.480 | F1=0.452 | AUC=0.664 | Time=31.09s\n",
            "Epoch 083 | Loss=0.8756 | TestAcc=0.467 | F1=0.439 | AUC=0.657 | Time=31.43s\n",
            "Epoch 084 | Loss=0.8583 | TestAcc=0.480 | F1=0.453 | AUC=0.668 | Time=31.77s\n",
            "Epoch 085 | Loss=0.8605 | TestAcc=0.483 | F1=0.443 | AUC=0.672 | Time=32.11s\n",
            "Epoch 086 | Loss=0.8454 | TestAcc=0.510 | F1=0.487 | AUC=0.670 | Time=32.46s\n",
            "Epoch 087 | Loss=0.8619 | TestAcc=0.487 | F1=0.460 | AUC=0.659 | Time=32.80s\n",
            "Epoch 088 | Loss=0.8574 | TestAcc=0.477 | F1=0.424 | AUC=0.642 | Time=33.14s\n",
            "Epoch 089 | Loss=0.8554 | TestAcc=0.487 | F1=0.446 | AUC=0.664 | Time=33.50s\n",
            "Epoch 090 | Loss=0.8806 | TestAcc=0.487 | F1=0.471 | AUC=0.681 | Time=33.84s\n",
            "Epoch 091 | Loss=0.8637 | TestAcc=0.470 | F1=0.444 | AUC=0.669 | Time=34.18s\n",
            "Epoch 092 | Loss=0.8600 | TestAcc=0.467 | F1=0.430 | AUC=0.647 | Time=34.52s\n",
            "Epoch 093 | Loss=0.8752 | TestAcc=0.490 | F1=0.456 | AUC=0.683 | Time=34.92s\n",
            "Epoch 094 | Loss=0.8587 | TestAcc=0.463 | F1=0.429 | AUC=0.654 | Time=35.40s\n",
            "Epoch 095 | Loss=0.8511 | TestAcc=0.497 | F1=0.478 | AUC=0.668 | Time=35.84s\n",
            "Epoch 096 | Loss=0.8463 | TestAcc=0.493 | F1=0.469 | AUC=0.672 | Time=36.30s\n",
            "Epoch 097 | Loss=0.8426 | TestAcc=0.490 | F1=0.456 | AUC=0.678 | Time=36.76s\n",
            "Epoch 098 | Loss=0.8435 | TestAcc=0.467 | F1=0.447 | AUC=0.662 | Time=37.25s\n",
            "Epoch 099 | Loss=0.8383 | TestAcc=0.470 | F1=0.467 | AUC=0.651 | Time=37.78s\n",
            "Epoch 100 | Loss=0.8559 | TestAcc=0.473 | F1=0.440 | AUC=0.643 | Time=38.12s\n",
            "Epoch 101 | Loss=0.8444 | TestAcc=0.473 | F1=0.442 | AUC=0.669 | Time=38.48s\n",
            "Epoch 102 | Loss=0.8436 | TestAcc=0.477 | F1=0.459 | AUC=0.662 | Time=38.83s\n",
            "Epoch 103 | Loss=0.8320 | TestAcc=0.500 | F1=0.488 | AUC=0.659 | Time=39.17s\n",
            "Epoch 104 | Loss=0.8479 | TestAcc=0.480 | F1=0.452 | AUC=0.645 | Time=39.50s\n",
            "Epoch 105 | Loss=0.8265 | TestAcc=0.487 | F1=0.456 | AUC=0.658 | Time=39.85s\n",
            "Epoch 106 | Loss=0.8422 | TestAcc=0.487 | F1=0.435 | AUC=0.657 | Time=40.18s\n",
            "Epoch 107 | Loss=0.8468 | TestAcc=0.473 | F1=0.430 | AUC=0.662 | Time=40.52s\n",
            "Epoch 108 | Loss=0.8298 | TestAcc=0.463 | F1=0.429 | AUC=0.651 | Time=40.87s\n",
            "Epoch 109 | Loss=0.8349 | TestAcc=0.490 | F1=0.479 | AUC=0.676 | Time=41.20s\n",
            "Epoch 110 | Loss=0.8350 | TestAcc=0.477 | F1=0.441 | AUC=0.675 | Time=41.54s\n",
            "Epoch 111 | Loss=0.8157 | TestAcc=0.483 | F1=0.449 | AUC=0.673 | Time=41.89s\n",
            "Epoch 112 | Loss=0.8260 | TestAcc=0.487 | F1=0.460 | AUC=0.666 | Time=42.23s\n",
            "Epoch 113 | Loss=0.8297 | TestAcc=0.487 | F1=0.454 | AUC=0.670 | Time=42.56s\n",
            "Epoch 114 | Loss=0.8393 | TestAcc=0.500 | F1=0.479 | AUC=0.673 | Time=42.92s\n",
            "Epoch 115 | Loss=0.8396 | TestAcc=0.477 | F1=0.441 | AUC=0.675 | Time=43.26s\n",
            "Epoch 116 | Loss=0.8325 | TestAcc=0.483 | F1=0.457 | AUC=0.666 | Time=43.59s\n",
            "Epoch 117 | Loss=0.8131 | TestAcc=0.467 | F1=0.429 | AUC=0.648 | Time=43.95s\n",
            "Epoch 118 | Loss=0.8448 | TestAcc=0.497 | F1=0.448 | AUC=0.687 | Time=44.28s\n",
            "Epoch 119 | Loss=0.8314 | TestAcc=0.473 | F1=0.447 | AUC=0.667 | Time=44.62s\n",
            "Epoch 120 | Loss=0.8453 | TestAcc=0.497 | F1=0.462 | AUC=0.675 | Time=44.97s\n",
            "Epoch 121 | Loss=0.8389 | TestAcc=0.480 | F1=0.452 | AUC=0.660 | Time=45.30s\n",
            "Epoch 122 | Loss=0.8408 | TestAcc=0.490 | F1=0.465 | AUC=0.679 | Time=45.63s\n",
            "Epoch 123 | Loss=0.8355 | TestAcc=0.507 | F1=0.483 | AUC=0.683 | Time=46.00s\n",
            "Epoch 124 | Loss=0.8407 | TestAcc=0.497 | F1=0.471 | AUC=0.673 | Time=46.34s\n",
            "Epoch 125 | Loss=0.8288 | TestAcc=0.493 | F1=0.461 | AUC=0.671 | Time=46.67s\n",
            "Epoch 126 | Loss=0.8252 | TestAcc=0.483 | F1=0.451 | AUC=0.670 | Time=47.02s\n",
            "Epoch 127 | Loss=0.8270 | TestAcc=0.483 | F1=0.453 | AUC=0.672 | Time=47.36s\n",
            "Epoch 128 | Loss=0.8330 | TestAcc=0.490 | F1=0.459 | AUC=0.661 | Time=47.70s\n",
            "Epoch 129 | Loss=0.8310 | TestAcc=0.477 | F1=0.448 | AUC=0.661 | Time=48.16s\n",
            "Epoch 130 | Loss=0.8319 | TestAcc=0.473 | F1=0.443 | AUC=0.670 | Time=48.61s\n",
            "Epoch 131 | Loss=0.8195 | TestAcc=0.480 | F1=0.444 | AUC=0.659 | Time=49.06s\n",
            "Epoch 132 | Loss=0.8622 | TestAcc=0.493 | F1=0.461 | AUC=0.686 | Time=49.52s\n",
            "Epoch 133 | Loss=0.8412 | TestAcc=0.487 | F1=0.473 | AUC=0.650 | Time=49.97s\n",
            "Epoch 134 | Loss=0.8414 | TestAcc=0.497 | F1=0.451 | AUC=0.671 | Time=50.49s\n",
            "Epoch 135 | Loss=0.8318 | TestAcc=0.493 | F1=0.468 | AUC=0.670 | Time=50.92s\n",
            "Epoch 136 | Loss=0.8453 | TestAcc=0.497 | F1=0.482 | AUC=0.660 | Time=51.27s\n",
            "Epoch 137 | Loss=0.8582 | TestAcc=0.480 | F1=0.450 | AUC=0.667 | Time=51.60s\n",
            "Epoch 138 | Loss=0.8383 | TestAcc=0.480 | F1=0.443 | AUC=0.654 | Time=51.94s\n",
            "Epoch 139 | Loss=0.8275 | TestAcc=0.507 | F1=0.468 | AUC=0.670 | Time=52.29s\n",
            "Epoch 140 | Loss=0.8448 | TestAcc=0.500 | F1=0.460 | AUC=0.671 | Time=52.63s\n",
            "Epoch 141 | Loss=0.8329 | TestAcc=0.493 | F1=0.457 | AUC=0.674 | Time=52.99s\n",
            "Epoch 142 | Loss=0.8191 | TestAcc=0.487 | F1=0.454 | AUC=0.650 | Time=53.35s\n",
            "Epoch 143 | Loss=0.8203 | TestAcc=0.500 | F1=0.473 | AUC=0.661 | Time=53.68s\n",
            "Epoch 144 | Loss=0.8323 | TestAcc=0.487 | F1=0.446 | AUC=0.679 | Time=54.06s\n",
            "Epoch 145 | Loss=0.8193 | TestAcc=0.457 | F1=0.430 | AUC=0.670 | Time=54.41s\n",
            "Epoch 146 | Loss=0.8258 | TestAcc=0.463 | F1=0.436 | AUC=0.679 | Time=54.76s\n",
            "Epoch 147 | Loss=0.8161 | TestAcc=0.490 | F1=0.458 | AUC=0.675 | Time=55.10s\n",
            "Epoch 148 | Loss=0.8159 | TestAcc=0.500 | F1=0.471 | AUC=0.672 | Time=55.45s\n",
            "Epoch 149 | Loss=0.8209 | TestAcc=0.493 | F1=0.470 | AUC=0.661 | Time=55.80s\n",
            "Epoch 150 | Loss=0.8247 | TestAcc=0.480 | F1=0.463 | AUC=0.662 | Time=56.14s\n",
            "Epoch 151 | Loss=0.8320 | TestAcc=0.497 | F1=0.468 | AUC=0.664 | Time=56.49s\n",
            "Epoch 152 | Loss=0.8188 | TestAcc=0.483 | F1=0.444 | AUC=0.669 | Time=56.84s\n",
            "Epoch 153 | Loss=0.8259 | TestAcc=0.497 | F1=0.474 | AUC=0.671 | Time=57.17s\n",
            "Epoch 154 | Loss=0.8178 | TestAcc=0.473 | F1=0.430 | AUC=0.660 | Time=57.53s\n",
            "Epoch 155 | Loss=0.8248 | TestAcc=0.507 | F1=0.489 | AUC=0.665 | Time=57.87s\n",
            "Epoch 156 | Loss=0.8224 | TestAcc=0.510 | F1=0.490 | AUC=0.680 | Time=58.20s\n",
            "Epoch 157 | Loss=0.8239 | TestAcc=0.510 | F1=0.491 | AUC=0.669 | Time=58.55s\n",
            "Epoch 158 | Loss=0.8137 | TestAcc=0.497 | F1=0.484 | AUC=0.664 | Time=58.90s\n",
            "Epoch 159 | Loss=0.8406 | TestAcc=0.497 | F1=0.466 | AUC=0.657 | Time=59.24s\n",
            "Epoch 160 | Loss=0.8175 | TestAcc=0.493 | F1=0.470 | AUC=0.670 | Time=59.59s\n",
            "Epoch 161 | Loss=0.8142 | TestAcc=0.503 | F1=0.474 | AUC=0.685 | Time=59.93s\n",
            "Epoch 162 | Loss=0.8106 | TestAcc=0.487 | F1=0.449 | AUC=0.661 | Time=60.27s\n",
            "Epoch 163 | Loss=0.8012 | TestAcc=0.513 | F1=0.495 | AUC=0.678 | Time=60.62s\n",
            "Epoch 164 | Loss=0.8113 | TestAcc=0.490 | F1=0.463 | AUC=0.668 | Time=61.07s\n",
            "Epoch 165 | Loss=0.8259 | TestAcc=0.483 | F1=0.457 | AUC=0.666 | Time=61.53s\n",
            "Epoch 166 | Loss=0.8180 | TestAcc=0.470 | F1=0.448 | AUC=0.661 | Time=61.96s\n",
            "Epoch 167 | Loss=0.8093 | TestAcc=0.493 | F1=0.462 | AUC=0.671 | Time=62.41s\n",
            "Epoch 168 | Loss=0.8175 | TestAcc=0.470 | F1=0.438 | AUC=0.637 | Time=62.85s\n",
            "Epoch 169 | Loss=0.8281 | TestAcc=0.477 | F1=0.448 | AUC=0.653 | Time=63.36s\n",
            "Epoch 170 | Loss=0.7989 | TestAcc=0.483 | F1=0.454 | AUC=0.646 | Time=63.86s\n",
            "Epoch 171 | Loss=0.8129 | TestAcc=0.467 | F1=0.437 | AUC=0.667 | Time=64.19s\n",
            "Epoch 172 | Loss=0.8026 | TestAcc=0.467 | F1=0.449 | AUC=0.651 | Time=64.53s\n",
            "Epoch 173 | Loss=0.8058 | TestAcc=0.487 | F1=0.466 | AUC=0.663 | Time=64.89s\n",
            "Epoch 174 | Loss=0.8145 | TestAcc=0.497 | F1=0.462 | AUC=0.654 | Time=65.21s\n",
            "Epoch 175 | Loss=0.7965 | TestAcc=0.493 | F1=0.460 | AUC=0.672 | Time=65.55s\n",
            "Epoch 176 | Loss=0.7942 | TestAcc=0.470 | F1=0.448 | AUC=0.650 | Time=65.90s\n",
            "Epoch 177 | Loss=0.8025 | TestAcc=0.487 | F1=0.454 | AUC=0.664 | Time=66.24s\n",
            "Epoch 178 | Loss=0.8101 | TestAcc=0.457 | F1=0.428 | AUC=0.653 | Time=66.59s\n",
            "Epoch 179 | Loss=0.8328 | TestAcc=0.480 | F1=0.453 | AUC=0.651 | Time=66.97s\n",
            "Epoch 180 | Loss=0.8152 | TestAcc=0.470 | F1=0.436 | AUC=0.646 | Time=67.31s\n",
            "Epoch 181 | Loss=0.8078 | TestAcc=0.470 | F1=0.456 | AUC=0.656 | Time=67.65s\n",
            "Epoch 182 | Loss=0.7925 | TestAcc=0.487 | F1=0.475 | AUC=0.651 | Time=67.99s\n",
            "Epoch 183 | Loss=0.8119 | TestAcc=0.473 | F1=0.462 | AUC=0.646 | Time=68.33s\n",
            "Epoch 184 | Loss=0.8403 | TestAcc=0.473 | F1=0.458 | AUC=0.651 | Time=68.66s\n",
            "Epoch 185 | Loss=0.8393 | TestAcc=0.483 | F1=0.455 | AUC=0.654 | Time=69.02s\n",
            "Epoch 186 | Loss=0.8260 | TestAcc=0.493 | F1=0.481 | AUC=0.648 | Time=69.36s\n",
            "Epoch 187 | Loss=0.8068 | TestAcc=0.483 | F1=0.449 | AUC=0.656 | Time=69.69s\n",
            "Epoch 188 | Loss=0.7971 | TestAcc=0.490 | F1=0.472 | AUC=0.657 | Time=70.04s\n",
            "Epoch 189 | Loss=0.8242 | TestAcc=0.483 | F1=0.446 | AUC=0.658 | Time=70.38s\n",
            "Epoch 190 | Loss=0.8171 | TestAcc=0.473 | F1=0.451 | AUC=0.638 | Time=70.71s\n",
            "Epoch 191 | Loss=0.8143 | TestAcc=0.497 | F1=0.482 | AUC=0.661 | Time=71.06s\n",
            "Epoch 192 | Loss=0.7904 | TestAcc=0.497 | F1=0.482 | AUC=0.661 | Time=71.39s\n",
            "Epoch 193 | Loss=0.7993 | TestAcc=0.500 | F1=0.478 | AUC=0.671 | Time=71.73s\n",
            "Epoch 194 | Loss=0.8240 | TestAcc=0.460 | F1=0.426 | AUC=0.664 | Time=72.07s\n",
            "Epoch 195 | Loss=0.8242 | TestAcc=0.437 | F1=0.430 | AUC=0.640 | Time=72.41s\n",
            "Epoch 196 | Loss=0.8268 | TestAcc=0.477 | F1=0.452 | AUC=0.657 | Time=72.74s\n",
            "Epoch 197 | Loss=0.8055 | TestAcc=0.487 | F1=0.468 | AUC=0.662 | Time=73.09s\n",
            "Epoch 198 | Loss=0.8739 | TestAcc=0.463 | F1=0.430 | AUC=0.652 | Time=73.43s\n",
            "Epoch 199 | Loss=0.8543 | TestAcc=0.497 | F1=0.474 | AUC=0.680 | Time=73.77s\n",
            "Epoch 200 | Loss=0.8337 | TestAcc=0.480 | F1=0.452 | AUC=0.661 | Time=74.25s\n",
            "\n",
            "Training summary stored in : /content/drive/MyDrive/InformationSystems/Classification/results/classification/gin.csv\n",
            "  method  seed     dataset optimization_enabled  embedding_dimension  \\\n",
            "0    GIN    43  IMDB-MULTI                  yes                   64   \n",
            "\n",
            "  objective_weights  num_layers   dropout        lr  weight_decay  epochs  \\\n",
            "0     (0.5,0.3,0.2)           3  0.034584  0.002228      0.000022     200   \n",
            "\n",
            "   best_epoch  best_loss  eval_loss  eval_acc  eval_f1  eval_auc  \\\n",
            "0          23     0.9742     0.9742    0.5133   0.5086    0.7035   \n",
            "\n",
            "   training_time (s)  generation_time (s)  memory_usage (MB)  \n",
            "0              74.25                21.29             1454.1  \n",
            "Saved model: /content/drive/MyDrive/InformationSystems/Classification/models/GIN_IMDB-MULTI_43.pth\n",
            "Dataset has no node features. Applying OneHotDegree transform...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:09:05,783] A new study created in memory with name: no-name-97ed628f-4d24-462b-8598-0ffbeb3d3f55\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset IMDB-MULTI: 1500 graphs, 89 node features, 3 classes\n",
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:09:08,760] Trial 0 finished with value: 0.5062039376314024 and parameters: {'num_layers': 6, 'dropout': 0.591489513435158, 'lr': 0.001150095660260473, 'weight_decay': 1.3470147161614586e-05}. Best is trial 0 with value: 0.5062039376314024.\n",
            "[I 2026-01-22 11:09:10,824] Trial 1 finished with value: 0.370913168003421 and parameters: {'num_layers': 5, 'dropout': 0.39820640180811545, 'lr': 0.00023503849378548862, 'weight_decay': 5.031665287942841e-06}. Best is trial 0 with value: 0.5062039376314024.\n",
            "[I 2026-01-22 11:09:12,853] Trial 2 finished with value: 0.3797934291852762 and parameters: {'num_layers': 5, 'dropout': 0.23676285581071474, 'lr': 0.0016124852504831152, 'weight_decay': 0.00048031176219663185}. Best is trial 0 with value: 0.5062039376314024.\n",
            "[I 2026-01-22 11:09:14,929] Trial 3 finished with value: 0.4717785734544052 and parameters: {'num_layers': 5, 'dropout': 0.028507213486510707, 'lr': 0.004158819592850438, 'weight_decay': 0.00011788356272845542}. Best is trial 0 with value: 0.5062039376314024.\n",
            "[I 2026-01-22 11:09:16,943] Trial 4 finished with value: 0.3684965252680181 and parameters: {'num_layers': 5, 'dropout': 0.3719450718089349, 'lr': 0.0054502819282647545, 'weight_decay': 6.049964772702123e-05}. Best is trial 0 with value: 0.5062039376314024.\n",
            "[I 2026-01-22 11:09:19,305] Trial 5 finished with value: 0.39021511213245247 and parameters: {'num_layers': 4, 'dropout': 0.5410945522054478, 'lr': 0.00033955097668054193, 'weight_decay': 6.827296798600185e-06}. Best is trial 0 with value: 0.5062039376314024.\n",
            "[I 2026-01-22 11:09:21,492] Trial 6 finished with value: 0.42604608559032797 and parameters: {'num_layers': 4, 'dropout': 0.13341703187873652, 'lr': 0.000377791007260343, 'weight_decay': 7.740942955835445e-06}. Best is trial 0 with value: 0.5062039376314024.\n",
            "[I 2026-01-22 11:09:23,551] Trial 7 finished with value: 0.411292304761558 and parameters: {'num_layers': 4, 'dropout': 0.17083358719210998, 'lr': 0.0001830174315009373, 'weight_decay': 3.6352891935781646e-06}. Best is trial 0 with value: 0.5062039376314024.\n",
            "[I 2026-01-22 11:09:25,738] Trial 8 finished with value: 0.48613128940595624 and parameters: {'num_layers': 6, 'dropout': 0.006113923471003546, 'lr': 0.002769769792190768, 'weight_decay': 9.073725996300423e-06}. Best is trial 0 with value: 0.5062039376314024.\n",
            "[I 2026-01-22 11:09:27,411] Trial 9 finished with value: 0.47181161060727644 and parameters: {'num_layers': 3, 'dropout': 0.3266123681100302, 'lr': 0.0007230894611479676, 'weight_decay': 1.8068874802331049e-06}. Best is trial 0 with value: 0.5062039376314024.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'num_layers': 6, 'dropout': 0.591489513435158, 'lr': 0.001150095660260473, 'weight_decay': 1.3470147161614586e-05}\n",
            "\n",
            "Running final training GIN...\n",
            "{'num_layers': 6, 'dropout': 0.591489513435158, 'lr': 0.001150095660260473, 'weight_decay': 1.3470147161614586e-05}\n",
            "Epoch 001 | Loss=3.0904 | TestAcc=0.457 | F1=0.436 | AUC=0.609 | Time=0.45s\n",
            "Epoch 002 | Loss=2.1186 | TestAcc=0.400 | F1=0.360 | AUC=0.582 | Time=0.91s\n",
            "Epoch 003 | Loss=1.6685 | TestAcc=0.333 | F1=0.268 | AUC=0.558 | Time=1.36s\n",
            "Epoch 004 | Loss=1.4748 | TestAcc=0.383 | F1=0.332 | AUC=0.605 | Time=1.84s\n",
            "Epoch 005 | Loss=1.2845 | TestAcc=0.400 | F1=0.392 | AUC=0.587 | Time=2.29s\n",
            "Epoch 006 | Loss=1.2112 | TestAcc=0.407 | F1=0.376 | AUC=0.596 | Time=2.81s\n",
            "Epoch 007 | Loss=1.2262 | TestAcc=0.440 | F1=0.422 | AUC=0.646 | Time=3.41s\n",
            "Epoch 008 | Loss=1.1405 | TestAcc=0.467 | F1=0.454 | AUC=0.625 | Time=4.02s\n",
            "Epoch 009 | Loss=1.1073 | TestAcc=0.430 | F1=0.402 | AUC=0.660 | Time=4.61s\n",
            "Epoch 010 | Loss=1.1030 | TestAcc=0.437 | F1=0.411 | AUC=0.652 | Time=5.28s\n",
            "Epoch 011 | Loss=1.0755 | TestAcc=0.477 | F1=0.465 | AUC=0.689 | Time=5.85s\n",
            "Epoch 012 | Loss=1.1075 | TestAcc=0.483 | F1=0.445 | AUC=0.659 | Time=6.31s\n",
            "Epoch 013 | Loss=1.0508 | TestAcc=0.537 | F1=0.517 | AUC=0.693 | Time=6.78s\n",
            "Epoch 014 | Loss=1.0604 | TestAcc=0.480 | F1=0.447 | AUC=0.680 | Time=7.24s\n",
            "Epoch 015 | Loss=1.0452 | TestAcc=0.477 | F1=0.447 | AUC=0.666 | Time=7.68s\n",
            "Epoch 016 | Loss=1.0524 | TestAcc=0.450 | F1=0.420 | AUC=0.655 | Time=8.14s\n",
            "Epoch 017 | Loss=1.0543 | TestAcc=0.473 | F1=0.448 | AUC=0.657 | Time=8.58s\n",
            "Epoch 018 | Loss=1.0391 | TestAcc=0.493 | F1=0.474 | AUC=0.673 | Time=9.04s\n",
            "Epoch 019 | Loss=1.0349 | TestAcc=0.460 | F1=0.440 | AUC=0.674 | Time=9.49s\n",
            "Epoch 020 | Loss=1.0489 | TestAcc=0.500 | F1=0.481 | AUC=0.665 | Time=9.94s\n",
            "Epoch 021 | Loss=1.0525 | TestAcc=0.460 | F1=0.448 | AUC=0.642 | Time=10.41s\n",
            "Epoch 022 | Loss=1.0502 | TestAcc=0.490 | F1=0.443 | AUC=0.677 | Time=10.87s\n",
            "Epoch 023 | Loss=1.0467 | TestAcc=0.487 | F1=0.457 | AUC=0.664 | Time=11.32s\n",
            "Epoch 024 | Loss=1.0353 | TestAcc=0.460 | F1=0.460 | AUC=0.679 | Time=11.78s\n",
            "Epoch 025 | Loss=1.0566 | TestAcc=0.503 | F1=0.503 | AUC=0.703 | Time=12.24s\n",
            "Epoch 026 | Loss=1.0494 | TestAcc=0.513 | F1=0.488 | AUC=0.703 | Time=12.69s\n",
            "Epoch 027 | Loss=1.0439 | TestAcc=0.517 | F1=0.503 | AUC=0.695 | Time=13.14s\n",
            "Epoch 028 | Loss=1.0305 | TestAcc=0.513 | F1=0.496 | AUC=0.693 | Time=13.60s\n",
            "Epoch 029 | Loss=1.0467 | TestAcc=0.513 | F1=0.500 | AUC=0.689 | Time=14.04s\n",
            "Epoch 030 | Loss=1.0448 | TestAcc=0.493 | F1=0.495 | AUC=0.694 | Time=14.51s\n",
            "Epoch 031 | Loss=1.0390 | TestAcc=0.517 | F1=0.511 | AUC=0.700 | Time=14.96s\n",
            "Epoch 032 | Loss=1.0469 | TestAcc=0.477 | F1=0.455 | AUC=0.672 | Time=15.42s\n",
            "Epoch 033 | Loss=1.0391 | TestAcc=0.493 | F1=0.491 | AUC=0.683 | Time=15.99s\n",
            "Epoch 034 | Loss=1.0242 | TestAcc=0.497 | F1=0.440 | AUC=0.683 | Time=16.62s\n",
            "Epoch 035 | Loss=1.0416 | TestAcc=0.473 | F1=0.470 | AUC=0.675 | Time=17.21s\n",
            "Epoch 036 | Loss=1.0213 | TestAcc=0.520 | F1=0.487 | AUC=0.704 | Time=17.81s\n",
            "Epoch 037 | Loss=1.0245 | TestAcc=0.527 | F1=0.495 | AUC=0.695 | Time=18.51s\n",
            "Epoch 038 | Loss=1.0312 | TestAcc=0.483 | F1=0.475 | AUC=0.684 | Time=19.02s\n",
            "Epoch 039 | Loss=1.0374 | TestAcc=0.480 | F1=0.476 | AUC=0.687 | Time=19.48s\n",
            "Epoch 040 | Loss=1.0246 | TestAcc=0.530 | F1=0.520 | AUC=0.694 | Time=19.94s\n",
            "Epoch 041 | Loss=1.0337 | TestAcc=0.517 | F1=0.495 | AUC=0.676 | Time=20.38s\n",
            "Epoch 042 | Loss=1.0264 | TestAcc=0.483 | F1=0.475 | AUC=0.668 | Time=20.86s\n",
            "Epoch 043 | Loss=1.0397 | TestAcc=0.503 | F1=0.491 | AUC=0.681 | Time=21.30s\n",
            "Epoch 044 | Loss=1.0295 | TestAcc=0.523 | F1=0.524 | AUC=0.709 | Time=21.78s\n",
            "Epoch 045 | Loss=1.0279 | TestAcc=0.490 | F1=0.482 | AUC=0.683 | Time=22.24s\n",
            "Epoch 046 | Loss=1.0106 | TestAcc=0.520 | F1=0.508 | AUC=0.696 | Time=22.70s\n",
            "Epoch 047 | Loss=1.0307 | TestAcc=0.510 | F1=0.488 | AUC=0.673 | Time=23.15s\n",
            "Epoch 048 | Loss=1.0219 | TestAcc=0.507 | F1=0.508 | AUC=0.679 | Time=23.60s\n",
            "Epoch 049 | Loss=1.0073 | TestAcc=0.503 | F1=0.485 | AUC=0.682 | Time=24.06s\n",
            "Epoch 050 | Loss=1.0333 | TestAcc=0.510 | F1=0.488 | AUC=0.698 | Time=24.51s\n",
            "Epoch 051 | Loss=1.0058 | TestAcc=0.510 | F1=0.504 | AUC=0.693 | Time=24.97s\n",
            "Epoch 052 | Loss=1.0266 | TestAcc=0.497 | F1=0.487 | AUC=0.675 | Time=25.41s\n",
            "Epoch 053 | Loss=1.0310 | TestAcc=0.500 | F1=0.474 | AUC=0.689 | Time=25.88s\n",
            "Epoch 054 | Loss=1.0188 | TestAcc=0.493 | F1=0.497 | AUC=0.702 | Time=26.33s\n",
            "Epoch 055 | Loss=1.0198 | TestAcc=0.513 | F1=0.491 | AUC=0.687 | Time=26.80s\n",
            "Epoch 056 | Loss=1.0129 | TestAcc=0.470 | F1=0.471 | AUC=0.677 | Time=27.24s\n",
            "Epoch 057 | Loss=1.0196 | TestAcc=0.473 | F1=0.454 | AUC=0.660 | Time=27.69s\n",
            "Epoch 058 | Loss=1.0057 | TestAcc=0.480 | F1=0.478 | AUC=0.690 | Time=28.16s\n",
            "Epoch 059 | Loss=1.0138 | TestAcc=0.477 | F1=0.475 | AUC=0.680 | Time=28.61s\n",
            "Epoch 060 | Loss=1.0233 | TestAcc=0.500 | F1=0.487 | AUC=0.688 | Time=29.23s\n",
            "Epoch 061 | Loss=1.0030 | TestAcc=0.463 | F1=0.442 | AUC=0.680 | Time=29.83s\n",
            "Epoch 062 | Loss=1.0173 | TestAcc=0.513 | F1=0.485 | AUC=0.687 | Time=30.42s\n",
            "Epoch 063 | Loss=1.0198 | TestAcc=0.503 | F1=0.503 | AUC=0.686 | Time=31.07s\n",
            "Epoch 064 | Loss=1.0027 | TestAcc=0.503 | F1=0.483 | AUC=0.660 | Time=31.74s\n",
            "Epoch 065 | Loss=1.0031 | TestAcc=0.507 | F1=0.503 | AUC=0.695 | Time=32.23s\n",
            "Epoch 066 | Loss=1.0042 | TestAcc=0.520 | F1=0.504 | AUC=0.689 | Time=32.68s\n",
            "Epoch 067 | Loss=1.0161 | TestAcc=0.483 | F1=0.453 | AUC=0.651 | Time=33.15s\n",
            "Epoch 068 | Loss=1.0272 | TestAcc=0.463 | F1=0.439 | AUC=0.647 | Time=33.60s\n",
            "Epoch 069 | Loss=1.0040 | TestAcc=0.413 | F1=0.410 | AUC=0.657 | Time=34.07s\n",
            "Epoch 070 | Loss=1.0323 | TestAcc=0.500 | F1=0.479 | AUC=0.683 | Time=34.52s\n",
            "Epoch 071 | Loss=1.0157 | TestAcc=0.497 | F1=0.466 | AUC=0.657 | Time=34.97s\n",
            "Epoch 072 | Loss=0.9961 | TestAcc=0.480 | F1=0.455 | AUC=0.677 | Time=35.45s\n",
            "Epoch 073 | Loss=0.9943 | TestAcc=0.423 | F1=0.390 | AUC=0.694 | Time=35.90s\n",
            "Epoch 074 | Loss=0.9931 | TestAcc=0.520 | F1=0.521 | AUC=0.688 | Time=36.37s\n",
            "Epoch 075 | Loss=1.0042 | TestAcc=0.483 | F1=0.448 | AUC=0.660 | Time=36.83s\n",
            "Epoch 076 | Loss=1.0096 | TestAcc=0.510 | F1=0.496 | AUC=0.684 | Time=37.29s\n",
            "Epoch 077 | Loss=1.0094 | TestAcc=0.503 | F1=0.477 | AUC=0.667 | Time=37.74s\n",
            "Epoch 078 | Loss=1.0003 | TestAcc=0.483 | F1=0.457 | AUC=0.677 | Time=38.20s\n",
            "Epoch 079 | Loss=0.9998 | TestAcc=0.513 | F1=0.482 | AUC=0.694 | Time=38.72s\n",
            "Epoch 080 | Loss=0.9857 | TestAcc=0.517 | F1=0.477 | AUC=0.671 | Time=39.18s\n",
            "Epoch 081 | Loss=0.9974 | TestAcc=0.520 | F1=0.511 | AUC=0.682 | Time=39.63s\n",
            "Epoch 082 | Loss=1.0124 | TestAcc=0.543 | F1=0.527 | AUC=0.687 | Time=40.08s\n",
            "Epoch 083 | Loss=0.9899 | TestAcc=0.503 | F1=0.487 | AUC=0.673 | Time=40.54s\n",
            "Epoch 084 | Loss=1.0098 | TestAcc=0.493 | F1=0.493 | AUC=0.671 | Time=40.99s\n",
            "Epoch 085 | Loss=1.0104 | TestAcc=0.437 | F1=0.437 | AUC=0.648 | Time=41.46s\n",
            "Epoch 086 | Loss=1.0008 | TestAcc=0.483 | F1=0.447 | AUC=0.658 | Time=42.03s\n",
            "Epoch 087 | Loss=1.0092 | TestAcc=0.520 | F1=0.511 | AUC=0.687 | Time=42.64s\n",
            "Epoch 088 | Loss=1.0240 | TestAcc=0.507 | F1=0.505 | AUC=0.682 | Time=43.23s\n",
            "Epoch 089 | Loss=1.0027 | TestAcc=0.510 | F1=0.472 | AUC=0.689 | Time=43.84s\n",
            "Epoch 090 | Loss=0.9800 | TestAcc=0.513 | F1=0.494 | AUC=0.680 | Time=44.52s\n",
            "Epoch 091 | Loss=0.9878 | TestAcc=0.493 | F1=0.471 | AUC=0.647 | Time=45.03s\n",
            "Epoch 092 | Loss=1.0206 | TestAcc=0.443 | F1=0.429 | AUC=0.670 | Time=45.47s\n",
            "Epoch 093 | Loss=1.0175 | TestAcc=0.527 | F1=0.523 | AUC=0.698 | Time=45.94s\n",
            "Epoch 094 | Loss=0.9977 | TestAcc=0.527 | F1=0.478 | AUC=0.682 | Time=46.41s\n",
            "Epoch 095 | Loss=0.9904 | TestAcc=0.487 | F1=0.435 | AUC=0.673 | Time=46.88s\n",
            "Epoch 096 | Loss=0.9840 | TestAcc=0.540 | F1=0.528 | AUC=0.694 | Time=47.33s\n",
            "Epoch 097 | Loss=0.9811 | TestAcc=0.493 | F1=0.473 | AUC=0.680 | Time=47.79s\n",
            "Epoch 098 | Loss=0.9756 | TestAcc=0.493 | F1=0.466 | AUC=0.671 | Time=48.23s\n",
            "Epoch 099 | Loss=0.9821 | TestAcc=0.490 | F1=0.450 | AUC=0.646 | Time=48.68s\n",
            "Epoch 100 | Loss=0.9597 | TestAcc=0.490 | F1=0.460 | AUC=0.664 | Time=49.14s\n",
            "Epoch 101 | Loss=0.9862 | TestAcc=0.503 | F1=0.482 | AUC=0.698 | Time=49.58s\n",
            "Epoch 102 | Loss=0.9651 | TestAcc=0.517 | F1=0.509 | AUC=0.685 | Time=50.05s\n",
            "Epoch 103 | Loss=0.9812 | TestAcc=0.500 | F1=0.467 | AUC=0.696 | Time=50.49s\n",
            "Epoch 104 | Loss=0.9843 | TestAcc=0.500 | F1=0.467 | AUC=0.644 | Time=50.95s\n",
            "Epoch 105 | Loss=0.9990 | TestAcc=0.507 | F1=0.496 | AUC=0.680 | Time=51.42s\n",
            "Epoch 106 | Loss=0.9820 | TestAcc=0.513 | F1=0.493 | AUC=0.657 | Time=51.89s\n",
            "Epoch 107 | Loss=0.9939 | TestAcc=0.500 | F1=0.475 | AUC=0.676 | Time=52.34s\n",
            "Epoch 108 | Loss=0.9784 | TestAcc=0.500 | F1=0.475 | AUC=0.681 | Time=52.80s\n",
            "Epoch 109 | Loss=0.9922 | TestAcc=0.503 | F1=0.490 | AUC=0.686 | Time=53.25s\n",
            "Epoch 110 | Loss=0.9638 | TestAcc=0.493 | F1=0.452 | AUC=0.679 | Time=53.70s\n",
            "Epoch 111 | Loss=0.9758 | TestAcc=0.530 | F1=0.517 | AUC=0.694 | Time=54.16s\n",
            "Epoch 112 | Loss=0.9827 | TestAcc=0.497 | F1=0.458 | AUC=0.680 | Time=54.60s\n",
            "Epoch 113 | Loss=0.9553 | TestAcc=0.490 | F1=0.478 | AUC=0.669 | Time=55.24s\n",
            "Epoch 114 | Loss=0.9764 | TestAcc=0.523 | F1=0.510 | AUC=0.686 | Time=55.82s\n",
            "Epoch 115 | Loss=0.9906 | TestAcc=0.477 | F1=0.468 | AUC=0.678 | Time=56.44s\n",
            "Epoch 116 | Loss=0.9623 | TestAcc=0.460 | F1=0.444 | AUC=0.653 | Time=57.08s\n",
            "Epoch 117 | Loss=0.9592 | TestAcc=0.493 | F1=0.489 | AUC=0.655 | Time=57.72s\n",
            "Epoch 118 | Loss=0.9605 | TestAcc=0.500 | F1=0.494 | AUC=0.676 | Time=58.18s\n",
            "Epoch 119 | Loss=1.0009 | TestAcc=0.510 | F1=0.501 | AUC=0.690 | Time=58.63s\n",
            "Epoch 120 | Loss=0.9821 | TestAcc=0.440 | F1=0.429 | AUC=0.695 | Time=59.11s\n",
            "Epoch 121 | Loss=0.9914 | TestAcc=0.500 | F1=0.468 | AUC=0.647 | Time=59.56s\n",
            "Epoch 122 | Loss=0.9625 | TestAcc=0.520 | F1=0.512 | AUC=0.689 | Time=60.02s\n",
            "Epoch 123 | Loss=0.9679 | TestAcc=0.517 | F1=0.501 | AUC=0.693 | Time=60.47s\n",
            "Epoch 124 | Loss=0.9824 | TestAcc=0.550 | F1=0.541 | AUC=0.707 | Time=60.93s\n",
            "Epoch 125 | Loss=0.9736 | TestAcc=0.540 | F1=0.521 | AUC=0.709 | Time=61.38s\n",
            "Epoch 126 | Loss=0.9862 | TestAcc=0.520 | F1=0.507 | AUC=0.699 | Time=61.86s\n",
            "Epoch 127 | Loss=0.9882 | TestAcc=0.443 | F1=0.398 | AUC=0.626 | Time=62.32s\n",
            "Epoch 128 | Loss=0.9787 | TestAcc=0.550 | F1=0.515 | AUC=0.678 | Time=62.77s\n",
            "Epoch 129 | Loss=1.0067 | TestAcc=0.520 | F1=0.484 | AUC=0.666 | Time=63.22s\n",
            "Epoch 130 | Loss=0.9862 | TestAcc=0.520 | F1=0.507 | AUC=0.687 | Time=63.67s\n",
            "Epoch 131 | Loss=0.9996 | TestAcc=0.500 | F1=0.476 | AUC=0.656 | Time=64.12s\n",
            "Epoch 132 | Loss=0.9820 | TestAcc=0.483 | F1=0.474 | AUC=0.667 | Time=64.57s\n",
            "Epoch 133 | Loss=0.9826 | TestAcc=0.523 | F1=0.489 | AUC=0.683 | Time=65.02s\n",
            "Epoch 134 | Loss=0.9707 | TestAcc=0.523 | F1=0.514 | AUC=0.686 | Time=65.47s\n",
            "Epoch 135 | Loss=0.9928 | TestAcc=0.530 | F1=0.522 | AUC=0.674 | Time=65.93s\n",
            "Epoch 136 | Loss=0.9815 | TestAcc=0.467 | F1=0.462 | AUC=0.676 | Time=66.39s\n",
            "Epoch 137 | Loss=0.9734 | TestAcc=0.493 | F1=0.479 | AUC=0.692 | Time=66.85s\n",
            "Epoch 138 | Loss=0.9757 | TestAcc=0.513 | F1=0.489 | AUC=0.669 | Time=67.32s\n",
            "Epoch 139 | Loss=0.9657 | TestAcc=0.480 | F1=0.447 | AUC=0.646 | Time=67.84s\n",
            "Epoch 140 | Loss=0.9751 | TestAcc=0.530 | F1=0.511 | AUC=0.693 | Time=68.45s\n",
            "Epoch 141 | Loss=0.9941 | TestAcc=0.527 | F1=0.512 | AUC=0.686 | Time=69.03s\n",
            "Epoch 142 | Loss=0.9943 | TestAcc=0.510 | F1=0.485 | AUC=0.682 | Time=69.64s\n",
            "Epoch 143 | Loss=0.9849 | TestAcc=0.537 | F1=0.515 | AUC=0.695 | Time=70.30s\n",
            "Epoch 144 | Loss=0.9673 | TestAcc=0.517 | F1=0.513 | AUC=0.705 | Time=70.88s\n",
            "Epoch 145 | Loss=0.9666 | TestAcc=0.500 | F1=0.480 | AUC=0.691 | Time=71.32s\n",
            "Epoch 146 | Loss=0.9594 | TestAcc=0.490 | F1=0.461 | AUC=0.677 | Time=71.78s\n",
            "Epoch 147 | Loss=0.9855 | TestAcc=0.497 | F1=0.470 | AUC=0.673 | Time=72.23s\n",
            "Epoch 148 | Loss=0.9678 | TestAcc=0.507 | F1=0.483 | AUC=0.673 | Time=72.69s\n",
            "Epoch 149 | Loss=0.9655 | TestAcc=0.530 | F1=0.517 | AUC=0.681 | Time=73.14s\n",
            "Epoch 150 | Loss=0.9782 | TestAcc=0.540 | F1=0.506 | AUC=0.691 | Time=73.60s\n",
            "Epoch 151 | Loss=0.9799 | TestAcc=0.530 | F1=0.496 | AUC=0.696 | Time=74.05s\n",
            "Epoch 152 | Loss=0.9726 | TestAcc=0.507 | F1=0.497 | AUC=0.680 | Time=74.50s\n",
            "Epoch 153 | Loss=0.9683 | TestAcc=0.537 | F1=0.505 | AUC=0.685 | Time=74.96s\n",
            "Epoch 154 | Loss=0.9541 | TestAcc=0.523 | F1=0.513 | AUC=0.686 | Time=75.41s\n",
            "Epoch 155 | Loss=0.9508 | TestAcc=0.490 | F1=0.475 | AUC=0.686 | Time=75.87s\n",
            "Epoch 156 | Loss=0.9684 | TestAcc=0.530 | F1=0.509 | AUC=0.687 | Time=76.35s\n",
            "Epoch 157 | Loss=0.9591 | TestAcc=0.517 | F1=0.501 | AUC=0.673 | Time=76.82s\n",
            "Epoch 158 | Loss=0.9734 | TestAcc=0.523 | F1=0.506 | AUC=0.692 | Time=77.27s\n",
            "Epoch 159 | Loss=0.9595 | TestAcc=0.520 | F1=0.507 | AUC=0.702 | Time=77.73s\n",
            "Epoch 160 | Loss=0.9519 | TestAcc=0.520 | F1=0.498 | AUC=0.685 | Time=78.17s\n",
            "Epoch 161 | Loss=0.9554 | TestAcc=0.527 | F1=0.500 | AUC=0.683 | Time=78.61s\n",
            "Epoch 162 | Loss=0.9774 | TestAcc=0.533 | F1=0.498 | AUC=0.683 | Time=79.08s\n",
            "Epoch 163 | Loss=0.9546 | TestAcc=0.523 | F1=0.489 | AUC=0.665 | Time=79.53s\n",
            "Epoch 164 | Loss=0.9589 | TestAcc=0.537 | F1=0.514 | AUC=0.680 | Time=79.99s\n",
            "Epoch 165 | Loss=0.9405 | TestAcc=0.503 | F1=0.497 | AUC=0.657 | Time=80.44s\n",
            "Epoch 166 | Loss=0.9534 | TestAcc=0.527 | F1=0.502 | AUC=0.687 | Time=81.02s\n",
            "Epoch 167 | Loss=0.9699 | TestAcc=0.533 | F1=0.508 | AUC=0.696 | Time=81.62s\n",
            "Epoch 168 | Loss=0.9498 | TestAcc=0.563 | F1=0.547 | AUC=0.706 | Time=82.25s\n",
            "Epoch 169 | Loss=0.9632 | TestAcc=0.547 | F1=0.537 | AUC=0.699 | Time=82.84s\n",
            "Epoch 170 | Loss=0.9626 | TestAcc=0.523 | F1=0.507 | AUC=0.685 | Time=83.53s\n",
            "Epoch 171 | Loss=0.9378 | TestAcc=0.533 | F1=0.517 | AUC=0.695 | Time=84.04s\n",
            "Epoch 172 | Loss=0.9561 | TestAcc=0.543 | F1=0.516 | AUC=0.686 | Time=84.48s\n",
            "Epoch 173 | Loss=0.9469 | TestAcc=0.533 | F1=0.508 | AUC=0.694 | Time=84.93s\n",
            "Epoch 174 | Loss=0.9401 | TestAcc=0.530 | F1=0.505 | AUC=0.683 | Time=85.39s\n",
            "Epoch 175 | Loss=0.9605 | TestAcc=0.527 | F1=0.510 | AUC=0.710 | Time=85.84s\n",
            "Epoch 176 | Loss=0.9484 | TestAcc=0.540 | F1=0.510 | AUC=0.688 | Time=86.30s\n",
            "Epoch 177 | Loss=0.9662 | TestAcc=0.560 | F1=0.551 | AUC=0.708 | Time=86.75s\n",
            "Epoch 178 | Loss=0.9435 | TestAcc=0.547 | F1=0.540 | AUC=0.708 | Time=87.21s\n",
            "Epoch 179 | Loss=0.9646 | TestAcc=0.553 | F1=0.528 | AUC=0.717 | Time=87.66s\n",
            "Epoch 180 | Loss=0.9529 | TestAcc=0.533 | F1=0.517 | AUC=0.711 | Time=88.11s\n",
            "Epoch 181 | Loss=0.9332 | TestAcc=0.537 | F1=0.519 | AUC=0.706 | Time=88.57s\n",
            "Epoch 182 | Loss=0.9448 | TestAcc=0.533 | F1=0.506 | AUC=0.687 | Time=89.02s\n",
            "Epoch 183 | Loss=0.9453 | TestAcc=0.530 | F1=0.505 | AUC=0.695 | Time=89.47s\n",
            "Epoch 184 | Loss=0.9487 | TestAcc=0.547 | F1=0.525 | AUC=0.707 | Time=89.92s\n",
            "Epoch 185 | Loss=0.9351 | TestAcc=0.527 | F1=0.512 | AUC=0.697 | Time=90.38s\n",
            "Epoch 186 | Loss=0.9534 | TestAcc=0.543 | F1=0.520 | AUC=0.697 | Time=90.83s\n",
            "Epoch 187 | Loss=0.9444 | TestAcc=0.523 | F1=0.495 | AUC=0.680 | Time=91.29s\n",
            "Epoch 188 | Loss=0.9426 | TestAcc=0.537 | F1=0.518 | AUC=0.707 | Time=91.75s\n",
            "Epoch 189 | Loss=0.9321 | TestAcc=0.520 | F1=0.489 | AUC=0.691 | Time=92.20s\n",
            "Epoch 190 | Loss=0.9521 | TestAcc=0.523 | F1=0.496 | AUC=0.684 | Time=92.66s\n",
            "Epoch 191 | Loss=0.9551 | TestAcc=0.533 | F1=0.508 | AUC=0.688 | Time=93.12s\n",
            "Epoch 192 | Loss=0.9465 | TestAcc=0.500 | F1=0.487 | AUC=0.683 | Time=93.58s\n",
            "Epoch 193 | Loss=0.9403 | TestAcc=0.547 | F1=0.533 | AUC=0.692 | Time=94.21s\n",
            "Epoch 194 | Loss=0.9346 | TestAcc=0.503 | F1=0.479 | AUC=0.670 | Time=94.81s\n",
            "Epoch 195 | Loss=0.9436 | TestAcc=0.527 | F1=0.497 | AUC=0.703 | Time=95.42s\n",
            "Epoch 196 | Loss=0.9434 | TestAcc=0.530 | F1=0.486 | AUC=0.700 | Time=96.06s\n",
            "Epoch 197 | Loss=0.9548 | TestAcc=0.540 | F1=0.532 | AUC=0.713 | Time=96.71s\n",
            "Epoch 198 | Loss=0.9517 | TestAcc=0.510 | F1=0.478 | AUC=0.673 | Time=97.17s\n",
            "Epoch 199 | Loss=0.9425 | TestAcc=0.497 | F1=0.468 | AUC=0.666 | Time=97.63s\n",
            "Epoch 200 | Loss=0.9329 | TestAcc=0.520 | F1=0.506 | AUC=0.690 | Time=98.08s\n",
            "\n",
            "Training summary stored in : /content/drive/MyDrive/InformationSystems/Classification/results/classification/gin.csv\n",
            "  method  seed     dataset optimization_enabled  embedding_dimension  \\\n",
            "0    GIN    44  IMDB-MULTI                  yes                   64   \n",
            "\n",
            "  objective_weights  num_layers  dropout       lr  weight_decay  epochs  \\\n",
            "0     (0.5,0.3,0.2)           6  0.59149  0.00115      0.000013     200   \n",
            "\n",
            "   best_epoch  best_loss  eval_loss  eval_acc  eval_f1  eval_auc  \\\n",
            "0         168     1.0186     1.0186    0.5633   0.5471    0.7055   \n",
            "\n",
            "   training_time (s)  generation_time (s)  memory_usage (MB)  \n",
            "0              98.08                21.63            1454.12  \n",
            "Saved model: /content/drive/MyDrive/InformationSystems/Classification/models/GIN_IMDB-MULTI_44.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/ENZYMES.zip\n",
            "Processing...\n",
            "Done!\n",
            "[I 2026-01-22 11:11:08,394] A new study created in memory with name: no-name-c01ed7b0-ed47-412f-9d2f-445fa9bed997\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset ENZYMES: 600 graphs, 3 node features, 6 classes\n",
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:11:08,923] Trial 0 finished with value: 0.266056755852866 and parameters: {'num_layers': 3, 'dropout': 0.0012509835979124472, 'lr': 0.0016911051482734931, 'weight_decay': 5.490794753533939e-05}. Best is trial 0 with value: 0.266056755852866.\n",
            "[I 2026-01-22 11:11:09,563] Trial 1 finished with value: 0.32801101235947683 and parameters: {'num_layers': 5, 'dropout': 0.5526943966387864, 'lr': 0.00019708162333681477, 'weight_decay': 0.0006282574995107843}. Best is trial 1 with value: 0.32801101235947683.\n",
            "[I 2026-01-22 11:11:10,246] Trial 2 finished with value: 0.27975160285759715 and parameters: {'num_layers': 6, 'dropout': 0.2866874205912621, 'lr': 0.003794124942561356, 'weight_decay': 3.0082004170742526e-06}. Best is trial 1 with value: 0.32801101235947683.\n",
            "[I 2026-01-22 11:11:10,910] Trial 3 finished with value: 0.3597272377659318 and parameters: {'num_layers': 5, 'dropout': 0.43930774218574076, 'lr': 0.0027336541202340786, 'weight_decay': 6.896961097292942e-06}. Best is trial 3 with value: 0.3597272377659318.\n",
            "[I 2026-01-22 11:11:11,479] Trial 4 finished with value: 0.45005073987141836 and parameters: {'num_layers': 4, 'dropout': 0.21688801204777006, 'lr': 0.003300874881595221, 'weight_decay': 2.2182896216433786e-06}. Best is trial 4 with value: 0.45005073987141836.\n",
            "[I 2026-01-22 11:11:12,103] Trial 5 finished with value: 0.2702651129995223 and parameters: {'num_layers': 5, 'dropout': 0.4358878078158533, 'lr': 0.0001962451304618388, 'weight_decay': 8.996728467140198e-05}. Best is trial 4 with value: 0.45005073987141836.\n",
            "[I 2026-01-22 11:11:12,736] Trial 6 finished with value: 0.34397375293242183 and parameters: {'num_layers': 5, 'dropout': 0.5831089576719699, 'lr': 0.0012409681141679308, 'weight_decay': 1.3215847123743441e-05}. Best is trial 4 with value: 0.45005073987141836.\n",
            "[I 2026-01-22 11:11:13,219] Trial 7 finished with value: 0.35748881932721865 and parameters: {'num_layers': 3, 'dropout': 0.5318132538224987, 'lr': 0.0038341203966063167, 'weight_decay': 6.4621360071164825e-06}. Best is trial 4 with value: 0.45005073987141836.\n",
            "[I 2026-01-22 11:11:13,917] Trial 8 finished with value: 0.3017356045187499 and parameters: {'num_layers': 6, 'dropout': 0.047726741189370724, 'lr': 0.009219211386137174, 'weight_decay': 5.357131264879793e-06}. Best is trial 4 with value: 0.45005073987141836.\n",
            "[I 2026-01-22 11:11:14,627] Trial 9 finished with value: 0.3398548804244982 and parameters: {'num_layers': 4, 'dropout': 0.5121288004697052, 'lr': 0.005413479323971575, 'weight_decay': 1.0423880351057324e-06}. Best is trial 4 with value: 0.45005073987141836.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'num_layers': 4, 'dropout': 0.21688801204777006, 'lr': 0.003300874881595221, 'weight_decay': 2.2182896216433786e-06}\n",
            "\n",
            "Running final training GIN...\n",
            "{'num_layers': 4, 'dropout': 0.21688801204777006, 'lr': 0.003300874881595221, 'weight_decay': 2.2182896216433786e-06}\n",
            "Epoch 001 | Loss=4.9039 | TestAcc=0.208 | F1=0.165 | AUC=0.541 | Time=0.17s\n",
            "Epoch 002 | Loss=2.7970 | TestAcc=0.233 | F1=0.180 | AUC=0.582 | Time=0.34s\n",
            "Epoch 003 | Loss=2.4628 | TestAcc=0.208 | F1=0.208 | AUC=0.541 | Time=0.50s\n",
            "Epoch 004 | Loss=2.2206 | TestAcc=0.233 | F1=0.231 | AUC=0.608 | Time=0.67s\n",
            "Epoch 005 | Loss=1.9358 | TestAcc=0.192 | F1=0.177 | AUC=0.569 | Time=0.83s\n",
            "Epoch 006 | Loss=1.9048 | TestAcc=0.258 | F1=0.256 | AUC=0.622 | Time=1.02s\n",
            "Epoch 007 | Loss=1.7849 | TestAcc=0.275 | F1=0.259 | AUC=0.654 | Time=1.19s\n",
            "Epoch 008 | Loss=1.7320 | TestAcc=0.317 | F1=0.298 | AUC=0.681 | Time=1.35s\n",
            "Epoch 009 | Loss=1.6443 | TestAcc=0.325 | F1=0.316 | AUC=0.703 | Time=1.52s\n",
            "Epoch 010 | Loss=1.6540 | TestAcc=0.325 | F1=0.307 | AUC=0.696 | Time=1.69s\n",
            "Epoch 011 | Loss=1.5831 | TestAcc=0.383 | F1=0.350 | AUC=0.722 | Time=1.87s\n",
            "Epoch 012 | Loss=1.6290 | TestAcc=0.358 | F1=0.363 | AUC=0.706 | Time=2.08s\n",
            "Epoch 013 | Loss=1.5309 | TestAcc=0.442 | F1=0.429 | AUC=0.714 | Time=2.26s\n",
            "Epoch 014 | Loss=1.4848 | TestAcc=0.442 | F1=0.440 | AUC=0.754 | Time=2.45s\n",
            "Epoch 015 | Loss=1.5084 | TestAcc=0.350 | F1=0.331 | AUC=0.743 | Time=2.63s\n",
            "Epoch 016 | Loss=1.4580 | TestAcc=0.442 | F1=0.432 | AUC=0.715 | Time=2.76s\n",
            "Epoch 017 | Loss=1.5019 | TestAcc=0.400 | F1=0.394 | AUC=0.733 | Time=2.89s\n",
            "Epoch 018 | Loss=1.4368 | TestAcc=0.408 | F1=0.411 | AUC=0.724 | Time=3.03s\n",
            "Epoch 019 | Loss=1.4287 | TestAcc=0.383 | F1=0.376 | AUC=0.713 | Time=3.15s\n",
            "Epoch 020 | Loss=1.4084 | TestAcc=0.342 | F1=0.341 | AUC=0.694 | Time=3.28s\n",
            "Epoch 021 | Loss=1.3953 | TestAcc=0.383 | F1=0.358 | AUC=0.741 | Time=3.40s\n",
            "Epoch 022 | Loss=1.3773 | TestAcc=0.408 | F1=0.388 | AUC=0.760 | Time=3.54s\n",
            "Epoch 023 | Loss=1.4190 | TestAcc=0.367 | F1=0.365 | AUC=0.725 | Time=3.67s\n",
            "Epoch 024 | Loss=1.4238 | TestAcc=0.467 | F1=0.469 | AUC=0.774 | Time=3.79s\n",
            "Epoch 025 | Loss=1.3880 | TestAcc=0.333 | F1=0.332 | AUC=0.695 | Time=3.92s\n",
            "Epoch 026 | Loss=1.3697 | TestAcc=0.450 | F1=0.449 | AUC=0.766 | Time=4.06s\n",
            "Epoch 027 | Loss=1.3490 | TestAcc=0.425 | F1=0.420 | AUC=0.741 | Time=4.19s\n",
            "Epoch 028 | Loss=1.3268 | TestAcc=0.383 | F1=0.361 | AUC=0.731 | Time=4.31s\n",
            "Epoch 029 | Loss=1.3149 | TestAcc=0.358 | F1=0.367 | AUC=0.765 | Time=4.44s\n",
            "Epoch 030 | Loss=1.2697 | TestAcc=0.358 | F1=0.363 | AUC=0.718 | Time=4.57s\n",
            "Epoch 031 | Loss=1.3258 | TestAcc=0.367 | F1=0.369 | AUC=0.715 | Time=4.69s\n",
            "Epoch 032 | Loss=1.4179 | TestAcc=0.358 | F1=0.357 | AUC=0.725 | Time=4.82s\n",
            "Epoch 033 | Loss=1.2787 | TestAcc=0.383 | F1=0.377 | AUC=0.730 | Time=4.95s\n",
            "Epoch 034 | Loss=1.3254 | TestAcc=0.442 | F1=0.441 | AUC=0.764 | Time=5.09s\n",
            "Epoch 035 | Loss=1.3211 | TestAcc=0.367 | F1=0.340 | AUC=0.718 | Time=5.21s\n",
            "Epoch 036 | Loss=1.3075 | TestAcc=0.375 | F1=0.364 | AUC=0.690 | Time=5.34s\n",
            "Epoch 037 | Loss=1.3741 | TestAcc=0.417 | F1=0.401 | AUC=0.762 | Time=5.46s\n",
            "Epoch 038 | Loss=1.3932 | TestAcc=0.408 | F1=0.408 | AUC=0.762 | Time=5.59s\n",
            "Epoch 039 | Loss=1.2933 | TestAcc=0.383 | F1=0.377 | AUC=0.753 | Time=5.72s\n",
            "Epoch 040 | Loss=1.2726 | TestAcc=0.467 | F1=0.444 | AUC=0.792 | Time=5.84s\n",
            "Epoch 041 | Loss=1.2747 | TestAcc=0.467 | F1=0.454 | AUC=0.797 | Time=5.97s\n",
            "Epoch 042 | Loss=1.1665 | TestAcc=0.483 | F1=0.481 | AUC=0.776 | Time=6.12s\n",
            "Epoch 043 | Loss=1.2636 | TestAcc=0.433 | F1=0.422 | AUC=0.774 | Time=6.24s\n",
            "Epoch 044 | Loss=1.1990 | TestAcc=0.442 | F1=0.437 | AUC=0.794 | Time=6.37s\n",
            "Epoch 045 | Loss=1.2545 | TestAcc=0.467 | F1=0.459 | AUC=0.736 | Time=6.49s\n",
            "Epoch 046 | Loss=1.2643 | TestAcc=0.358 | F1=0.363 | AUC=0.702 | Time=6.63s\n",
            "Epoch 047 | Loss=1.2947 | TestAcc=0.342 | F1=0.344 | AUC=0.712 | Time=6.75s\n",
            "Epoch 048 | Loss=1.1857 | TestAcc=0.492 | F1=0.479 | AUC=0.784 | Time=6.88s\n",
            "Epoch 049 | Loss=1.1743 | TestAcc=0.500 | F1=0.490 | AUC=0.796 | Time=7.00s\n",
            "Epoch 050 | Loss=1.2490 | TestAcc=0.417 | F1=0.406 | AUC=0.769 | Time=7.14s\n",
            "Epoch 051 | Loss=1.1873 | TestAcc=0.442 | F1=0.435 | AUC=0.788 | Time=7.27s\n",
            "Epoch 052 | Loss=1.1836 | TestAcc=0.492 | F1=0.478 | AUC=0.810 | Time=7.39s\n",
            "Epoch 053 | Loss=1.1976 | TestAcc=0.375 | F1=0.368 | AUC=0.719 | Time=7.52s\n",
            "Epoch 054 | Loss=1.1429 | TestAcc=0.408 | F1=0.410 | AUC=0.763 | Time=7.65s\n",
            "Epoch 055 | Loss=1.1697 | TestAcc=0.450 | F1=0.448 | AUC=0.799 | Time=7.77s\n",
            "Epoch 056 | Loss=1.1843 | TestAcc=0.350 | F1=0.351 | AUC=0.713 | Time=7.90s\n",
            "Epoch 057 | Loss=1.1907 | TestAcc=0.325 | F1=0.309 | AUC=0.673 | Time=8.03s\n",
            "Epoch 058 | Loss=1.2340 | TestAcc=0.442 | F1=0.440 | AUC=0.757 | Time=8.16s\n",
            "Epoch 059 | Loss=1.2250 | TestAcc=0.500 | F1=0.508 | AUC=0.786 | Time=8.29s\n",
            "Epoch 060 | Loss=1.2243 | TestAcc=0.458 | F1=0.460 | AUC=0.792 | Time=8.42s\n",
            "Epoch 061 | Loss=1.1631 | TestAcc=0.375 | F1=0.359 | AUC=0.722 | Time=8.55s\n",
            "Epoch 062 | Loss=1.1684 | TestAcc=0.458 | F1=0.452 | AUC=0.741 | Time=8.68s\n",
            "Epoch 063 | Loss=1.1380 | TestAcc=0.450 | F1=0.446 | AUC=0.779 | Time=8.80s\n",
            "Epoch 064 | Loss=1.1645 | TestAcc=0.475 | F1=0.474 | AUC=0.794 | Time=8.93s\n",
            "Epoch 065 | Loss=1.1224 | TestAcc=0.450 | F1=0.438 | AUC=0.773 | Time=9.06s\n",
            "Epoch 066 | Loss=1.1399 | TestAcc=0.500 | F1=0.491 | AUC=0.768 | Time=9.19s\n",
            "Epoch 067 | Loss=1.1195 | TestAcc=0.533 | F1=0.535 | AUC=0.827 | Time=9.31s\n",
            "Epoch 068 | Loss=0.9986 | TestAcc=0.508 | F1=0.515 | AUC=0.786 | Time=9.44s\n",
            "Epoch 069 | Loss=1.0477 | TestAcc=0.508 | F1=0.501 | AUC=0.803 | Time=9.56s\n",
            "Epoch 070 | Loss=1.1510 | TestAcc=0.467 | F1=0.473 | AUC=0.760 | Time=9.69s\n",
            "Epoch 071 | Loss=1.1247 | TestAcc=0.383 | F1=0.381 | AUC=0.759 | Time=9.82s\n",
            "Epoch 072 | Loss=1.0653 | TestAcc=0.417 | F1=0.411 | AUC=0.720 | Time=9.94s\n",
            "Epoch 073 | Loss=1.0907 | TestAcc=0.400 | F1=0.397 | AUC=0.767 | Time=10.07s\n",
            "Epoch 074 | Loss=1.0881 | TestAcc=0.467 | F1=0.462 | AUC=0.767 | Time=10.20s\n",
            "Epoch 075 | Loss=0.9933 | TestAcc=0.483 | F1=0.484 | AUC=0.779 | Time=10.33s\n",
            "Epoch 076 | Loss=1.0263 | TestAcc=0.517 | F1=0.515 | AUC=0.798 | Time=10.46s\n",
            "Epoch 077 | Loss=1.0149 | TestAcc=0.425 | F1=0.423 | AUC=0.781 | Time=10.58s\n",
            "Epoch 078 | Loss=1.0198 | TestAcc=0.483 | F1=0.486 | AUC=0.754 | Time=10.71s\n",
            "Epoch 079 | Loss=1.0793 | TestAcc=0.525 | F1=0.524 | AUC=0.821 | Time=10.83s\n",
            "Epoch 080 | Loss=1.0611 | TestAcc=0.425 | F1=0.418 | AUC=0.727 | Time=10.96s\n",
            "Epoch 081 | Loss=1.1915 | TestAcc=0.492 | F1=0.476 | AUC=0.764 | Time=11.09s\n",
            "Epoch 082 | Loss=1.1022 | TestAcc=0.500 | F1=0.490 | AUC=0.808 | Time=11.21s\n",
            "Epoch 083 | Loss=1.0018 | TestAcc=0.333 | F1=0.338 | AUC=0.714 | Time=11.35s\n",
            "Epoch 084 | Loss=1.0871 | TestAcc=0.458 | F1=0.437 | AUC=0.783 | Time=11.48s\n",
            "Epoch 085 | Loss=0.9984 | TestAcc=0.533 | F1=0.525 | AUC=0.794 | Time=11.60s\n",
            "Epoch 086 | Loss=0.9682 | TestAcc=0.575 | F1=0.570 | AUC=0.830 | Time=11.73s\n",
            "Epoch 087 | Loss=0.9602 | TestAcc=0.542 | F1=0.540 | AUC=0.790 | Time=11.85s\n",
            "Epoch 088 | Loss=0.9044 | TestAcc=0.450 | F1=0.448 | AUC=0.815 | Time=11.98s\n",
            "Epoch 089 | Loss=1.0616 | TestAcc=0.433 | F1=0.424 | AUC=0.735 | Time=12.11s\n",
            "Epoch 090 | Loss=1.0515 | TestAcc=0.425 | F1=0.436 | AUC=0.750 | Time=12.25s\n",
            "Epoch 091 | Loss=1.0032 | TestAcc=0.483 | F1=0.483 | AUC=0.798 | Time=12.38s\n",
            "Epoch 092 | Loss=1.0262 | TestAcc=0.500 | F1=0.502 | AUC=0.797 | Time=12.51s\n",
            "Epoch 093 | Loss=0.9898 | TestAcc=0.533 | F1=0.536 | AUC=0.817 | Time=12.67s\n",
            "Epoch 094 | Loss=1.0204 | TestAcc=0.483 | F1=0.478 | AUC=0.779 | Time=12.86s\n",
            "Epoch 095 | Loss=1.0334 | TestAcc=0.458 | F1=0.462 | AUC=0.784 | Time=13.03s\n",
            "Epoch 096 | Loss=0.9925 | TestAcc=0.483 | F1=0.483 | AUC=0.797 | Time=13.20s\n",
            "Epoch 097 | Loss=0.9147 | TestAcc=0.442 | F1=0.447 | AUC=0.765 | Time=13.37s\n",
            "Epoch 098 | Loss=0.9669 | TestAcc=0.483 | F1=0.487 | AUC=0.785 | Time=13.54s\n",
            "Epoch 099 | Loss=0.8951 | TestAcc=0.492 | F1=0.485 | AUC=0.774 | Time=13.70s\n",
            "Epoch 100 | Loss=0.9946 | TestAcc=0.508 | F1=0.507 | AUC=0.811 | Time=13.87s\n",
            "Epoch 101 | Loss=0.8439 | TestAcc=0.533 | F1=0.538 | AUC=0.792 | Time=14.04s\n",
            "Epoch 102 | Loss=1.0947 | TestAcc=0.525 | F1=0.530 | AUC=0.811 | Time=14.21s\n",
            "Epoch 103 | Loss=0.9660 | TestAcc=0.483 | F1=0.481 | AUC=0.762 | Time=14.38s\n",
            "Epoch 104 | Loss=0.9511 | TestAcc=0.408 | F1=0.405 | AUC=0.796 | Time=14.56s\n",
            "Epoch 105 | Loss=0.9414 | TestAcc=0.517 | F1=0.511 | AUC=0.799 | Time=14.71s\n",
            "Epoch 106 | Loss=0.9488 | TestAcc=0.550 | F1=0.546 | AUC=0.803 | Time=14.90s\n",
            "Epoch 107 | Loss=0.9253 | TestAcc=0.458 | F1=0.452 | AUC=0.797 | Time=15.09s\n",
            "Epoch 108 | Loss=0.9168 | TestAcc=0.458 | F1=0.455 | AUC=0.762 | Time=15.27s\n",
            "Epoch 109 | Loss=0.9585 | TestAcc=0.517 | F1=0.515 | AUC=0.796 | Time=15.48s\n",
            "Epoch 110 | Loss=0.9090 | TestAcc=0.517 | F1=0.510 | AUC=0.775 | Time=15.66s\n",
            "Epoch 111 | Loss=0.8910 | TestAcc=0.375 | F1=0.371 | AUC=0.730 | Time=15.79s\n",
            "Epoch 112 | Loss=0.8845 | TestAcc=0.508 | F1=0.499 | AUC=0.816 | Time=15.91s\n",
            "Epoch 113 | Loss=0.8304 | TestAcc=0.467 | F1=0.465 | AUC=0.774 | Time=16.04s\n",
            "Epoch 114 | Loss=1.0146 | TestAcc=0.442 | F1=0.437 | AUC=0.779 | Time=16.16s\n",
            "Epoch 115 | Loss=0.9056 | TestAcc=0.442 | F1=0.436 | AUC=0.743 | Time=16.29s\n",
            "Epoch 116 | Loss=0.9773 | TestAcc=0.450 | F1=0.450 | AUC=0.773 | Time=16.42s\n",
            "Epoch 117 | Loss=1.0584 | TestAcc=0.367 | F1=0.373 | AUC=0.721 | Time=16.56s\n",
            "Epoch 118 | Loss=0.9921 | TestAcc=0.458 | F1=0.456 | AUC=0.782 | Time=16.69s\n",
            "Epoch 119 | Loss=0.8803 | TestAcc=0.525 | F1=0.520 | AUC=0.805 | Time=16.81s\n",
            "Epoch 120 | Loss=0.7870 | TestAcc=0.533 | F1=0.528 | AUC=0.808 | Time=16.94s\n",
            "Epoch 121 | Loss=0.7392 | TestAcc=0.517 | F1=0.519 | AUC=0.801 | Time=17.06s\n",
            "Epoch 122 | Loss=0.8233 | TestAcc=0.483 | F1=0.480 | AUC=0.769 | Time=17.19s\n",
            "Epoch 123 | Loss=0.8709 | TestAcc=0.450 | F1=0.444 | AUC=0.777 | Time=17.31s\n",
            "Epoch 124 | Loss=1.0039 | TestAcc=0.517 | F1=0.514 | AUC=0.778 | Time=17.43s\n",
            "Epoch 125 | Loss=0.9749 | TestAcc=0.433 | F1=0.429 | AUC=0.740 | Time=17.57s\n",
            "Epoch 126 | Loss=1.0170 | TestAcc=0.392 | F1=0.384 | AUC=0.735 | Time=17.70s\n",
            "Epoch 127 | Loss=0.8871 | TestAcc=0.500 | F1=0.488 | AUC=0.784 | Time=17.82s\n",
            "Epoch 128 | Loss=0.8583 | TestAcc=0.525 | F1=0.530 | AUC=0.809 | Time=17.94s\n",
            "Epoch 129 | Loss=0.8766 | TestAcc=0.450 | F1=0.452 | AUC=0.757 | Time=18.07s\n",
            "Epoch 130 | Loss=0.8676 | TestAcc=0.525 | F1=0.525 | AUC=0.793 | Time=18.20s\n",
            "Epoch 131 | Loss=0.7317 | TestAcc=0.508 | F1=0.501 | AUC=0.768 | Time=18.32s\n",
            "Epoch 132 | Loss=0.7662 | TestAcc=0.467 | F1=0.473 | AUC=0.796 | Time=18.44s\n",
            "Epoch 133 | Loss=0.8413 | TestAcc=0.483 | F1=0.475 | AUC=0.780 | Time=18.59s\n",
            "Epoch 134 | Loss=0.7768 | TestAcc=0.550 | F1=0.549 | AUC=0.797 | Time=18.71s\n",
            "Epoch 135 | Loss=0.8495 | TestAcc=0.375 | F1=0.372 | AUC=0.705 | Time=18.83s\n",
            "Epoch 136 | Loss=0.8966 | TestAcc=0.425 | F1=0.426 | AUC=0.732 | Time=18.96s\n",
            "Epoch 137 | Loss=0.8798 | TestAcc=0.517 | F1=0.512 | AUC=0.779 | Time=19.09s\n",
            "Epoch 138 | Loss=0.9540 | TestAcc=0.525 | F1=0.522 | AUC=0.811 | Time=19.21s\n",
            "Epoch 139 | Loss=0.7734 | TestAcc=0.525 | F1=0.518 | AUC=0.812 | Time=19.34s\n",
            "Epoch 140 | Loss=0.7383 | TestAcc=0.492 | F1=0.485 | AUC=0.800 | Time=19.46s\n",
            "Epoch 141 | Loss=0.8202 | TestAcc=0.425 | F1=0.434 | AUC=0.738 | Time=19.60s\n",
            "Epoch 142 | Loss=0.7596 | TestAcc=0.533 | F1=0.530 | AUC=0.791 | Time=19.73s\n",
            "Epoch 143 | Loss=0.7026 | TestAcc=0.542 | F1=0.542 | AUC=0.817 | Time=19.85s\n",
            "Epoch 144 | Loss=0.7897 | TestAcc=0.550 | F1=0.548 | AUC=0.809 | Time=19.98s\n",
            "Epoch 145 | Loss=0.7437 | TestAcc=0.417 | F1=0.425 | AUC=0.790 | Time=20.11s\n",
            "Epoch 146 | Loss=0.7885 | TestAcc=0.425 | F1=0.414 | AUC=0.760 | Time=20.23s\n",
            "Epoch 147 | Loss=0.8054 | TestAcc=0.525 | F1=0.517 | AUC=0.803 | Time=20.36s\n",
            "Epoch 148 | Loss=0.6902 | TestAcc=0.475 | F1=0.469 | AUC=0.808 | Time=20.48s\n",
            "Epoch 149 | Loss=0.7639 | TestAcc=0.508 | F1=0.507 | AUC=0.821 | Time=20.63s\n",
            "Epoch 150 | Loss=0.7466 | TestAcc=0.558 | F1=0.560 | AUC=0.793 | Time=20.75s\n",
            "Epoch 151 | Loss=0.6494 | TestAcc=0.483 | F1=0.483 | AUC=0.797 | Time=20.88s\n",
            "Epoch 152 | Loss=0.7172 | TestAcc=0.483 | F1=0.480 | AUC=0.791 | Time=21.00s\n",
            "Epoch 153 | Loss=0.7077 | TestAcc=0.550 | F1=0.546 | AUC=0.807 | Time=21.13s\n",
            "Epoch 154 | Loss=0.7396 | TestAcc=0.350 | F1=0.348 | AUC=0.679 | Time=21.26s\n",
            "Epoch 155 | Loss=0.8114 | TestAcc=0.500 | F1=0.498 | AUC=0.805 | Time=21.38s\n",
            "Epoch 156 | Loss=0.7383 | TestAcc=0.525 | F1=0.526 | AUC=0.803 | Time=21.51s\n",
            "Epoch 157 | Loss=0.7446 | TestAcc=0.475 | F1=0.472 | AUC=0.776 | Time=21.65s\n",
            "Epoch 158 | Loss=0.8545 | TestAcc=0.475 | F1=0.475 | AUC=0.818 | Time=21.77s\n",
            "Epoch 159 | Loss=0.6189 | TestAcc=0.483 | F1=0.481 | AUC=0.802 | Time=21.90s\n",
            "Epoch 160 | Loss=0.6476 | TestAcc=0.450 | F1=0.445 | AUC=0.795 | Time=22.02s\n",
            "Epoch 161 | Loss=0.6944 | TestAcc=0.517 | F1=0.515 | AUC=0.814 | Time=22.15s\n",
            "Epoch 162 | Loss=0.6788 | TestAcc=0.525 | F1=0.513 | AUC=0.812 | Time=22.27s\n",
            "Epoch 163 | Loss=0.7442 | TestAcc=0.475 | F1=0.481 | AUC=0.792 | Time=22.40s\n",
            "Epoch 164 | Loss=0.6287 | TestAcc=0.475 | F1=0.471 | AUC=0.781 | Time=22.52s\n",
            "Epoch 165 | Loss=0.6917 | TestAcc=0.558 | F1=0.553 | AUC=0.817 | Time=22.65s\n",
            "Epoch 166 | Loss=0.6422 | TestAcc=0.508 | F1=0.510 | AUC=0.798 | Time=22.79s\n",
            "Epoch 167 | Loss=0.6559 | TestAcc=0.483 | F1=0.481 | AUC=0.794 | Time=22.92s\n",
            "Epoch 168 | Loss=0.7157 | TestAcc=0.483 | F1=0.485 | AUC=0.797 | Time=23.04s\n",
            "Epoch 169 | Loss=0.6840 | TestAcc=0.500 | F1=0.502 | AUC=0.822 | Time=23.17s\n",
            "Epoch 170 | Loss=0.6648 | TestAcc=0.450 | F1=0.451 | AUC=0.750 | Time=23.29s\n",
            "Epoch 171 | Loss=0.8215 | TestAcc=0.517 | F1=0.512 | AUC=0.776 | Time=23.42s\n",
            "Epoch 172 | Loss=0.8290 | TestAcc=0.517 | F1=0.514 | AUC=0.745 | Time=23.55s\n",
            "Epoch 173 | Loss=0.7687 | TestAcc=0.508 | F1=0.509 | AUC=0.802 | Time=23.68s\n",
            "Epoch 174 | Loss=0.7270 | TestAcc=0.450 | F1=0.448 | AUC=0.784 | Time=23.81s\n",
            "Epoch 175 | Loss=0.6270 | TestAcc=0.508 | F1=0.498 | AUC=0.799 | Time=23.94s\n",
            "Epoch 176 | Loss=0.5920 | TestAcc=0.442 | F1=0.434 | AUC=0.751 | Time=24.07s\n",
            "Epoch 177 | Loss=0.6675 | TestAcc=0.533 | F1=0.529 | AUC=0.816 | Time=24.19s\n",
            "Epoch 178 | Loss=0.7552 | TestAcc=0.442 | F1=0.453 | AUC=0.754 | Time=24.31s\n",
            "Epoch 179 | Loss=0.6230 | TestAcc=0.575 | F1=0.570 | AUC=0.814 | Time=24.44s\n",
            "Epoch 180 | Loss=0.6616 | TestAcc=0.575 | F1=0.572 | AUC=0.827 | Time=24.56s\n",
            "Epoch 181 | Loss=0.6925 | TestAcc=0.517 | F1=0.514 | AUC=0.794 | Time=24.69s\n",
            "Epoch 182 | Loss=0.7607 | TestAcc=0.417 | F1=0.417 | AUC=0.772 | Time=24.83s\n",
            "Epoch 183 | Loss=0.6825 | TestAcc=0.467 | F1=0.466 | AUC=0.778 | Time=24.96s\n",
            "Epoch 184 | Loss=0.7463 | TestAcc=0.558 | F1=0.560 | AUC=0.796 | Time=25.09s\n",
            "Epoch 185 | Loss=0.7067 | TestAcc=0.458 | F1=0.461 | AUC=0.764 | Time=25.21s\n",
            "Epoch 186 | Loss=0.6840 | TestAcc=0.508 | F1=0.502 | AUC=0.796 | Time=25.33s\n",
            "Epoch 187 | Loss=0.6882 | TestAcc=0.508 | F1=0.501 | AUC=0.780 | Time=25.46s\n",
            "Epoch 188 | Loss=0.6227 | TestAcc=0.442 | F1=0.436 | AUC=0.785 | Time=25.59s\n",
            "Epoch 189 | Loss=0.7119 | TestAcc=0.458 | F1=0.460 | AUC=0.778 | Time=25.79s\n",
            "Epoch 190 | Loss=0.6328 | TestAcc=0.475 | F1=0.477 | AUC=0.791 | Time=25.96s\n",
            "Epoch 191 | Loss=0.6696 | TestAcc=0.458 | F1=0.460 | AUC=0.775 | Time=26.14s\n",
            "Epoch 192 | Loss=0.6529 | TestAcc=0.508 | F1=0.510 | AUC=0.825 | Time=26.31s\n",
            "Epoch 193 | Loss=0.6352 | TestAcc=0.492 | F1=0.488 | AUC=0.779 | Time=26.48s\n",
            "Epoch 194 | Loss=0.5400 | TestAcc=0.500 | F1=0.499 | AUC=0.821 | Time=26.64s\n",
            "Epoch 195 | Loss=0.5703 | TestAcc=0.492 | F1=0.493 | AUC=0.789 | Time=26.82s\n",
            "Epoch 196 | Loss=0.6453 | TestAcc=0.483 | F1=0.483 | AUC=0.796 | Time=26.99s\n",
            "Epoch 197 | Loss=0.5639 | TestAcc=0.542 | F1=0.544 | AUC=0.808 | Time=27.17s\n",
            "Epoch 198 | Loss=0.6338 | TestAcc=0.492 | F1=0.494 | AUC=0.793 | Time=27.33s\n",
            "Epoch 199 | Loss=0.5912 | TestAcc=0.525 | F1=0.516 | AUC=0.812 | Time=27.50s\n",
            "Epoch 200 | Loss=0.6053 | TestAcc=0.508 | F1=0.507 | AUC=0.799 | Time=27.67s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:11:42,441] A new study created in memory with name: no-name-3c2ac270-249e-4611-8b12-b609cfde907c\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training summary stored in : /content/drive/MyDrive/InformationSystems/Classification/results/classification/gin.csv\n",
            "  method  seed  dataset optimization_enabled  embedding_dimension  \\\n",
            "0    GIN    42  ENZYMES                  yes                   64   \n",
            "\n",
            "  objective_weights  num_layers   dropout        lr  weight_decay  epochs  \\\n",
            "0     (0.5,0.3,0.2)           4  0.216888  0.003301      0.000002     200   \n",
            "\n",
            "   best_epoch  best_loss  eval_loss  eval_acc  eval_f1  eval_auc  \\\n",
            "0          86     1.4847     1.4847     0.575   0.5703    0.8304   \n",
            "\n",
            "   training_time (s)  generation_time (s)  memory_usage (MB)  \n",
            "0              27.67                 6.23            1461.65  \n",
            "Saved model: /content/drive/MyDrive/InformationSystems/Classification/models/GIN_ENZYMES_42.pth\n",
            "Loaded dataset ENZYMES: 600 graphs, 3 node features, 6 classes\n",
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:11:43,235] Trial 0 finished with value: 0.3234383872804873 and parameters: {'num_layers': 3, 'dropout': 0.3422576848730136, 'lr': 0.00015673307985484202, 'weight_decay': 7.786552355884828e-06}. Best is trial 0 with value: 0.3234383872804873.\n",
            "[I 2026-01-22 11:11:43,828] Trial 1 finished with value: 0.2425768917638369 and parameters: {'num_layers': 4, 'dropout': 0.05778190396568621, 'lr': 0.0001369135597960873, 'weight_decay': 5.655552517503553e-06}. Best is trial 0 with value: 0.3234383872804873.\n",
            "[I 2026-01-22 11:11:44,334] Trial 2 finished with value: 0.2743085366332295 and parameters: {'num_layers': 3, 'dropout': 0.07230781737974057, 'lr': 0.0006301649297219171, 'weight_decay': 0.00047028346919216694}. Best is trial 0 with value: 0.3234383872804873.\n",
            "[I 2026-01-22 11:11:45,026] Trial 3 finished with value: 0.2306116259432894 and parameters: {'num_layers': 6, 'dropout': 0.3280421825752161, 'lr': 0.00029938512901408977, 'weight_decay': 0.0005715934508779792}. Best is trial 0 with value: 0.3234383872804873.\n",
            "[I 2026-01-22 11:11:45,503] Trial 4 finished with value: 0.3615798284914983 and parameters: {'num_layers': 3, 'dropout': 0.03778372396652885, 'lr': 0.0002568763097605338, 'weight_decay': 0.00019715685811761064}. Best is trial 4 with value: 0.3615798284914983.\n",
            "[I 2026-01-22 11:11:46,062] Trial 5 finished with value: 0.27073937005385784 and parameters: {'num_layers': 4, 'dropout': 0.27584068880848345, 'lr': 0.00015067684238253878, 'weight_decay': 2.560114281718231e-05}. Best is trial 4 with value: 0.3615798284914983.\n",
            "[I 2026-01-22 11:11:46,692] Trial 6 finished with value: 0.28474094812972317 and parameters: {'num_layers': 5, 'dropout': 0.357215186545818, 'lr': 0.003075017563527089, 'weight_decay': 0.0007292365121674628}. Best is trial 4 with value: 0.3615798284914983.\n",
            "[I 2026-01-22 11:11:47,378] Trial 7 finished with value: 0.2561671525493134 and parameters: {'num_layers': 6, 'dropout': 0.4876608244024911, 'lr': 0.00048707169912351624, 'weight_decay': 6.763508126226416e-06}. Best is trial 4 with value: 0.3615798284914983.\n",
            "[I 2026-01-22 11:11:48,021] Trial 8 finished with value: 0.32318013099043497 and parameters: {'num_layers': 5, 'dropout': 0.40431396540360937, 'lr': 0.0003445909096540493, 'weight_decay': 1.956813383035815e-05}. Best is trial 4 with value: 0.3615798284914983.\n",
            "[I 2026-01-22 11:11:48,576] Trial 9 finished with value: 0.2884794426239114 and parameters: {'num_layers': 4, 'dropout': 0.29212026156621745, 'lr': 0.0005784441044036546, 'weight_decay': 0.00023061724998157729}. Best is trial 4 with value: 0.3615798284914983.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'num_layers': 3, 'dropout': 0.03778372396652885, 'lr': 0.0002568763097605338, 'weight_decay': 0.00019715685811761064}\n",
            "\n",
            "Running final training GIN...\n",
            "{'num_layers': 3, 'dropout': 0.03778372396652885, 'lr': 0.0002568763097605338, 'weight_decay': 0.00019715685811761064}\n",
            "Epoch 001 | Loss=3.6071 | TestAcc=0.125 | F1=0.076 | AUC=0.476 | Time=0.11s\n",
            "Epoch 002 | Loss=2.4986 | TestAcc=0.183 | F1=0.172 | AUC=0.535 | Time=0.24s\n",
            "Epoch 003 | Loss=2.2202 | TestAcc=0.225 | F1=0.215 | AUC=0.561 | Time=0.35s\n",
            "Epoch 004 | Loss=2.0223 | TestAcc=0.233 | F1=0.213 | AUC=0.598 | Time=0.46s\n",
            "Epoch 005 | Loss=1.9581 | TestAcc=0.275 | F1=0.274 | AUC=0.601 | Time=0.57s\n",
            "Epoch 006 | Loss=1.8379 | TestAcc=0.283 | F1=0.282 | AUC=0.654 | Time=0.68s\n",
            "Epoch 007 | Loss=1.7111 | TestAcc=0.292 | F1=0.298 | AUC=0.669 | Time=0.80s\n",
            "Epoch 008 | Loss=1.7478 | TestAcc=0.342 | F1=0.343 | AUC=0.685 | Time=0.91s\n",
            "Epoch 009 | Loss=1.6591 | TestAcc=0.325 | F1=0.322 | AUC=0.658 | Time=1.02s\n",
            "Epoch 010 | Loss=1.5489 | TestAcc=0.333 | F1=0.331 | AUC=0.674 | Time=1.14s\n",
            "Epoch 011 | Loss=1.4278 | TestAcc=0.342 | F1=0.343 | AUC=0.672 | Time=1.26s\n",
            "Epoch 012 | Loss=1.4552 | TestAcc=0.383 | F1=0.376 | AUC=0.698 | Time=1.37s\n",
            "Epoch 013 | Loss=1.4848 | TestAcc=0.358 | F1=0.344 | AUC=0.695 | Time=1.48s\n",
            "Epoch 014 | Loss=1.4039 | TestAcc=0.375 | F1=0.374 | AUC=0.698 | Time=1.60s\n",
            "Epoch 015 | Loss=1.4048 | TestAcc=0.375 | F1=0.378 | AUC=0.713 | Time=1.71s\n",
            "Epoch 016 | Loss=1.3922 | TestAcc=0.383 | F1=0.361 | AUC=0.695 | Time=1.82s\n",
            "Epoch 017 | Loss=1.3157 | TestAcc=0.308 | F1=0.308 | AUC=0.690 | Time=1.93s\n",
            "Epoch 018 | Loss=1.3257 | TestAcc=0.325 | F1=0.311 | AUC=0.671 | Time=2.04s\n",
            "Epoch 019 | Loss=1.3375 | TestAcc=0.358 | F1=0.353 | AUC=0.687 | Time=2.16s\n",
            "Epoch 020 | Loss=1.2292 | TestAcc=0.317 | F1=0.330 | AUC=0.652 | Time=2.28s\n",
            "Epoch 021 | Loss=1.3183 | TestAcc=0.375 | F1=0.372 | AUC=0.710 | Time=2.39s\n",
            "Epoch 022 | Loss=1.2793 | TestAcc=0.392 | F1=0.382 | AUC=0.691 | Time=2.50s\n",
            "Epoch 023 | Loss=1.2413 | TestAcc=0.367 | F1=0.363 | AUC=0.710 | Time=2.61s\n",
            "Epoch 024 | Loss=1.1651 | TestAcc=0.358 | F1=0.361 | AUC=0.663 | Time=2.73s\n",
            "Epoch 025 | Loss=1.2128 | TestAcc=0.375 | F1=0.377 | AUC=0.669 | Time=2.83s\n",
            "Epoch 026 | Loss=1.1616 | TestAcc=0.392 | F1=0.384 | AUC=0.714 | Time=2.95s\n",
            "Epoch 027 | Loss=1.1026 | TestAcc=0.425 | F1=0.420 | AUC=0.701 | Time=3.07s\n",
            "Epoch 028 | Loss=1.1850 | TestAcc=0.400 | F1=0.395 | AUC=0.671 | Time=3.18s\n",
            "Epoch 029 | Loss=1.1465 | TestAcc=0.442 | F1=0.442 | AUC=0.724 | Time=3.30s\n",
            "Epoch 030 | Loss=1.1211 | TestAcc=0.392 | F1=0.394 | AUC=0.699 | Time=3.41s\n",
            "Epoch 031 | Loss=1.0690 | TestAcc=0.375 | F1=0.368 | AUC=0.689 | Time=3.52s\n",
            "Epoch 032 | Loss=1.0615 | TestAcc=0.333 | F1=0.333 | AUC=0.686 | Time=3.63s\n",
            "Epoch 033 | Loss=1.0270 | TestAcc=0.417 | F1=0.414 | AUC=0.734 | Time=3.75s\n",
            "Epoch 034 | Loss=1.1003 | TestAcc=0.400 | F1=0.396 | AUC=0.708 | Time=3.86s\n",
            "Epoch 035 | Loss=1.0008 | TestAcc=0.408 | F1=0.413 | AUC=0.717 | Time=3.97s\n",
            "Epoch 036 | Loss=1.0688 | TestAcc=0.400 | F1=0.390 | AUC=0.701 | Time=4.08s\n",
            "Epoch 037 | Loss=1.0488 | TestAcc=0.367 | F1=0.356 | AUC=0.682 | Time=4.19s\n",
            "Epoch 038 | Loss=1.0249 | TestAcc=0.425 | F1=0.429 | AUC=0.710 | Time=4.31s\n",
            "Epoch 039 | Loss=1.0244 | TestAcc=0.392 | F1=0.390 | AUC=0.708 | Time=4.42s\n",
            "Epoch 040 | Loss=1.0326 | TestAcc=0.425 | F1=0.419 | AUC=0.707 | Time=4.53s\n",
            "Epoch 041 | Loss=0.9219 | TestAcc=0.408 | F1=0.403 | AUC=0.696 | Time=4.65s\n",
            "Epoch 042 | Loss=1.0262 | TestAcc=0.383 | F1=0.383 | AUC=0.706 | Time=4.78s\n",
            "Epoch 043 | Loss=0.9696 | TestAcc=0.408 | F1=0.413 | AUC=0.727 | Time=4.94s\n",
            "Epoch 044 | Loss=0.9795 | TestAcc=0.442 | F1=0.433 | AUC=0.743 | Time=5.10s\n",
            "Epoch 045 | Loss=0.9327 | TestAcc=0.375 | F1=0.374 | AUC=0.703 | Time=5.25s\n",
            "Epoch 046 | Loss=0.9508 | TestAcc=0.367 | F1=0.373 | AUC=0.716 | Time=5.41s\n",
            "Epoch 047 | Loss=0.9044 | TestAcc=0.400 | F1=0.396 | AUC=0.700 | Time=5.56s\n",
            "Epoch 048 | Loss=0.9060 | TestAcc=0.433 | F1=0.430 | AUC=0.712 | Time=5.71s\n",
            "Epoch 049 | Loss=0.8829 | TestAcc=0.408 | F1=0.417 | AUC=0.708 | Time=5.85s\n",
            "Epoch 050 | Loss=0.8672 | TestAcc=0.417 | F1=0.410 | AUC=0.730 | Time=6.00s\n",
            "Epoch 051 | Loss=0.8644 | TestAcc=0.442 | F1=0.442 | AUC=0.740 | Time=6.15s\n",
            "Epoch 052 | Loss=0.8411 | TestAcc=0.442 | F1=0.452 | AUC=0.744 | Time=6.31s\n",
            "Epoch 053 | Loss=0.8608 | TestAcc=0.417 | F1=0.414 | AUC=0.711 | Time=6.46s\n",
            "Epoch 054 | Loss=0.7978 | TestAcc=0.442 | F1=0.436 | AUC=0.706 | Time=6.61s\n",
            "Epoch 055 | Loss=0.8016 | TestAcc=0.442 | F1=0.445 | AUC=0.707 | Time=6.75s\n",
            "Epoch 056 | Loss=0.7941 | TestAcc=0.425 | F1=0.420 | AUC=0.733 | Time=6.91s\n",
            "Epoch 057 | Loss=0.8301 | TestAcc=0.367 | F1=0.361 | AUC=0.686 | Time=7.07s\n",
            "Epoch 058 | Loss=0.8037 | TestAcc=0.458 | F1=0.456 | AUC=0.738 | Time=7.24s\n",
            "Epoch 059 | Loss=0.7864 | TestAcc=0.367 | F1=0.372 | AUC=0.712 | Time=7.41s\n",
            "Epoch 060 | Loss=0.8221 | TestAcc=0.417 | F1=0.410 | AUC=0.713 | Time=7.60s\n",
            "Epoch 061 | Loss=0.8060 | TestAcc=0.425 | F1=0.429 | AUC=0.721 | Time=7.76s\n",
            "Epoch 062 | Loss=0.8521 | TestAcc=0.383 | F1=0.379 | AUC=0.694 | Time=7.87s\n",
            "Epoch 063 | Loss=0.8801 | TestAcc=0.408 | F1=0.406 | AUC=0.720 | Time=7.98s\n",
            "Epoch 064 | Loss=0.8172 | TestAcc=0.433 | F1=0.436 | AUC=0.736 | Time=8.09s\n",
            "Epoch 065 | Loss=0.7576 | TestAcc=0.483 | F1=0.486 | AUC=0.741 | Time=8.21s\n",
            "Epoch 066 | Loss=0.7053 | TestAcc=0.433 | F1=0.429 | AUC=0.716 | Time=8.32s\n",
            "Epoch 067 | Loss=0.7319 | TestAcc=0.425 | F1=0.429 | AUC=0.729 | Time=8.43s\n",
            "Epoch 068 | Loss=0.7081 | TestAcc=0.442 | F1=0.444 | AUC=0.683 | Time=8.55s\n",
            "Epoch 069 | Loss=0.7412 | TestAcc=0.425 | F1=0.419 | AUC=0.687 | Time=8.67s\n",
            "Epoch 070 | Loss=0.7186 | TestAcc=0.392 | F1=0.385 | AUC=0.713 | Time=8.79s\n",
            "Epoch 071 | Loss=0.6875 | TestAcc=0.408 | F1=0.392 | AUC=0.699 | Time=8.89s\n",
            "Epoch 072 | Loss=0.6724 | TestAcc=0.358 | F1=0.353 | AUC=0.717 | Time=9.01s\n",
            "Epoch 073 | Loss=0.6138 | TestAcc=0.383 | F1=0.372 | AUC=0.720 | Time=9.12s\n",
            "Epoch 074 | Loss=0.6781 | TestAcc=0.433 | F1=0.435 | AUC=0.712 | Time=9.23s\n",
            "Epoch 075 | Loss=0.6668 | TestAcc=0.408 | F1=0.408 | AUC=0.676 | Time=9.34s\n",
            "Epoch 076 | Loss=0.7315 | TestAcc=0.442 | F1=0.438 | AUC=0.723 | Time=9.45s\n",
            "Epoch 077 | Loss=0.6611 | TestAcc=0.433 | F1=0.428 | AUC=0.701 | Time=9.57s\n",
            "Epoch 078 | Loss=0.6241 | TestAcc=0.383 | F1=0.382 | AUC=0.719 | Time=9.69s\n",
            "Epoch 079 | Loss=0.6415 | TestAcc=0.433 | F1=0.431 | AUC=0.708 | Time=9.80s\n",
            "Epoch 080 | Loss=0.6688 | TestAcc=0.417 | F1=0.417 | AUC=0.723 | Time=9.91s\n",
            "Epoch 081 | Loss=0.6268 | TestAcc=0.408 | F1=0.419 | AUC=0.718 | Time=10.01s\n",
            "Epoch 082 | Loss=0.5844 | TestAcc=0.408 | F1=0.411 | AUC=0.744 | Time=10.13s\n",
            "Epoch 083 | Loss=0.5679 | TestAcc=0.425 | F1=0.417 | AUC=0.724 | Time=10.24s\n",
            "Epoch 084 | Loss=0.6489 | TestAcc=0.408 | F1=0.407 | AUC=0.706 | Time=10.35s\n",
            "Epoch 085 | Loss=0.6043 | TestAcc=0.433 | F1=0.427 | AUC=0.712 | Time=10.46s\n",
            "Epoch 086 | Loss=0.5357 | TestAcc=0.433 | F1=0.441 | AUC=0.723 | Time=10.58s\n",
            "Epoch 087 | Loss=0.6634 | TestAcc=0.425 | F1=0.413 | AUC=0.702 | Time=10.70s\n",
            "Epoch 088 | Loss=0.6170 | TestAcc=0.433 | F1=0.445 | AUC=0.713 | Time=10.81s\n",
            "Epoch 089 | Loss=0.5453 | TestAcc=0.450 | F1=0.446 | AUC=0.740 | Time=10.92s\n",
            "Epoch 090 | Loss=0.6971 | TestAcc=0.467 | F1=0.464 | AUC=0.727 | Time=11.03s\n",
            "Epoch 091 | Loss=0.5809 | TestAcc=0.458 | F1=0.464 | AUC=0.717 | Time=11.14s\n",
            "Epoch 092 | Loss=0.5709 | TestAcc=0.383 | F1=0.383 | AUC=0.717 | Time=11.25s\n",
            "Epoch 093 | Loss=0.5438 | TestAcc=0.408 | F1=0.405 | AUC=0.736 | Time=11.36s\n",
            "Epoch 094 | Loss=0.5608 | TestAcc=0.433 | F1=0.427 | AUC=0.744 | Time=11.47s\n",
            "Epoch 095 | Loss=0.5456 | TestAcc=0.450 | F1=0.459 | AUC=0.752 | Time=11.58s\n",
            "Epoch 096 | Loss=0.5959 | TestAcc=0.483 | F1=0.480 | AUC=0.733 | Time=11.71s\n",
            "Epoch 097 | Loss=0.5464 | TestAcc=0.483 | F1=0.478 | AUC=0.731 | Time=11.82s\n",
            "Epoch 098 | Loss=0.5010 | TestAcc=0.467 | F1=0.458 | AUC=0.744 | Time=11.93s\n",
            "Epoch 099 | Loss=0.6041 | TestAcc=0.442 | F1=0.444 | AUC=0.716 | Time=12.04s\n",
            "Epoch 100 | Loss=0.5840 | TestAcc=0.467 | F1=0.469 | AUC=0.706 | Time=12.15s\n",
            "Epoch 101 | Loss=0.4883 | TestAcc=0.308 | F1=0.311 | AUC=0.690 | Time=12.27s\n",
            "Epoch 102 | Loss=0.5332 | TestAcc=0.442 | F1=0.442 | AUC=0.734 | Time=12.38s\n",
            "Epoch 103 | Loss=0.5073 | TestAcc=0.350 | F1=0.342 | AUC=0.739 | Time=12.49s\n",
            "Epoch 104 | Loss=0.5397 | TestAcc=0.458 | F1=0.456 | AUC=0.716 | Time=12.60s\n",
            "Epoch 105 | Loss=0.5016 | TestAcc=0.350 | F1=0.345 | AUC=0.655 | Time=12.72s\n",
            "Epoch 106 | Loss=0.5530 | TestAcc=0.475 | F1=0.484 | AUC=0.736 | Time=12.84s\n",
            "Epoch 107 | Loss=0.5059 | TestAcc=0.442 | F1=0.450 | AUC=0.714 | Time=12.94s\n",
            "Epoch 108 | Loss=0.4728 | TestAcc=0.442 | F1=0.446 | AUC=0.722 | Time=13.06s\n",
            "Epoch 109 | Loss=0.4694 | TestAcc=0.458 | F1=0.462 | AUC=0.737 | Time=13.17s\n",
            "Epoch 110 | Loss=0.4948 | TestAcc=0.433 | F1=0.431 | AUC=0.740 | Time=13.28s\n",
            "Epoch 111 | Loss=0.4877 | TestAcc=0.500 | F1=0.501 | AUC=0.744 | Time=13.40s\n",
            "Epoch 112 | Loss=0.4920 | TestAcc=0.467 | F1=0.469 | AUC=0.753 | Time=13.51s\n",
            "Epoch 113 | Loss=0.4960 | TestAcc=0.392 | F1=0.388 | AUC=0.764 | Time=13.62s\n",
            "Epoch 114 | Loss=0.4531 | TestAcc=0.417 | F1=0.416 | AUC=0.687 | Time=13.75s\n",
            "Epoch 115 | Loss=0.4782 | TestAcc=0.475 | F1=0.480 | AUC=0.737 | Time=13.86s\n",
            "Epoch 116 | Loss=0.5073 | TestAcc=0.458 | F1=0.467 | AUC=0.737 | Time=13.97s\n",
            "Epoch 117 | Loss=0.4568 | TestAcc=0.442 | F1=0.443 | AUC=0.750 | Time=14.08s\n",
            "Epoch 118 | Loss=0.5463 | TestAcc=0.342 | F1=0.345 | AUC=0.646 | Time=14.19s\n",
            "Epoch 119 | Loss=0.5500 | TestAcc=0.408 | F1=0.413 | AUC=0.743 | Time=14.30s\n",
            "Epoch 120 | Loss=0.5163 | TestAcc=0.425 | F1=0.422 | AUC=0.744 | Time=14.41s\n",
            "Epoch 121 | Loss=0.5204 | TestAcc=0.392 | F1=0.392 | AUC=0.736 | Time=14.52s\n",
            "Epoch 122 | Loss=0.4685 | TestAcc=0.467 | F1=0.471 | AUC=0.730 | Time=14.65s\n",
            "Epoch 123 | Loss=0.4570 | TestAcc=0.450 | F1=0.448 | AUC=0.741 | Time=14.77s\n",
            "Epoch 124 | Loss=0.4315 | TestAcc=0.467 | F1=0.474 | AUC=0.756 | Time=14.89s\n",
            "Epoch 125 | Loss=0.3762 | TestAcc=0.475 | F1=0.472 | AUC=0.762 | Time=15.00s\n",
            "Epoch 126 | Loss=0.3923 | TestAcc=0.433 | F1=0.444 | AUC=0.729 | Time=15.11s\n",
            "Epoch 127 | Loss=0.4308 | TestAcc=0.408 | F1=0.402 | AUC=0.745 | Time=15.22s\n",
            "Epoch 128 | Loss=0.4292 | TestAcc=0.417 | F1=0.419 | AUC=0.716 | Time=15.33s\n",
            "Epoch 129 | Loss=0.3819 | TestAcc=0.483 | F1=0.484 | AUC=0.740 | Time=15.44s\n",
            "Epoch 130 | Loss=0.5128 | TestAcc=0.492 | F1=0.491 | AUC=0.744 | Time=15.55s\n",
            "Epoch 131 | Loss=0.4151 | TestAcc=0.458 | F1=0.470 | AUC=0.720 | Time=15.66s\n",
            "Epoch 132 | Loss=0.4124 | TestAcc=0.367 | F1=0.353 | AUC=0.728 | Time=15.79s\n",
            "Epoch 133 | Loss=0.4461 | TestAcc=0.450 | F1=0.449 | AUC=0.754 | Time=15.90s\n",
            "Epoch 134 | Loss=0.4286 | TestAcc=0.492 | F1=0.495 | AUC=0.749 | Time=16.01s\n",
            "Epoch 135 | Loss=0.3949 | TestAcc=0.483 | F1=0.489 | AUC=0.745 | Time=16.12s\n",
            "Epoch 136 | Loss=0.3605 | TestAcc=0.475 | F1=0.473 | AUC=0.743 | Time=16.23s\n",
            "Epoch 137 | Loss=0.3631 | TestAcc=0.400 | F1=0.392 | AUC=0.727 | Time=16.34s\n",
            "Epoch 138 | Loss=0.3374 | TestAcc=0.458 | F1=0.461 | AUC=0.752 | Time=16.45s\n",
            "Epoch 139 | Loss=0.3950 | TestAcc=0.425 | F1=0.426 | AUC=0.707 | Time=16.56s\n",
            "Epoch 140 | Loss=0.4347 | TestAcc=0.450 | F1=0.438 | AUC=0.759 | Time=16.67s\n",
            "Epoch 141 | Loss=0.3979 | TestAcc=0.425 | F1=0.430 | AUC=0.730 | Time=16.80s\n",
            "Epoch 142 | Loss=0.4365 | TestAcc=0.458 | F1=0.459 | AUC=0.735 | Time=16.91s\n",
            "Epoch 143 | Loss=0.4102 | TestAcc=0.417 | F1=0.408 | AUC=0.739 | Time=17.02s\n",
            "Epoch 144 | Loss=0.3621 | TestAcc=0.458 | F1=0.452 | AUC=0.709 | Time=17.13s\n",
            "Epoch 145 | Loss=0.3687 | TestAcc=0.392 | F1=0.393 | AUC=0.731 | Time=17.24s\n",
            "Epoch 146 | Loss=0.3971 | TestAcc=0.442 | F1=0.450 | AUC=0.748 | Time=17.35s\n",
            "Epoch 147 | Loss=0.4165 | TestAcc=0.433 | F1=0.434 | AUC=0.722 | Time=17.46s\n",
            "Epoch 148 | Loss=0.4074 | TestAcc=0.400 | F1=0.408 | AUC=0.720 | Time=17.57s\n",
            "Epoch 149 | Loss=0.4489 | TestAcc=0.467 | F1=0.468 | AUC=0.766 | Time=17.69s\n",
            "Epoch 150 | Loss=0.4087 | TestAcc=0.483 | F1=0.483 | AUC=0.753 | Time=17.86s\n",
            "Epoch 151 | Loss=0.3685 | TestAcc=0.517 | F1=0.518 | AUC=0.753 | Time=18.01s\n",
            "Epoch 152 | Loss=0.3848 | TestAcc=0.417 | F1=0.418 | AUC=0.721 | Time=18.16s\n",
            "Epoch 153 | Loss=0.3343 | TestAcc=0.475 | F1=0.484 | AUC=0.734 | Time=18.31s\n",
            "Epoch 154 | Loss=0.3708 | TestAcc=0.408 | F1=0.409 | AUC=0.734 | Time=18.46s\n",
            "Epoch 155 | Loss=0.3260 | TestAcc=0.450 | F1=0.452 | AUC=0.710 | Time=18.61s\n",
            "Epoch 156 | Loss=0.2966 | TestAcc=0.492 | F1=0.491 | AUC=0.745 | Time=18.75s\n",
            "Epoch 157 | Loss=0.3232 | TestAcc=0.483 | F1=0.483 | AUC=0.744 | Time=18.90s\n",
            "Epoch 158 | Loss=0.3259 | TestAcc=0.467 | F1=0.466 | AUC=0.743 | Time=19.05s\n",
            "Epoch 159 | Loss=0.2570 | TestAcc=0.475 | F1=0.472 | AUC=0.761 | Time=19.20s\n",
            "Epoch 160 | Loss=0.3006 | TestAcc=0.450 | F1=0.448 | AUC=0.733 | Time=19.35s\n",
            "Epoch 161 | Loss=0.3541 | TestAcc=0.425 | F1=0.433 | AUC=0.729 | Time=19.50s\n",
            "Epoch 162 | Loss=0.3440 | TestAcc=0.500 | F1=0.495 | AUC=0.742 | Time=19.65s\n",
            "Epoch 163 | Loss=0.3041 | TestAcc=0.475 | F1=0.474 | AUC=0.770 | Time=19.78s\n",
            "Epoch 164 | Loss=0.2772 | TestAcc=0.492 | F1=0.496 | AUC=0.734 | Time=19.94s\n",
            "Epoch 165 | Loss=0.3338 | TestAcc=0.450 | F1=0.448 | AUC=0.749 | Time=20.12s\n",
            "Epoch 166 | Loss=0.2977 | TestAcc=0.450 | F1=0.457 | AUC=0.732 | Time=20.29s\n",
            "Epoch 167 | Loss=0.2678 | TestAcc=0.475 | F1=0.476 | AUC=0.756 | Time=20.45s\n",
            "Epoch 168 | Loss=0.3076 | TestAcc=0.458 | F1=0.463 | AUC=0.735 | Time=20.63s\n",
            "Epoch 169 | Loss=0.3100 | TestAcc=0.500 | F1=0.501 | AUC=0.754 | Time=20.78s\n",
            "Epoch 170 | Loss=0.4260 | TestAcc=0.475 | F1=0.472 | AUC=0.744 | Time=20.89s\n",
            "Epoch 171 | Loss=0.3120 | TestAcc=0.467 | F1=0.474 | AUC=0.733 | Time=21.01s\n",
            "Epoch 172 | Loss=0.3527 | TestAcc=0.442 | F1=0.442 | AUC=0.773 | Time=21.13s\n",
            "Epoch 173 | Loss=0.3601 | TestAcc=0.442 | F1=0.438 | AUC=0.716 | Time=21.24s\n",
            "Epoch 174 | Loss=0.2774 | TestAcc=0.417 | F1=0.416 | AUC=0.758 | Time=21.35s\n",
            "Epoch 175 | Loss=0.2910 | TestAcc=0.467 | F1=0.465 | AUC=0.761 | Time=21.46s\n",
            "Epoch 176 | Loss=0.2754 | TestAcc=0.500 | F1=0.499 | AUC=0.755 | Time=21.57s\n",
            "Epoch 177 | Loss=0.3049 | TestAcc=0.467 | F1=0.471 | AUC=0.744 | Time=21.68s\n",
            "Epoch 178 | Loss=0.3132 | TestAcc=0.483 | F1=0.485 | AUC=0.768 | Time=21.79s\n",
            "Epoch 179 | Loss=0.2689 | TestAcc=0.475 | F1=0.480 | AUC=0.743 | Time=21.90s\n",
            "Epoch 180 | Loss=0.2648 | TestAcc=0.408 | F1=0.411 | AUC=0.700 | Time=22.02s\n",
            "Epoch 181 | Loss=0.2557 | TestAcc=0.475 | F1=0.477 | AUC=0.742 | Time=22.14s\n",
            "Epoch 182 | Loss=0.2743 | TestAcc=0.475 | F1=0.483 | AUC=0.747 | Time=22.26s\n",
            "Epoch 183 | Loss=0.3466 | TestAcc=0.450 | F1=0.458 | AUC=0.746 | Time=22.37s\n",
            "Epoch 184 | Loss=0.2510 | TestAcc=0.475 | F1=0.482 | AUC=0.767 | Time=22.48s\n",
            "Epoch 185 | Loss=0.2694 | TestAcc=0.450 | F1=0.458 | AUC=0.761 | Time=22.59s\n",
            "Epoch 186 | Loss=0.2511 | TestAcc=0.475 | F1=0.467 | AUC=0.744 | Time=22.70s\n",
            "Epoch 187 | Loss=0.3039 | TestAcc=0.467 | F1=0.465 | AUC=0.739 | Time=22.81s\n",
            "Epoch 188 | Loss=0.2522 | TestAcc=0.500 | F1=0.504 | AUC=0.774 | Time=22.92s\n",
            "Epoch 189 | Loss=0.3098 | TestAcc=0.450 | F1=0.446 | AUC=0.740 | Time=23.03s\n",
            "Epoch 190 | Loss=0.2423 | TestAcc=0.500 | F1=0.501 | AUC=0.767 | Time=23.16s\n",
            "Epoch 191 | Loss=0.2320 | TestAcc=0.483 | F1=0.490 | AUC=0.758 | Time=23.27s\n",
            "Epoch 192 | Loss=0.2978 | TestAcc=0.500 | F1=0.504 | AUC=0.740 | Time=23.38s\n",
            "Epoch 193 | Loss=0.2698 | TestAcc=0.450 | F1=0.453 | AUC=0.734 | Time=23.49s\n",
            "Epoch 194 | Loss=0.3138 | TestAcc=0.433 | F1=0.427 | AUC=0.757 | Time=23.60s\n",
            "Epoch 195 | Loss=0.2648 | TestAcc=0.500 | F1=0.506 | AUC=0.781 | Time=23.71s\n",
            "Epoch 196 | Loss=0.2114 | TestAcc=0.508 | F1=0.509 | AUC=0.770 | Time=23.83s\n",
            "Epoch 197 | Loss=0.2312 | TestAcc=0.492 | F1=0.488 | AUC=0.756 | Time=23.94s\n",
            "Epoch 198 | Loss=0.2736 | TestAcc=0.475 | F1=0.470 | AUC=0.727 | Time=24.05s\n",
            "Epoch 199 | Loss=0.2985 | TestAcc=0.433 | F1=0.441 | AUC=0.712 | Time=24.18s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:12:12,991] A new study created in memory with name: no-name-245344f0-6b18-4416-99d8-93e0adf7b0f8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 200 | Loss=0.3062 | TestAcc=0.492 | F1=0.492 | AUC=0.756 | Time=24.29s\n",
            "\n",
            "Training summary stored in : /content/drive/MyDrive/InformationSystems/Classification/results/classification/gin.csv\n",
            "  method  seed  dataset optimization_enabled  embedding_dimension  \\\n",
            "0    GIN    43  ENZYMES                  yes                   64   \n",
            "\n",
            "  objective_weights  num_layers   dropout        lr  weight_decay  epochs  \\\n",
            "0     (0.5,0.3,0.2)           3  0.037784  0.000257      0.000197     200   \n",
            "\n",
            "   best_epoch  best_loss  eval_loss  eval_acc  eval_f1  eval_auc  \\\n",
            "0         151     2.3662     2.3662    0.5167   0.5183    0.7534   \n",
            "\n",
            "   training_time (s)  generation_time (s)  memory_usage (MB)  \n",
            "0              24.29                 6.14            1461.68  \n",
            "Saved model: /content/drive/MyDrive/InformationSystems/Classification/models/GIN_ENZYMES_43.pth\n",
            "Loaded dataset ENZYMES: 600 graphs, 3 node features, 6 classes\n",
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:12:13,516] Trial 0 finished with value: 0.37530853045219115 and parameters: {'num_layers': 3, 'dropout': 0.09659127268405955, 'lr': 0.0031172286936856372, 'weight_decay': 0.00014892683747394522}. Best is trial 0 with value: 0.37530853045219115.\n",
            "[I 2026-01-22 11:12:14,008] Trial 1 finished with value: 0.4212079683392995 and parameters: {'num_layers': 3, 'dropout': 0.056571795849207705, 'lr': 0.0006771045594229636, 'weight_decay': 4.894397043329903e-05}. Best is trial 1 with value: 0.4212079683392995.\n",
            "[I 2026-01-22 11:12:14,647] Trial 2 finished with value: 0.4392296413040872 and parameters: {'num_layers': 5, 'dropout': 0.2529907080594751, 'lr': 0.0006543848738445222, 'weight_decay': 0.0007382976535253114}. Best is trial 2 with value: 0.4392296413040872.\n",
            "[I 2026-01-22 11:12:15,349] Trial 3 finished with value: 0.29442014933763894 and parameters: {'num_layers': 6, 'dropout': 0.32241583681395164, 'lr': 0.00029129966765916533, 'weight_decay': 6.404629200350204e-05}. Best is trial 2 with value: 0.4392296413040872.\n",
            "[I 2026-01-22 11:12:15,841] Trial 4 finished with value: 0.35590412600120724 and parameters: {'num_layers': 3, 'dropout': 0.17113963003117563, 'lr': 0.0005796930666936042, 'weight_decay': 1.0978511438315034e-05}. Best is trial 2 with value: 0.4392296413040872.\n",
            "[I 2026-01-22 11:12:16,524] Trial 5 finished with value: 0.32473050731492836 and parameters: {'num_layers': 6, 'dropout': 0.28294050782845154, 'lr': 0.006520436010434682, 'weight_decay': 2.932460312314146e-05}. Best is trial 2 with value: 0.4392296413040872.\n",
            "[I 2026-01-22 11:12:17,092] Trial 6 finished with value: 0.3179597396448993 and parameters: {'num_layers': 4, 'dropout': 0.13810109766678458, 'lr': 0.0001053862104800805, 'weight_decay': 2.0349746990209954e-05}. Best is trial 2 with value: 0.4392296413040872.\n",
            "[I 2026-01-22 11:12:17,641] Trial 7 finished with value: 0.38386671899715963 and parameters: {'num_layers': 4, 'dropout': 0.1221383370732078, 'lr': 0.008830937051127296, 'weight_decay': 0.0002585338033724637}. Best is trial 2 with value: 0.4392296413040872.\n",
            "[I 2026-01-22 11:12:18,281] Trial 8 finished with value: 0.3127729637333936 and parameters: {'num_layers': 5, 'dropout': 0.04995294731778605, 'lr': 0.0001936792665894558, 'weight_decay': 0.000494740221609113}. Best is trial 2 with value: 0.4392296413040872.\n",
            "[I 2026-01-22 11:12:18,758] Trial 9 finished with value: 0.3913096311488349 and parameters: {'num_layers': 3, 'dropout': 0.5232911874922971, 'lr': 0.00023451445260218296, 'weight_decay': 2.310978840586503e-05}. Best is trial 2 with value: 0.4392296413040872.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'num_layers': 5, 'dropout': 0.2529907080594751, 'lr': 0.0006543848738445222, 'weight_decay': 0.0007382976535253114}\n",
            "\n",
            "Running final training GIN...\n",
            "{'num_layers': 5, 'dropout': 0.2529907080594751, 'lr': 0.0006543848738445222, 'weight_decay': 0.0007382976535253114}\n",
            "Epoch 001 | Loss=5.1149 | TestAcc=0.150 | F1=0.061 | AUC=0.509 | Time=0.16s\n",
            "Epoch 002 | Loss=3.6571 | TestAcc=0.208 | F1=0.183 | AUC=0.565 | Time=0.30s\n",
            "Epoch 003 | Loss=3.3292 | TestAcc=0.267 | F1=0.258 | AUC=0.616 | Time=0.45s\n",
            "Epoch 004 | Loss=3.0345 | TestAcc=0.208 | F1=0.215 | AUC=0.629 | Time=0.61s\n",
            "Epoch 005 | Loss=2.7308 | TestAcc=0.267 | F1=0.269 | AUC=0.623 | Time=0.81s\n",
            "Epoch 006 | Loss=2.3758 | TestAcc=0.275 | F1=0.279 | AUC=0.644 | Time=1.01s\n",
            "Epoch 007 | Loss=2.3289 | TestAcc=0.283 | F1=0.278 | AUC=0.652 | Time=1.22s\n",
            "Epoch 008 | Loss=2.1634 | TestAcc=0.342 | F1=0.333 | AUC=0.690 | Time=1.40s\n",
            "Epoch 009 | Loss=1.9345 | TestAcc=0.358 | F1=0.360 | AUC=0.668 | Time=1.59s\n",
            "Epoch 010 | Loss=1.8981 | TestAcc=0.333 | F1=0.337 | AUC=0.677 | Time=1.77s\n",
            "Epoch 011 | Loss=1.8040 | TestAcc=0.258 | F1=0.257 | AUC=0.596 | Time=1.97s\n",
            "Epoch 012 | Loss=1.7005 | TestAcc=0.425 | F1=0.429 | AUC=0.715 | Time=2.15s\n",
            "Epoch 013 | Loss=1.6326 | TestAcc=0.417 | F1=0.425 | AUC=0.728 | Time=2.37s\n",
            "Epoch 014 | Loss=1.6526 | TestAcc=0.467 | F1=0.461 | AUC=0.733 | Time=2.55s\n",
            "Epoch 015 | Loss=1.5071 | TestAcc=0.475 | F1=0.481 | AUC=0.753 | Time=2.75s\n",
            "Epoch 016 | Loss=1.6376 | TestAcc=0.392 | F1=0.400 | AUC=0.743 | Time=2.96s\n",
            "Epoch 017 | Loss=1.5664 | TestAcc=0.425 | F1=0.420 | AUC=0.744 | Time=3.17s\n",
            "Epoch 018 | Loss=1.4544 | TestAcc=0.400 | F1=0.398 | AUC=0.745 | Time=3.40s\n",
            "Epoch 019 | Loss=1.4723 | TestAcc=0.433 | F1=0.437 | AUC=0.745 | Time=3.59s\n",
            "Epoch 020 | Loss=1.4442 | TestAcc=0.433 | F1=0.437 | AUC=0.765 | Time=3.74s\n",
            "Epoch 021 | Loss=1.3407 | TestAcc=0.425 | F1=0.423 | AUC=0.760 | Time=3.89s\n",
            "Epoch 022 | Loss=1.3894 | TestAcc=0.492 | F1=0.494 | AUC=0.778 | Time=4.03s\n",
            "Epoch 023 | Loss=1.3700 | TestAcc=0.467 | F1=0.466 | AUC=0.774 | Time=4.17s\n",
            "Epoch 024 | Loss=1.3570 | TestAcc=0.467 | F1=0.468 | AUC=0.779 | Time=4.31s\n",
            "Epoch 025 | Loss=1.3116 | TestAcc=0.450 | F1=0.455 | AUC=0.785 | Time=4.46s\n",
            "Epoch 026 | Loss=1.3214 | TestAcc=0.475 | F1=0.484 | AUC=0.774 | Time=4.61s\n",
            "Epoch 027 | Loss=1.3355 | TestAcc=0.442 | F1=0.452 | AUC=0.778 | Time=4.75s\n",
            "Epoch 028 | Loss=1.2108 | TestAcc=0.467 | F1=0.468 | AUC=0.775 | Time=4.89s\n",
            "Epoch 029 | Loss=1.2210 | TestAcc=0.508 | F1=0.510 | AUC=0.775 | Time=5.03s\n",
            "Epoch 030 | Loss=1.2229 | TestAcc=0.408 | F1=0.413 | AUC=0.744 | Time=5.17s\n",
            "Epoch 031 | Loss=1.2264 | TestAcc=0.450 | F1=0.456 | AUC=0.734 | Time=5.31s\n",
            "Epoch 032 | Loss=1.2450 | TestAcc=0.500 | F1=0.503 | AUC=0.765 | Time=5.47s\n",
            "Epoch 033 | Loss=1.1976 | TestAcc=0.408 | F1=0.408 | AUC=0.779 | Time=5.61s\n",
            "Epoch 034 | Loss=1.1986 | TestAcc=0.475 | F1=0.470 | AUC=0.772 | Time=5.75s\n",
            "Epoch 035 | Loss=1.0699 | TestAcc=0.467 | F1=0.467 | AUC=0.776 | Time=5.89s\n",
            "Epoch 036 | Loss=1.2741 | TestAcc=0.450 | F1=0.457 | AUC=0.762 | Time=6.04s\n",
            "Epoch 037 | Loss=1.1883 | TestAcc=0.383 | F1=0.389 | AUC=0.751 | Time=6.18s\n",
            "Epoch 038 | Loss=1.1258 | TestAcc=0.433 | F1=0.428 | AUC=0.744 | Time=6.32s\n",
            "Epoch 039 | Loss=1.1472 | TestAcc=0.500 | F1=0.504 | AUC=0.762 | Time=6.48s\n",
            "Epoch 040 | Loss=1.0497 | TestAcc=0.467 | F1=0.474 | AUC=0.769 | Time=6.62s\n",
            "Epoch 041 | Loss=1.0839 | TestAcc=0.425 | F1=0.435 | AUC=0.750 | Time=6.75s\n",
            "Epoch 042 | Loss=1.1255 | TestAcc=0.508 | F1=0.506 | AUC=0.764 | Time=6.89s\n",
            "Epoch 043 | Loss=1.0778 | TestAcc=0.542 | F1=0.543 | AUC=0.776 | Time=7.03s\n",
            "Epoch 044 | Loss=0.9677 | TestAcc=0.542 | F1=0.536 | AUC=0.784 | Time=7.17s\n",
            "Epoch 045 | Loss=1.0290 | TestAcc=0.500 | F1=0.502 | AUC=0.774 | Time=7.31s\n",
            "Epoch 046 | Loss=1.0008 | TestAcc=0.458 | F1=0.458 | AUC=0.760 | Time=7.47s\n",
            "Epoch 047 | Loss=0.9562 | TestAcc=0.458 | F1=0.439 | AUC=0.762 | Time=7.61s\n",
            "Epoch 048 | Loss=1.0748 | TestAcc=0.475 | F1=0.476 | AUC=0.767 | Time=7.75s\n",
            "Epoch 049 | Loss=1.0954 | TestAcc=0.483 | F1=0.494 | AUC=0.773 | Time=7.88s\n",
            "Epoch 050 | Loss=1.1363 | TestAcc=0.475 | F1=0.481 | AUC=0.756 | Time=8.02s\n",
            "Epoch 051 | Loss=1.0378 | TestAcc=0.417 | F1=0.422 | AUC=0.760 | Time=8.16s\n",
            "Epoch 052 | Loss=0.9464 | TestAcc=0.450 | F1=0.445 | AUC=0.770 | Time=8.31s\n",
            "Epoch 053 | Loss=0.9629 | TestAcc=0.492 | F1=0.495 | AUC=0.769 | Time=8.45s\n",
            "Epoch 054 | Loss=0.9848 | TestAcc=0.467 | F1=0.474 | AUC=0.734 | Time=8.61s\n",
            "Epoch 055 | Loss=0.8910 | TestAcc=0.517 | F1=0.526 | AUC=0.780 | Time=8.75s\n",
            "Epoch 056 | Loss=0.9586 | TestAcc=0.483 | F1=0.486 | AUC=0.773 | Time=8.89s\n",
            "Epoch 057 | Loss=0.9961 | TestAcc=0.442 | F1=0.450 | AUC=0.751 | Time=9.03s\n",
            "Epoch 058 | Loss=1.0510 | TestAcc=0.508 | F1=0.510 | AUC=0.787 | Time=9.17s\n",
            "Epoch 059 | Loss=0.8448 | TestAcc=0.542 | F1=0.542 | AUC=0.788 | Time=9.31s\n",
            "Epoch 060 | Loss=0.8985 | TestAcc=0.508 | F1=0.514 | AUC=0.776 | Time=9.46s\n",
            "Epoch 061 | Loss=0.9066 | TestAcc=0.458 | F1=0.462 | AUC=0.765 | Time=9.61s\n",
            "Epoch 062 | Loss=0.8842 | TestAcc=0.517 | F1=0.523 | AUC=0.764 | Time=9.75s\n",
            "Epoch 063 | Loss=0.8713 | TestAcc=0.567 | F1=0.574 | AUC=0.793 | Time=9.89s\n",
            "Epoch 064 | Loss=0.8687 | TestAcc=0.467 | F1=0.467 | AUC=0.749 | Time=10.03s\n",
            "Epoch 065 | Loss=0.8412 | TestAcc=0.483 | F1=0.486 | AUC=0.779 | Time=10.17s\n",
            "Epoch 066 | Loss=0.8344 | TestAcc=0.492 | F1=0.498 | AUC=0.787 | Time=10.32s\n",
            "Epoch 067 | Loss=0.8216 | TestAcc=0.450 | F1=0.441 | AUC=0.748 | Time=10.46s\n",
            "Epoch 068 | Loss=0.8327 | TestAcc=0.467 | F1=0.467 | AUC=0.772 | Time=10.61s\n",
            "Epoch 069 | Loss=0.7653 | TestAcc=0.392 | F1=0.384 | AUC=0.742 | Time=10.74s\n",
            "Epoch 070 | Loss=0.8523 | TestAcc=0.425 | F1=0.437 | AUC=0.741 | Time=10.88s\n",
            "Epoch 071 | Loss=0.8306 | TestAcc=0.458 | F1=0.461 | AUC=0.755 | Time=11.02s\n",
            "Epoch 072 | Loss=0.8524 | TestAcc=0.458 | F1=0.458 | AUC=0.749 | Time=11.16s\n",
            "Epoch 073 | Loss=0.7554 | TestAcc=0.517 | F1=0.532 | AUC=0.769 | Time=11.30s\n",
            "Epoch 074 | Loss=0.7697 | TestAcc=0.508 | F1=0.514 | AUC=0.782 | Time=11.44s\n",
            "Epoch 075 | Loss=0.7668 | TestAcc=0.525 | F1=0.521 | AUC=0.787 | Time=11.59s\n",
            "Epoch 076 | Loss=0.8758 | TestAcc=0.483 | F1=0.488 | AUC=0.759 | Time=11.73s\n",
            "Epoch 077 | Loss=0.7732 | TestAcc=0.475 | F1=0.480 | AUC=0.745 | Time=11.87s\n",
            "Epoch 078 | Loss=0.8144 | TestAcc=0.442 | F1=0.432 | AUC=0.753 | Time=12.00s\n",
            "Epoch 079 | Loss=0.7682 | TestAcc=0.433 | F1=0.420 | AUC=0.758 | Time=12.14s\n",
            "Epoch 080 | Loss=0.9121 | TestAcc=0.475 | F1=0.480 | AUC=0.762 | Time=12.28s\n",
            "Epoch 081 | Loss=0.8282 | TestAcc=0.500 | F1=0.513 | AUC=0.771 | Time=12.42s\n",
            "Epoch 082 | Loss=0.8601 | TestAcc=0.467 | F1=0.470 | AUC=0.759 | Time=12.56s\n",
            "Epoch 083 | Loss=0.7701 | TestAcc=0.508 | F1=0.518 | AUC=0.780 | Time=12.71s\n",
            "Epoch 084 | Loss=0.7805 | TestAcc=0.475 | F1=0.478 | AUC=0.778 | Time=12.85s\n",
            "Epoch 085 | Loss=0.7994 | TestAcc=0.500 | F1=0.509 | AUC=0.785 | Time=12.99s\n",
            "Epoch 086 | Loss=0.7773 | TestAcc=0.525 | F1=0.528 | AUC=0.783 | Time=13.13s\n",
            "Epoch 087 | Loss=0.6808 | TestAcc=0.500 | F1=0.506 | AUC=0.765 | Time=13.27s\n",
            "Epoch 088 | Loss=0.7924 | TestAcc=0.383 | F1=0.395 | AUC=0.720 | Time=13.42s\n",
            "Epoch 089 | Loss=0.7567 | TestAcc=0.517 | F1=0.517 | AUC=0.785 | Time=13.56s\n",
            "Epoch 090 | Loss=0.6874 | TestAcc=0.533 | F1=0.533 | AUC=0.782 | Time=13.79s\n",
            "Epoch 091 | Loss=0.6980 | TestAcc=0.458 | F1=0.462 | AUC=0.750 | Time=13.97s\n",
            "Epoch 092 | Loss=0.7848 | TestAcc=0.450 | F1=0.439 | AUC=0.757 | Time=14.18s\n",
            "Epoch 093 | Loss=0.7034 | TestAcc=0.475 | F1=0.472 | AUC=0.786 | Time=14.36s\n",
            "Epoch 094 | Loss=0.7073 | TestAcc=0.508 | F1=0.518 | AUC=0.776 | Time=14.55s\n",
            "Epoch 095 | Loss=0.6246 | TestAcc=0.550 | F1=0.551 | AUC=0.798 | Time=14.75s\n",
            "Epoch 096 | Loss=0.6965 | TestAcc=0.458 | F1=0.466 | AUC=0.760 | Time=14.94s\n",
            "Epoch 097 | Loss=0.7276 | TestAcc=0.542 | F1=0.546 | AUC=0.783 | Time=15.13s\n",
            "Epoch 098 | Loss=0.7486 | TestAcc=0.492 | F1=0.501 | AUC=0.793 | Time=15.32s\n",
            "Epoch 099 | Loss=0.6697 | TestAcc=0.483 | F1=0.475 | AUC=0.765 | Time=15.50s\n",
            "Epoch 100 | Loss=0.6155 | TestAcc=0.517 | F1=0.525 | AUC=0.764 | Time=15.68s\n",
            "Epoch 101 | Loss=0.6887 | TestAcc=0.542 | F1=0.544 | AUC=0.795 | Time=15.92s\n",
            "Epoch 102 | Loss=0.6176 | TestAcc=0.592 | F1=0.596 | AUC=0.825 | Time=16.13s\n",
            "Epoch 103 | Loss=0.5632 | TestAcc=0.517 | F1=0.523 | AUC=0.807 | Time=16.34s\n",
            "Epoch 104 | Loss=0.6097 | TestAcc=0.575 | F1=0.575 | AUC=0.802 | Time=16.57s\n",
            "Epoch 105 | Loss=0.5997 | TestAcc=0.517 | F1=0.527 | AUC=0.795 | Time=16.71s\n",
            "Epoch 106 | Loss=0.6302 | TestAcc=0.517 | F1=0.524 | AUC=0.779 | Time=16.86s\n",
            "Epoch 107 | Loss=0.6762 | TestAcc=0.400 | F1=0.396 | AUC=0.716 | Time=17.00s\n",
            "Epoch 108 | Loss=0.6977 | TestAcc=0.442 | F1=0.454 | AUC=0.768 | Time=17.14s\n",
            "Epoch 109 | Loss=0.6959 | TestAcc=0.417 | F1=0.414 | AUC=0.722 | Time=17.28s\n",
            "Epoch 110 | Loss=0.6614 | TestAcc=0.550 | F1=0.550 | AUC=0.805 | Time=17.42s\n",
            "Epoch 111 | Loss=0.6653 | TestAcc=0.517 | F1=0.525 | AUC=0.786 | Time=17.56s\n",
            "Epoch 112 | Loss=0.6304 | TestAcc=0.542 | F1=0.540 | AUC=0.779 | Time=17.70s\n",
            "Epoch 113 | Loss=0.5780 | TestAcc=0.467 | F1=0.475 | AUC=0.759 | Time=17.84s\n",
            "Epoch 114 | Loss=0.5996 | TestAcc=0.533 | F1=0.535 | AUC=0.777 | Time=18.00s\n",
            "Epoch 115 | Loss=0.5284 | TestAcc=0.567 | F1=0.569 | AUC=0.791 | Time=18.13s\n",
            "Epoch 116 | Loss=0.6110 | TestAcc=0.475 | F1=0.488 | AUC=0.764 | Time=18.28s\n",
            "Epoch 117 | Loss=0.5020 | TestAcc=0.508 | F1=0.511 | AUC=0.776 | Time=18.42s\n",
            "Epoch 118 | Loss=0.5992 | TestAcc=0.500 | F1=0.498 | AUC=0.771 | Time=18.56s\n",
            "Epoch 119 | Loss=0.5588 | TestAcc=0.433 | F1=0.443 | AUC=0.740 | Time=18.71s\n",
            "Epoch 120 | Loss=0.6519 | TestAcc=0.517 | F1=0.524 | AUC=0.792 | Time=18.85s\n",
            "Epoch 121 | Loss=0.5106 | TestAcc=0.508 | F1=0.501 | AUC=0.799 | Time=19.00s\n",
            "Epoch 122 | Loss=0.5800 | TestAcc=0.517 | F1=0.512 | AUC=0.751 | Time=19.14s\n",
            "Epoch 123 | Loss=0.6569 | TestAcc=0.442 | F1=0.443 | AUC=0.725 | Time=19.28s\n",
            "Epoch 124 | Loss=0.5848 | TestAcc=0.533 | F1=0.534 | AUC=0.775 | Time=19.43s\n",
            "Epoch 125 | Loss=0.5795 | TestAcc=0.542 | F1=0.550 | AUC=0.794 | Time=19.57s\n",
            "Epoch 126 | Loss=0.5186 | TestAcc=0.575 | F1=0.581 | AUC=0.803 | Time=19.71s\n",
            "Epoch 127 | Loss=0.5943 | TestAcc=0.550 | F1=0.552 | AUC=0.769 | Time=19.85s\n",
            "Epoch 128 | Loss=0.4998 | TestAcc=0.525 | F1=0.525 | AUC=0.776 | Time=20.00s\n",
            "Epoch 129 | Loss=0.5021 | TestAcc=0.475 | F1=0.484 | AUC=0.771 | Time=20.14s\n",
            "Epoch 130 | Loss=0.5028 | TestAcc=0.508 | F1=0.516 | AUC=0.758 | Time=20.29s\n",
            "Epoch 131 | Loss=0.5446 | TestAcc=0.442 | F1=0.440 | AUC=0.738 | Time=20.43s\n",
            "Epoch 132 | Loss=0.5679 | TestAcc=0.517 | F1=0.524 | AUC=0.779 | Time=20.57s\n",
            "Epoch 133 | Loss=0.6189 | TestAcc=0.550 | F1=0.555 | AUC=0.774 | Time=20.71s\n",
            "Epoch 134 | Loss=0.4402 | TestAcc=0.517 | F1=0.518 | AUC=0.780 | Time=20.85s\n",
            "Epoch 135 | Loss=0.6457 | TestAcc=0.517 | F1=0.522 | AUC=0.804 | Time=21.01s\n",
            "Epoch 136 | Loss=0.5074 | TestAcc=0.458 | F1=0.470 | AUC=0.754 | Time=21.15s\n",
            "Epoch 137 | Loss=0.5361 | TestAcc=0.492 | F1=0.490 | AUC=0.779 | Time=21.29s\n",
            "Epoch 138 | Loss=0.5493 | TestAcc=0.558 | F1=0.560 | AUC=0.797 | Time=21.43s\n",
            "Epoch 139 | Loss=0.5007 | TestAcc=0.517 | F1=0.523 | AUC=0.792 | Time=21.57s\n",
            "Epoch 140 | Loss=0.5291 | TestAcc=0.567 | F1=0.567 | AUC=0.815 | Time=21.71s\n",
            "Epoch 141 | Loss=0.5951 | TestAcc=0.483 | F1=0.495 | AUC=0.768 | Time=21.85s\n",
            "Epoch 142 | Loss=0.5411 | TestAcc=0.517 | F1=0.516 | AUC=0.770 | Time=22.01s\n",
            "Epoch 143 | Loss=0.5285 | TestAcc=0.475 | F1=0.481 | AUC=0.751 | Time=22.15s\n",
            "Epoch 144 | Loss=0.5599 | TestAcc=0.442 | F1=0.447 | AUC=0.776 | Time=22.29s\n",
            "Epoch 145 | Loss=0.5925 | TestAcc=0.558 | F1=0.551 | AUC=0.782 | Time=22.43s\n",
            "Epoch 146 | Loss=0.5425 | TestAcc=0.558 | F1=0.559 | AUC=0.787 | Time=22.58s\n",
            "Epoch 147 | Loss=0.4282 | TestAcc=0.533 | F1=0.539 | AUC=0.790 | Time=22.71s\n",
            "Epoch 148 | Loss=0.4104 | TestAcc=0.467 | F1=0.463 | AUC=0.744 | Time=22.86s\n",
            "Epoch 149 | Loss=0.3721 | TestAcc=0.517 | F1=0.525 | AUC=0.766 | Time=22.99s\n",
            "Epoch 150 | Loss=0.4487 | TestAcc=0.550 | F1=0.549 | AUC=0.785 | Time=23.15s\n",
            "Epoch 151 | Loss=0.3676 | TestAcc=0.517 | F1=0.523 | AUC=0.798 | Time=23.29s\n",
            "Epoch 152 | Loss=0.4458 | TestAcc=0.550 | F1=0.549 | AUC=0.777 | Time=23.43s\n",
            "Epoch 153 | Loss=0.4885 | TestAcc=0.542 | F1=0.547 | AUC=0.812 | Time=23.57s\n",
            "Epoch 154 | Loss=0.4460 | TestAcc=0.550 | F1=0.555 | AUC=0.795 | Time=23.71s\n",
            "Epoch 155 | Loss=0.4039 | TestAcc=0.508 | F1=0.513 | AUC=0.781 | Time=23.85s\n",
            "Epoch 156 | Loss=0.4729 | TestAcc=0.467 | F1=0.466 | AUC=0.750 | Time=23.99s\n",
            "Epoch 157 | Loss=0.4680 | TestAcc=0.525 | F1=0.516 | AUC=0.770 | Time=24.14s\n",
            "Epoch 158 | Loss=0.5108 | TestAcc=0.558 | F1=0.569 | AUC=0.782 | Time=24.29s\n",
            "Epoch 159 | Loss=0.4189 | TestAcc=0.525 | F1=0.535 | AUC=0.768 | Time=24.43s\n",
            "Epoch 160 | Loss=0.4757 | TestAcc=0.558 | F1=0.565 | AUC=0.796 | Time=24.57s\n",
            "Epoch 161 | Loss=0.4660 | TestAcc=0.558 | F1=0.564 | AUC=0.805 | Time=24.71s\n",
            "Epoch 162 | Loss=0.5343 | TestAcc=0.517 | F1=0.521 | AUC=0.764 | Time=24.85s\n",
            "Epoch 163 | Loss=0.5134 | TestAcc=0.458 | F1=0.453 | AUC=0.724 | Time=24.99s\n",
            "Epoch 164 | Loss=0.4905 | TestAcc=0.558 | F1=0.557 | AUC=0.777 | Time=25.14s\n",
            "Epoch 165 | Loss=0.4002 | TestAcc=0.542 | F1=0.543 | AUC=0.780 | Time=25.28s\n",
            "Epoch 166 | Loss=0.4184 | TestAcc=0.508 | F1=0.514 | AUC=0.766 | Time=25.43s\n",
            "Epoch 167 | Loss=0.5114 | TestAcc=0.508 | F1=0.515 | AUC=0.771 | Time=25.57s\n",
            "Epoch 168 | Loss=0.4787 | TestAcc=0.442 | F1=0.461 | AUC=0.748 | Time=25.75s\n",
            "Epoch 169 | Loss=0.4206 | TestAcc=0.475 | F1=0.474 | AUC=0.747 | Time=25.89s\n",
            "Epoch 170 | Loss=0.3895 | TestAcc=0.508 | F1=0.510 | AUC=0.776 | Time=26.03s\n",
            "Epoch 171 | Loss=0.3764 | TestAcc=0.467 | F1=0.473 | AUC=0.763 | Time=26.18s\n",
            "Epoch 172 | Loss=0.3612 | TestAcc=0.542 | F1=0.540 | AUC=0.809 | Time=26.33s\n",
            "Epoch 173 | Loss=0.3469 | TestAcc=0.475 | F1=0.489 | AUC=0.749 | Time=26.47s\n",
            "Epoch 174 | Loss=0.3492 | TestAcc=0.517 | F1=0.520 | AUC=0.794 | Time=26.64s\n",
            "Epoch 175 | Loss=0.3152 | TestAcc=0.533 | F1=0.539 | AUC=0.797 | Time=26.84s\n",
            "Epoch 176 | Loss=0.2840 | TestAcc=0.567 | F1=0.567 | AUC=0.789 | Time=27.03s\n",
            "Epoch 177 | Loss=0.3554 | TestAcc=0.558 | F1=0.559 | AUC=0.813 | Time=27.23s\n",
            "Epoch 178 | Loss=0.4417 | TestAcc=0.575 | F1=0.580 | AUC=0.786 | Time=27.43s\n",
            "Epoch 179 | Loss=0.3586 | TestAcc=0.567 | F1=0.569 | AUC=0.817 | Time=27.61s\n",
            "Epoch 180 | Loss=0.3148 | TestAcc=0.533 | F1=0.539 | AUC=0.787 | Time=27.79s\n",
            "Epoch 181 | Loss=0.4747 | TestAcc=0.475 | F1=0.479 | AUC=0.754 | Time=27.98s\n",
            "Epoch 182 | Loss=0.3186 | TestAcc=0.508 | F1=0.511 | AUC=0.808 | Time=28.17s\n",
            "Epoch 183 | Loss=0.5132 | TestAcc=0.375 | F1=0.369 | AUC=0.706 | Time=28.38s\n",
            "Epoch 184 | Loss=0.5727 | TestAcc=0.492 | F1=0.493 | AUC=0.773 | Time=28.56s\n",
            "Epoch 185 | Loss=0.4890 | TestAcc=0.558 | F1=0.571 | AUC=0.781 | Time=28.76s\n",
            "Epoch 186 | Loss=0.4288 | TestAcc=0.500 | F1=0.507 | AUC=0.783 | Time=29.01s\n",
            "Epoch 187 | Loss=0.3058 | TestAcc=0.500 | F1=0.504 | AUC=0.769 | Time=29.24s\n",
            "Epoch 188 | Loss=0.3644 | TestAcc=0.558 | F1=0.566 | AUC=0.797 | Time=29.50s\n",
            "Epoch 189 | Loss=0.3771 | TestAcc=0.517 | F1=0.517 | AUC=0.799 | Time=29.70s\n",
            "Epoch 190 | Loss=0.3568 | TestAcc=0.575 | F1=0.578 | AUC=0.768 | Time=29.84s\n",
            "Epoch 191 | Loss=0.3605 | TestAcc=0.550 | F1=0.543 | AUC=0.773 | Time=29.98s\n",
            "Epoch 192 | Loss=0.3780 | TestAcc=0.542 | F1=0.549 | AUC=0.807 | Time=30.13s\n",
            "Epoch 193 | Loss=0.4242 | TestAcc=0.458 | F1=0.461 | AUC=0.757 | Time=30.27s\n",
            "Epoch 194 | Loss=0.3492 | TestAcc=0.525 | F1=0.533 | AUC=0.770 | Time=30.43s\n",
            "Epoch 195 | Loss=0.3616 | TestAcc=0.525 | F1=0.522 | AUC=0.783 | Time=30.57s\n",
            "Epoch 196 | Loss=0.3816 | TestAcc=0.517 | F1=0.522 | AUC=0.792 | Time=30.71s\n",
            "Epoch 197 | Loss=0.3266 | TestAcc=0.475 | F1=0.484 | AUC=0.759 | Time=30.85s\n",
            "Epoch 198 | Loss=0.3820 | TestAcc=0.483 | F1=0.483 | AUC=0.793 | Time=31.00s\n",
            "Epoch 199 | Loss=0.3554 | TestAcc=0.542 | F1=0.547 | AUC=0.780 | Time=31.14s\n",
            "Epoch 200 | Loss=0.3868 | TestAcc=0.483 | F1=0.486 | AUC=0.776 | Time=31.28s\n",
            "\n",
            "Training summary stored in : /content/drive/MyDrive/InformationSystems/Classification/results/classification/gin.csv\n",
            "  method  seed  dataset optimization_enabled  embedding_dimension  \\\n",
            "0    GIN    44  ENZYMES                  yes                   64   \n",
            "\n",
            "  objective_weights  num_layers   dropout        lr  weight_decay  epochs  \\\n",
            "0     (0.5,0.3,0.2)           5  0.252991  0.000654      0.000738     200   \n",
            "\n",
            "   best_epoch  best_loss  eval_loss  eval_acc  eval_f1  eval_auc  \\\n",
            "0         102     1.9768     1.9768    0.5917   0.5963    0.8246   \n",
            "\n",
            "   training_time (s)  generation_time (s)  memory_usage (MB)  \n",
            "0              31.28                 5.77            1461.69  \n",
            "Saved model: /content/drive/MyDrive/InformationSystems/Classification/models/GIN_ENZYMES_44.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/MUTAG.zip\n",
            "Processing...\n",
            "Done!\n",
            "[I 2026-01-22 11:12:51,265] A new study created in memory with name: no-name-60997d75-4477-4716-ba68-7c5699f7f566\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset MUTAG: 188 graphs, 7 node features, 2 classes\n",
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:12:51,504] Trial 0 finished with value: 0.5657639862367785 and parameters: {'num_layers': 5, 'dropout': 0.23587448340248027, 'lr': 0.00031950538617386885, 'weight_decay': 4.251740969693828e-05}. Best is trial 0 with value: 0.5657639862367785.\n",
            "[I 2026-01-22 11:12:51,698] Trial 1 finished with value: 0.7096484526825084 and parameters: {'num_layers': 4, 'dropout': 0.38522350658838234, 'lr': 0.00832313817343258, 'weight_decay': 1.0416724044507338e-05}. Best is trial 1 with value: 0.7096484526825084.\n",
            "[I 2026-01-22 11:12:51,864] Trial 2 finished with value: 0.6888944904847653 and parameters: {'num_layers': 3, 'dropout': 0.2977102468353113, 'lr': 0.001592431439163522, 'weight_decay': 8.219069959959967e-05}. Best is trial 1 with value: 0.7096484526825084.\n",
            "[I 2026-01-22 11:12:52,079] Trial 3 finished with value: 0.5299096445000837 and parameters: {'num_layers': 5, 'dropout': 0.010147589130641664, 'lr': 0.0018695082632222914, 'weight_decay': 7.157635624458671e-06}. Best is trial 1 with value: 0.7096484526825084.\n",
            "[I 2026-01-22 11:12:52,313] Trial 4 finished with value: 0.3540441176470588 and parameters: {'num_layers': 5, 'dropout': 0.029125426632553374, 'lr': 0.0013527381634634566, 'weight_decay': 3.971795043790781e-06}. Best is trial 1 with value: 0.7096484526825084.\n",
            "[I 2026-01-22 11:12:52,551] Trial 5 finished with value: 0.7041156534384089 and parameters: {'num_layers': 6, 'dropout': 0.4697969212529304, 'lr': 0.003148347326916468, 'weight_decay': 1.5437739108981592e-06}. Best is trial 1 with value: 0.7096484526825084.\n",
            "[I 2026-01-22 11:12:52,790] Trial 6 finished with value: 0.8030294653746666 and parameters: {'num_layers': 6, 'dropout': 0.5519048261571232, 'lr': 0.007146656944551834, 'weight_decay': 6.692465944944054e-06}. Best is trial 6 with value: 0.8030294653746666.\n",
            "[I 2026-01-22 11:12:53,003] Trial 7 finished with value: 0.661630010856017 and parameters: {'num_layers': 5, 'dropout': 0.3373778396727044, 'lr': 0.004198690751200965, 'weight_decay': 9.749460191356626e-05}. Best is trial 6 with value: 0.8030294653746666.\n",
            "[I 2026-01-22 11:12:53,255] Trial 8 finished with value: 0.5321505408586271 and parameters: {'num_layers': 6, 'dropout': 0.5125262426806082, 'lr': 0.0005593669018661922, 'weight_decay': 0.00040509698678874406}. Best is trial 6 with value: 0.8030294653746666.\n",
            "[I 2026-01-22 11:12:53,500] Trial 9 finished with value: 0.3727571602803801 and parameters: {'num_layers': 6, 'dropout': 0.24874012171628382, 'lr': 0.0051219472638275474, 'weight_decay': 0.0008305572752801849}. Best is trial 6 with value: 0.8030294653746666.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'num_layers': 6, 'dropout': 0.5519048261571232, 'lr': 0.007146656944551834, 'weight_decay': 6.692465944944054e-06}\n",
            "\n",
            "Running final training GIN...\n",
            "{'num_layers': 6, 'dropout': 0.5519048261571232, 'lr': 0.007146656944551834, 'weight_decay': 6.692465944944054e-06}\n",
            "Epoch 001 | Loss=1.7109 | TestAcc=0.658 | F1=0.638 | AUC=0.857 | Time=0.06s\n",
            "Epoch 002 | Loss=1.4498 | TestAcc=0.553 | F1=0.467 | AUC=0.835 | Time=0.11s\n",
            "Epoch 003 | Loss=0.8523 | TestAcc=0.842 | F1=0.842 | AUC=0.885 | Time=0.16s\n",
            "Epoch 004 | Loss=0.5351 | TestAcc=0.842 | F1=0.841 | AUC=0.885 | Time=0.22s\n",
            "Epoch 005 | Loss=0.5278 | TestAcc=0.763 | F1=0.760 | AUC=0.882 | Time=0.28s\n",
            "Epoch 006 | Loss=0.4522 | TestAcc=0.763 | F1=0.762 | AUC=0.894 | Time=0.33s\n",
            "Epoch 007 | Loss=0.5566 | TestAcc=0.868 | F1=0.869 | AUC=0.882 | Time=0.39s\n",
            "Epoch 008 | Loss=0.3248 | TestAcc=0.842 | F1=0.842 | AUC=0.868 | Time=0.44s\n",
            "Epoch 009 | Loss=0.2855 | TestAcc=0.737 | F1=0.731 | AUC=0.874 | Time=0.50s\n",
            "Epoch 010 | Loss=0.2964 | TestAcc=0.789 | F1=0.785 | AUC=0.874 | Time=0.55s\n",
            "Epoch 011 | Loss=0.3413 | TestAcc=0.684 | F1=0.670 | AUC=0.871 | Time=0.61s\n",
            "Epoch 012 | Loss=0.2298 | TestAcc=0.789 | F1=0.780 | AUC=0.902 | Time=0.67s\n",
            "Epoch 013 | Loss=0.5703 | TestAcc=0.789 | F1=0.788 | AUC=0.891 | Time=0.72s\n",
            "Epoch 014 | Loss=0.3572 | TestAcc=0.842 | F1=0.843 | AUC=0.891 | Time=0.78s\n",
            "Epoch 015 | Loss=0.2789 | TestAcc=0.816 | F1=0.816 | AUC=0.863 | Time=0.84s\n",
            "Epoch 016 | Loss=0.2895 | TestAcc=0.816 | F1=0.816 | AUC=0.868 | Time=0.90s\n",
            "Epoch 017 | Loss=0.2798 | TestAcc=0.816 | F1=0.816 | AUC=0.880 | Time=0.96s\n",
            "Epoch 018 | Loss=0.2735 | TestAcc=0.868 | F1=0.869 | AUC=0.899 | Time=1.01s\n",
            "Epoch 019 | Loss=0.2360 | TestAcc=0.921 | F1=0.921 | AUC=0.899 | Time=1.07s\n",
            "Epoch 020 | Loss=0.4455 | TestAcc=0.868 | F1=0.869 | AUC=0.905 | Time=1.13s\n",
            "Epoch 021 | Loss=0.2713 | TestAcc=0.842 | F1=0.842 | AUC=0.908 | Time=1.18s\n",
            "Epoch 022 | Loss=0.3416 | TestAcc=0.789 | F1=0.788 | AUC=0.910 | Time=1.24s\n",
            "Epoch 023 | Loss=0.2745 | TestAcc=0.842 | F1=0.842 | AUC=0.905 | Time=1.30s\n",
            "Epoch 024 | Loss=0.2828 | TestAcc=0.842 | F1=0.842 | AUC=0.908 | Time=1.36s\n",
            "Epoch 025 | Loss=0.1906 | TestAcc=0.868 | F1=0.869 | AUC=0.905 | Time=1.42s\n",
            "Epoch 026 | Loss=0.2607 | TestAcc=0.816 | F1=0.815 | AUC=0.894 | Time=1.47s\n",
            "Epoch 027 | Loss=0.2934 | TestAcc=0.842 | F1=0.842 | AUC=0.880 | Time=1.53s\n",
            "Epoch 028 | Loss=0.2812 | TestAcc=0.737 | F1=0.731 | AUC=0.857 | Time=1.59s\n",
            "Epoch 029 | Loss=0.2799 | TestAcc=0.789 | F1=0.790 | AUC=0.891 | Time=1.65s\n",
            "Epoch 030 | Loss=0.2208 | TestAcc=0.868 | F1=0.868 | AUC=0.930 | Time=1.70s\n",
            "Epoch 031 | Loss=0.3119 | TestAcc=0.737 | F1=0.725 | AUC=0.882 | Time=1.76s\n",
            "Epoch 032 | Loss=0.2880 | TestAcc=0.711 | F1=0.694 | AUC=0.905 | Time=1.83s\n",
            "Epoch 033 | Loss=0.2422 | TestAcc=0.737 | F1=0.725 | AUC=0.910 | Time=1.88s\n",
            "Epoch 034 | Loss=0.2677 | TestAcc=0.816 | F1=0.813 | AUC=0.916 | Time=1.94s\n",
            "Epoch 035 | Loss=0.2171 | TestAcc=0.868 | F1=0.868 | AUC=0.894 | Time=1.99s\n",
            "Epoch 036 | Loss=0.2009 | TestAcc=0.789 | F1=0.785 | AUC=0.927 | Time=2.05s\n",
            "Epoch 037 | Loss=0.1699 | TestAcc=0.737 | F1=0.717 | AUC=0.930 | Time=2.10s\n",
            "Epoch 038 | Loss=0.3278 | TestAcc=0.763 | F1=0.749 | AUC=0.919 | Time=2.16s\n",
            "Epoch 039 | Loss=0.2029 | TestAcc=0.553 | F1=0.393 | AUC=0.500 | Time=2.21s\n",
            "Epoch 040 | Loss=0.2268 | TestAcc=0.763 | F1=0.755 | AUC=0.895 | Time=2.27s\n",
            "Epoch 041 | Loss=0.2644 | TestAcc=0.658 | F1=0.592 | AUC=0.916 | Time=2.32s\n",
            "Epoch 042 | Loss=0.2731 | TestAcc=0.737 | F1=0.717 | AUC=0.916 | Time=2.38s\n",
            "Epoch 043 | Loss=0.2352 | TestAcc=0.763 | F1=0.755 | AUC=0.910 | Time=2.44s\n",
            "Epoch 044 | Loss=0.2000 | TestAcc=0.789 | F1=0.788 | AUC=0.880 | Time=2.50s\n",
            "Epoch 045 | Loss=0.2108 | TestAcc=0.789 | F1=0.789 | AUC=0.882 | Time=2.55s\n",
            "Epoch 046 | Loss=0.2052 | TestAcc=0.711 | F1=0.701 | AUC=0.871 | Time=2.61s\n",
            "Epoch 047 | Loss=0.1637 | TestAcc=0.789 | F1=0.788 | AUC=0.877 | Time=2.66s\n",
            "Epoch 048 | Loss=0.1930 | TestAcc=0.842 | F1=0.842 | AUC=0.902 | Time=2.72s\n",
            "Epoch 049 | Loss=0.3037 | TestAcc=0.816 | F1=0.813 | AUC=0.874 | Time=2.77s\n",
            "Epoch 050 | Loss=0.3030 | TestAcc=0.737 | F1=0.731 | AUC=0.832 | Time=2.83s\n",
            "Epoch 051 | Loss=0.3383 | TestAcc=0.763 | F1=0.749 | AUC=0.877 | Time=2.90s\n",
            "Epoch 052 | Loss=0.2825 | TestAcc=0.763 | F1=0.749 | AUC=0.924 | Time=2.96s\n",
            "Epoch 053 | Loss=0.2846 | TestAcc=0.789 | F1=0.780 | AUC=0.930 | Time=3.01s\n",
            "Epoch 054 | Loss=0.2961 | TestAcc=0.763 | F1=0.755 | AUC=0.910 | Time=3.06s\n",
            "Epoch 055 | Loss=0.2484 | TestAcc=0.816 | F1=0.815 | AUC=0.888 | Time=3.12s\n",
            "Epoch 056 | Loss=0.2286 | TestAcc=0.842 | F1=0.842 | AUC=0.913 | Time=3.18s\n",
            "Epoch 057 | Loss=0.1766 | TestAcc=0.868 | F1=0.869 | AUC=0.924 | Time=3.23s\n",
            "Epoch 058 | Loss=0.2443 | TestAcc=0.842 | F1=0.842 | AUC=0.908 | Time=3.29s\n",
            "Epoch 059 | Loss=0.2810 | TestAcc=0.789 | F1=0.789 | AUC=0.846 | Time=3.34s\n",
            "Epoch 060 | Loss=0.2614 | TestAcc=0.868 | F1=0.869 | AUC=0.880 | Time=3.40s\n",
            "Epoch 061 | Loss=0.2527 | TestAcc=0.789 | F1=0.788 | AUC=0.849 | Time=3.45s\n",
            "Epoch 062 | Loss=0.2731 | TestAcc=0.737 | F1=0.731 | AUC=0.833 | Time=3.51s\n",
            "Epoch 063 | Loss=0.2169 | TestAcc=0.789 | F1=0.788 | AUC=0.840 | Time=3.56s\n",
            "Epoch 064 | Loss=0.2222 | TestAcc=0.789 | F1=0.788 | AUC=0.868 | Time=3.62s\n",
            "Epoch 065 | Loss=0.1679 | TestAcc=0.816 | F1=0.815 | AUC=0.880 | Time=3.68s\n",
            "Epoch 066 | Loss=0.2623 | TestAcc=0.789 | F1=0.785 | AUC=0.910 | Time=3.74s\n",
            "Epoch 067 | Loss=0.2180 | TestAcc=0.737 | F1=0.717 | AUC=0.888 | Time=3.79s\n",
            "Epoch 068 | Loss=0.2397 | TestAcc=0.711 | F1=0.684 | AUC=0.901 | Time=3.85s\n",
            "Epoch 069 | Loss=0.2133 | TestAcc=0.816 | F1=0.813 | AUC=0.906 | Time=3.92s\n",
            "Epoch 070 | Loss=0.2639 | TestAcc=0.868 | F1=0.869 | AUC=0.894 | Time=3.97s\n",
            "Epoch 071 | Loss=0.2341 | TestAcc=0.789 | F1=0.788 | AUC=0.922 | Time=4.03s\n",
            "Epoch 072 | Loss=0.2370 | TestAcc=0.842 | F1=0.842 | AUC=0.899 | Time=4.09s\n",
            "Epoch 073 | Loss=0.4799 | TestAcc=0.842 | F1=0.842 | AUC=0.905 | Time=4.15s\n",
            "Epoch 074 | Loss=0.2794 | TestAcc=0.816 | F1=0.816 | AUC=0.882 | Time=4.20s\n",
            "Epoch 075 | Loss=0.2450 | TestAcc=0.816 | F1=0.810 | AUC=0.930 | Time=4.26s\n",
            "Epoch 076 | Loss=0.3458 | TestAcc=0.816 | F1=0.813 | AUC=0.906 | Time=4.32s\n",
            "Epoch 077 | Loss=0.3457 | TestAcc=0.895 | F1=0.895 | AUC=0.934 | Time=4.37s\n",
            "Epoch 078 | Loss=0.2413 | TestAcc=0.895 | F1=0.895 | AUC=0.927 | Time=4.43s\n",
            "Epoch 079 | Loss=0.2468 | TestAcc=0.895 | F1=0.895 | AUC=0.938 | Time=4.48s\n",
            "Epoch 080 | Loss=0.2365 | TestAcc=0.895 | F1=0.895 | AUC=0.941 | Time=4.54s\n",
            "Epoch 081 | Loss=0.1739 | TestAcc=0.842 | F1=0.841 | AUC=0.938 | Time=4.60s\n",
            "Epoch 082 | Loss=0.1832 | TestAcc=0.868 | F1=0.868 | AUC=0.937 | Time=4.66s\n",
            "Epoch 083 | Loss=0.1587 | TestAcc=0.842 | F1=0.842 | AUC=0.933 | Time=4.71s\n",
            "Epoch 084 | Loss=0.1528 | TestAcc=0.895 | F1=0.895 | AUC=0.938 | Time=4.77s\n",
            "Epoch 085 | Loss=0.2159 | TestAcc=0.895 | F1=0.895 | AUC=0.930 | Time=4.83s\n",
            "Epoch 086 | Loss=0.2228 | TestAcc=0.895 | F1=0.895 | AUC=0.913 | Time=4.88s\n",
            "Epoch 087 | Loss=0.3479 | TestAcc=0.895 | F1=0.895 | AUC=0.908 | Time=4.97s\n",
            "Epoch 088 | Loss=0.3241 | TestAcc=0.789 | F1=0.788 | AUC=0.831 | Time=5.06s\n",
            "Epoch 089 | Loss=0.2429 | TestAcc=0.737 | F1=0.731 | AUC=0.831 | Time=5.14s\n",
            "Epoch 090 | Loss=0.2245 | TestAcc=0.816 | F1=0.813 | AUC=0.894 | Time=5.21s\n",
            "Epoch 091 | Loss=0.1496 | TestAcc=0.842 | F1=0.842 | AUC=0.913 | Time=5.29s\n",
            "Epoch 092 | Loss=0.1971 | TestAcc=0.895 | F1=0.895 | AUC=0.905 | Time=5.36s\n",
            "Epoch 093 | Loss=0.3895 | TestAcc=0.868 | F1=0.869 | AUC=0.922 | Time=5.44s\n",
            "Epoch 094 | Loss=0.2694 | TestAcc=0.737 | F1=0.717 | AUC=0.896 | Time=5.51s\n",
            "Epoch 095 | Loss=0.2424 | TestAcc=0.737 | F1=0.717 | AUC=0.894 | Time=5.59s\n",
            "Epoch 096 | Loss=0.1982 | TestAcc=0.684 | F1=0.649 | AUC=0.894 | Time=5.66s\n",
            "Epoch 097 | Loss=0.2690 | TestAcc=0.737 | F1=0.725 | AUC=0.910 | Time=5.74s\n",
            "Epoch 098 | Loss=0.2138 | TestAcc=0.816 | F1=0.813 | AUC=0.905 | Time=5.82s\n",
            "Epoch 099 | Loss=0.2563 | TestAcc=0.842 | F1=0.842 | AUC=0.894 | Time=5.89s\n",
            "Epoch 100 | Loss=0.2480 | TestAcc=0.868 | F1=0.869 | AUC=0.891 | Time=5.96s\n",
            "Epoch 101 | Loss=0.1623 | TestAcc=0.868 | F1=0.869 | AUC=0.902 | Time=6.04s\n",
            "Epoch 102 | Loss=0.2780 | TestAcc=0.816 | F1=0.816 | AUC=0.896 | Time=6.11s\n",
            "Epoch 103 | Loss=0.1935 | TestAcc=0.842 | F1=0.842 | AUC=0.894 | Time=6.19s\n",
            "Epoch 104 | Loss=0.2589 | TestAcc=0.868 | F1=0.869 | AUC=0.899 | Time=6.26s\n",
            "Epoch 105 | Loss=0.2525 | TestAcc=0.868 | F1=0.869 | AUC=0.899 | Time=6.34s\n",
            "Epoch 106 | Loss=0.1912 | TestAcc=0.842 | F1=0.842 | AUC=0.913 | Time=6.42s\n",
            "Epoch 107 | Loss=0.2529 | TestAcc=0.447 | F1=0.378 | AUC=0.448 | Time=6.49s\n",
            "Epoch 108 | Loss=0.2979 | TestAcc=0.816 | F1=0.816 | AUC=0.856 | Time=6.56s\n",
            "Epoch 109 | Loss=0.1939 | TestAcc=0.763 | F1=0.749 | AUC=0.885 | Time=6.64s\n",
            "Epoch 110 | Loss=0.1778 | TestAcc=0.816 | F1=0.805 | AUC=0.880 | Time=6.72s\n",
            "Epoch 111 | Loss=0.1882 | TestAcc=0.868 | F1=0.866 | AUC=0.923 | Time=6.79s\n",
            "Epoch 112 | Loss=0.3285 | TestAcc=0.842 | F1=0.842 | AUC=0.860 | Time=6.86s\n",
            "Epoch 113 | Loss=0.2666 | TestAcc=0.842 | F1=0.841 | AUC=0.882 | Time=6.93s\n",
            "Epoch 114 | Loss=0.2754 | TestAcc=0.842 | F1=0.842 | AUC=0.930 | Time=7.00s\n",
            "Epoch 115 | Loss=0.2518 | TestAcc=0.816 | F1=0.816 | AUC=0.790 | Time=7.10s\n",
            "Epoch 116 | Loss=0.2798 | TestAcc=0.474 | F1=0.419 | AUC=0.574 | Time=7.18s\n",
            "Epoch 117 | Loss=0.3338 | TestAcc=0.737 | F1=0.735 | AUC=0.790 | Time=7.27s\n",
            "Epoch 118 | Loss=0.2129 | TestAcc=0.789 | F1=0.789 | AUC=0.818 | Time=7.36s\n",
            "Epoch 119 | Loss=0.2293 | TestAcc=0.842 | F1=0.843 | AUC=0.819 | Time=7.44s\n",
            "Epoch 120 | Loss=0.2224 | TestAcc=0.868 | F1=0.869 | AUC=0.884 | Time=7.51s\n",
            "Epoch 121 | Loss=0.2910 | TestAcc=0.816 | F1=0.815 | AUC=0.910 | Time=7.61s\n",
            "Epoch 122 | Loss=0.1575 | TestAcc=0.868 | F1=0.869 | AUC=0.905 | Time=7.69s\n",
            "Epoch 123 | Loss=0.1933 | TestAcc=0.789 | F1=0.785 | AUC=0.902 | Time=7.78s\n",
            "Epoch 124 | Loss=0.1499 | TestAcc=0.737 | F1=0.735 | AUC=0.866 | Time=7.87s\n",
            "Epoch 125 | Loss=0.1297 | TestAcc=0.711 | F1=0.710 | AUC=0.812 | Time=7.94s\n",
            "Epoch 126 | Loss=0.2027 | TestAcc=0.789 | F1=0.789 | AUC=0.807 | Time=8.00s\n",
            "Epoch 127 | Loss=0.1661 | TestAcc=0.789 | F1=0.790 | AUC=0.849 | Time=8.05s\n",
            "Epoch 128 | Loss=0.1913 | TestAcc=0.711 | F1=0.694 | AUC=0.882 | Time=8.12s\n",
            "Epoch 129 | Loss=0.3907 | TestAcc=0.763 | F1=0.764 | AUC=0.782 | Time=8.18s\n",
            "Epoch 130 | Loss=0.2778 | TestAcc=0.868 | F1=0.869 | AUC=0.902 | Time=8.23s\n",
            "Epoch 131 | Loss=0.2359 | TestAcc=0.816 | F1=0.815 | AUC=0.919 | Time=8.29s\n",
            "Epoch 132 | Loss=0.3063 | TestAcc=0.789 | F1=0.788 | AUC=0.916 | Time=8.34s\n",
            "Epoch 133 | Loss=0.2209 | TestAcc=0.737 | F1=0.738 | AUC=0.801 | Time=8.40s\n",
            "Epoch 134 | Loss=0.2635 | TestAcc=0.842 | F1=0.842 | AUC=0.888 | Time=8.45s\n",
            "Epoch 135 | Loss=0.2334 | TestAcc=0.763 | F1=0.755 | AUC=0.905 | Time=8.51s\n",
            "Epoch 136 | Loss=0.2028 | TestAcc=0.763 | F1=0.762 | AUC=0.910 | Time=8.56s\n",
            "Epoch 137 | Loss=0.1594 | TestAcc=0.842 | F1=0.843 | AUC=0.908 | Time=8.62s\n",
            "Epoch 138 | Loss=0.2262 | TestAcc=0.868 | F1=0.869 | AUC=0.880 | Time=8.67s\n",
            "Epoch 139 | Loss=0.1794 | TestAcc=0.868 | F1=0.869 | AUC=0.891 | Time=8.73s\n",
            "Epoch 140 | Loss=0.1940 | TestAcc=0.842 | F1=0.842 | AUC=0.898 | Time=8.79s\n",
            "Epoch 141 | Loss=0.1835 | TestAcc=0.737 | F1=0.731 | AUC=0.922 | Time=8.85s\n",
            "Epoch 142 | Loss=0.2997 | TestAcc=0.763 | F1=0.749 | AUC=0.905 | Time=8.90s\n",
            "Epoch 143 | Loss=0.1638 | TestAcc=0.737 | F1=0.731 | AUC=0.868 | Time=8.95s\n",
            "Epoch 144 | Loss=0.2468 | TestAcc=0.816 | F1=0.815 | AUC=0.866 | Time=9.01s\n",
            "Epoch 145 | Loss=0.3143 | TestAcc=0.816 | F1=0.815 | AUC=0.899 | Time=9.06s\n",
            "Epoch 146 | Loss=0.1679 | TestAcc=0.789 | F1=0.788 | AUC=0.885 | Time=9.13s\n",
            "Epoch 147 | Loss=0.1625 | TestAcc=0.789 | F1=0.788 | AUC=0.885 | Time=9.18s\n",
            "Epoch 148 | Loss=0.2057 | TestAcc=0.868 | F1=0.869 | AUC=0.891 | Time=9.24s\n",
            "Epoch 149 | Loss=0.1545 | TestAcc=0.868 | F1=0.869 | AUC=0.896 | Time=9.30s\n",
            "Epoch 150 | Loss=0.1357 | TestAcc=0.816 | F1=0.815 | AUC=0.902 | Time=9.35s\n",
            "Epoch 151 | Loss=0.1483 | TestAcc=0.816 | F1=0.813 | AUC=0.913 | Time=9.41s\n",
            "Epoch 152 | Loss=0.1331 | TestAcc=0.763 | F1=0.755 | AUC=0.910 | Time=9.46s\n",
            "Epoch 153 | Loss=0.1910 | TestAcc=0.868 | F1=0.868 | AUC=0.905 | Time=9.52s\n",
            "Epoch 154 | Loss=0.1148 | TestAcc=0.816 | F1=0.815 | AUC=0.922 | Time=9.57s\n",
            "Epoch 155 | Loss=0.1191 | TestAcc=0.868 | F1=0.868 | AUC=0.924 | Time=9.63s\n",
            "Epoch 156 | Loss=0.1784 | TestAcc=0.842 | F1=0.843 | AUC=0.874 | Time=9.68s\n",
            "Epoch 157 | Loss=0.1279 | TestAcc=0.868 | F1=0.869 | AUC=0.885 | Time=9.74s\n",
            "Epoch 158 | Loss=0.1435 | TestAcc=0.895 | F1=0.895 | AUC=0.913 | Time=9.80s\n",
            "Epoch 159 | Loss=0.2167 | TestAcc=0.868 | F1=0.869 | AUC=0.894 | Time=9.86s\n",
            "Epoch 160 | Loss=0.1524 | TestAcc=0.868 | F1=0.869 | AUC=0.899 | Time=9.92s\n",
            "Epoch 161 | Loss=0.1732 | TestAcc=0.895 | F1=0.895 | AUC=0.922 | Time=9.97s\n",
            "Epoch 162 | Loss=0.1302 | TestAcc=0.842 | F1=0.841 | AUC=0.924 | Time=10.02s\n",
            "Epoch 163 | Loss=0.3241 | TestAcc=0.816 | F1=0.815 | AUC=0.913 | Time=10.08s\n",
            "Epoch 164 | Loss=0.2638 | TestAcc=0.763 | F1=0.749 | AUC=0.902 | Time=10.14s\n",
            "Epoch 165 | Loss=0.2207 | TestAcc=0.658 | F1=0.638 | AUC=0.839 | Time=10.21s\n",
            "Epoch 166 | Loss=0.4006 | TestAcc=0.763 | F1=0.755 | AUC=0.882 | Time=10.26s\n",
            "Epoch 167 | Loss=0.2297 | TestAcc=0.763 | F1=0.762 | AUC=0.838 | Time=10.31s\n",
            "Epoch 168 | Loss=0.2254 | TestAcc=0.789 | F1=0.789 | AUC=0.891 | Time=10.37s\n",
            "Epoch 169 | Loss=0.1987 | TestAcc=0.789 | F1=0.785 | AUC=0.871 | Time=10.42s\n",
            "Epoch 170 | Loss=0.2042 | TestAcc=0.684 | F1=0.661 | AUC=0.843 | Time=10.48s\n",
            "Epoch 171 | Loss=0.2311 | TestAcc=0.605 | F1=0.530 | AUC=0.826 | Time=10.53s\n",
            "Epoch 172 | Loss=0.2568 | TestAcc=0.632 | F1=0.590 | AUC=0.874 | Time=10.59s\n",
            "Epoch 173 | Loss=0.5970 | TestAcc=0.763 | F1=0.764 | AUC=0.796 | Time=10.64s\n",
            "Epoch 174 | Loss=0.3024 | TestAcc=0.421 | F1=0.361 | AUC=0.190 | Time=10.70s\n",
            "Epoch 175 | Loss=0.5056 | TestAcc=0.579 | F1=0.575 | AUC=0.503 | Time=10.76s\n",
            "Epoch 176 | Loss=0.3876 | TestAcc=0.789 | F1=0.785 | AUC=0.927 | Time=10.81s\n",
            "Epoch 177 | Loss=0.3673 | TestAcc=0.763 | F1=0.749 | AUC=0.909 | Time=10.87s\n",
            "Epoch 178 | Loss=0.3244 | TestAcc=0.711 | F1=0.684 | AUC=0.901 | Time=10.92s\n",
            "Epoch 179 | Loss=0.3549 | TestAcc=0.789 | F1=0.785 | AUC=0.927 | Time=10.98s\n",
            "Epoch 180 | Loss=0.2520 | TestAcc=0.789 | F1=0.785 | AUC=0.919 | Time=11.03s\n",
            "Epoch 181 | Loss=0.2534 | TestAcc=0.816 | F1=0.813 | AUC=0.916 | Time=11.11s\n",
            "Epoch 182 | Loss=0.2757 | TestAcc=0.895 | F1=0.895 | AUC=0.896 | Time=11.18s\n",
            "Epoch 183 | Loss=0.2476 | TestAcc=0.868 | F1=0.869 | AUC=0.896 | Time=11.24s\n",
            "Epoch 184 | Loss=0.2430 | TestAcc=0.842 | F1=0.842 | AUC=0.887 | Time=11.30s\n",
            "Epoch 185 | Loss=0.2151 | TestAcc=0.842 | F1=0.842 | AUC=0.884 | Time=11.35s\n",
            "Epoch 186 | Loss=0.2307 | TestAcc=0.816 | F1=0.815 | AUC=0.894 | Time=11.41s\n",
            "Epoch 187 | Loss=0.1822 | TestAcc=0.868 | F1=0.869 | AUC=0.908 | Time=11.46s\n",
            "Epoch 188 | Loss=0.2340 | TestAcc=0.763 | F1=0.749 | AUC=0.905 | Time=11.52s\n",
            "Epoch 189 | Loss=0.2588 | TestAcc=0.763 | F1=0.755 | AUC=0.930 | Time=11.59s\n",
            "Epoch 190 | Loss=0.1738 | TestAcc=0.763 | F1=0.755 | AUC=0.930 | Time=11.64s\n",
            "Epoch 191 | Loss=0.2032 | TestAcc=0.789 | F1=0.788 | AUC=0.930 | Time=11.70s\n",
            "Epoch 192 | Loss=0.2068 | TestAcc=0.842 | F1=0.842 | AUC=0.936 | Time=11.76s\n",
            "Epoch 193 | Loss=0.2290 | TestAcc=0.868 | F1=0.869 | AUC=0.941 | Time=11.81s\n",
            "Epoch 194 | Loss=0.2166 | TestAcc=0.842 | F1=0.842 | AUC=0.916 | Time=11.87s\n",
            "Epoch 195 | Loss=0.1532 | TestAcc=0.868 | F1=0.868 | AUC=0.896 | Time=11.92s\n",
            "Epoch 196 | Loss=0.3951 | TestAcc=0.816 | F1=0.815 | AUC=0.930 | Time=11.98s\n",
            "Epoch 197 | Loss=0.2186 | TestAcc=0.737 | F1=0.731 | AUC=0.913 | Time=12.03s\n",
            "Epoch 198 | Loss=0.2146 | TestAcc=0.711 | F1=0.701 | AUC=0.891 | Time=12.09s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:13:05,792] A new study created in memory with name: no-name-aaf48dec-597b-4950-829e-418f9f8949e4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 199 | Loss=0.2126 | TestAcc=0.816 | F1=0.815 | AUC=0.860 | Time=12.14s\n",
            "Epoch 200 | Loss=0.2484 | TestAcc=0.816 | F1=0.816 | AUC=0.857 | Time=12.20s\n",
            "\n",
            "Training summary stored in : /content/drive/MyDrive/InformationSystems/Classification/results/classification/gin.csv\n",
            "  method  seed dataset optimization_enabled  embedding_dimension  \\\n",
            "0    GIN    42   MUTAG                  yes                   64   \n",
            "\n",
            "  objective_weights  num_layers   dropout        lr  weight_decay  epochs  \\\n",
            "0     (0.5,0.3,0.2)           6  0.551905  0.007147      0.000007     200   \n",
            "\n",
            "   best_epoch  best_loss  eval_loss  eval_acc  eval_f1  eval_auc  \\\n",
            "0          19     0.5125     0.5125    0.9211   0.9212    0.8992   \n",
            "\n",
            "   training_time (s)  generation_time (s)  memory_usage (MB)  \n",
            "0               12.2                 2.24            1461.76  \n",
            "Saved model: /content/drive/MyDrive/InformationSystems/Classification/models/GIN_MUTAG_42.pth\n",
            "Loaded dataset MUTAG: 188 graphs, 7 node features, 2 classes\n",
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:13:06,050] Trial 0 finished with value: 0.7082051282051283 and parameters: {'num_layers': 6, 'dropout': 0.18582724449086532, 'lr': 0.0009899207966873249, 'weight_decay': 1.1448262879389502e-06}. Best is trial 0 with value: 0.7082051282051283.\n",
            "[I 2026-01-22 11:13:06,247] Trial 1 finished with value: 0.21490688259109308 and parameters: {'num_layers': 4, 'dropout': 0.42022575860529743, 'lr': 0.00022253030543542795, 'weight_decay': 4.657133240526548e-06}. Best is trial 0 with value: 0.7082051282051283.\n",
            "[I 2026-01-22 11:13:06,464] Trial 2 finished with value: 0.21106072874493925 and parameters: {'num_layers': 5, 'dropout': 0.3146743012414436, 'lr': 0.00035225419976977747, 'weight_decay': 1.066938499403176e-06}. Best is trial 0 with value: 0.7082051282051283.\n",
            "[I 2026-01-22 11:13:06,703] Trial 3 finished with value: 0.22965047233468283 and parameters: {'num_layers': 6, 'dropout': 0.38024445847162397, 'lr': 0.002588128422768989, 'weight_decay': 1.5152628110181373e-06}. Best is trial 0 with value: 0.7082051282051283.\n",
            "[I 2026-01-22 11:13:06,953] Trial 4 finished with value: 0.23093252361673414 and parameters: {'num_layers': 6, 'dropout': 0.5230666019927538, 'lr': 0.0029890474996496852, 'weight_decay': 1.7454429049309259e-06}. Best is trial 0 with value: 0.7082051282051283.\n",
            "[I 2026-01-22 11:13:07,118] Trial 5 finished with value: 0.8968575284364758 and parameters: {'num_layers': 3, 'dropout': 0.5025752272451087, 'lr': 0.0004369195157162352, 'weight_decay': 0.0009578053481133659}. Best is trial 5 with value: 0.8968575284364758.\n",
            "[I 2026-01-22 11:13:07,340] Trial 6 finished with value: 0.677471322537112 and parameters: {'num_layers': 5, 'dropout': 0.20327779358405823, 'lr': 0.0005932104136540711, 'weight_decay': 2.625078344603275e-05}. Best is trial 5 with value: 0.8968575284364758.\n",
            "[I 2026-01-22 11:13:07,531] Trial 7 finished with value: 0.42044739775861056 and parameters: {'num_layers': 4, 'dropout': 0.08830842449826393, 'lr': 0.0005174500773744729, 'weight_decay': 9.394354278098077e-05}. Best is trial 5 with value: 0.8968575284364758.\n",
            "[I 2026-01-22 11:13:07,722] Trial 8 finished with value: 0.2743789105631211 and parameters: {'num_layers': 4, 'dropout': 0.3020823021156448, 'lr': 0.00039324716205676476, 'weight_decay': 1.3782385364596688e-05}. Best is trial 5 with value: 0.8968575284364758.\n",
            "[I 2026-01-22 11:13:07,971] Trial 9 finished with value: 0.36278723365189536 and parameters: {'num_layers': 6, 'dropout': 0.05169030441451017, 'lr': 0.0010747853314413698, 'weight_decay': 0.00013930396731681033}. Best is trial 5 with value: 0.8968575284364758.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'num_layers': 3, 'dropout': 0.5025752272451087, 'lr': 0.0004369195157162352, 'weight_decay': 0.0009578053481133659}\n",
            "\n",
            "Running final training GIN...\n",
            "{'num_layers': 3, 'dropout': 0.5025752272451087, 'lr': 0.0004369195157162352, 'weight_decay': 0.0009578053481133659}\n",
            "Epoch 001 | Loss=1.1145 | TestAcc=0.316 | F1=0.152 | AUC=0.144 | Time=0.04s\n",
            "Epoch 002 | Loss=0.7772 | TestAcc=0.316 | F1=0.152 | AUC=0.042 | Time=0.08s\n",
            "Epoch 003 | Loss=0.6676 | TestAcc=0.263 | F1=0.169 | AUC=0.112 | Time=0.12s\n",
            "Epoch 004 | Loss=0.6047 | TestAcc=0.316 | F1=0.281 | AUC=0.186 | Time=0.16s\n",
            "Epoch 005 | Loss=0.5983 | TestAcc=0.263 | F1=0.263 | AUC=0.196 | Time=0.20s\n",
            "Epoch 006 | Loss=0.4435 | TestAcc=0.342 | F1=0.357 | AUC=0.285 | Time=0.24s\n",
            "Epoch 007 | Loss=0.3582 | TestAcc=0.447 | F1=0.467 | AUC=0.452 | Time=0.28s\n",
            "Epoch 008 | Loss=0.4419 | TestAcc=0.605 | F1=0.609 | AUC=0.529 | Time=0.33s\n",
            "Epoch 009 | Loss=0.4919 | TestAcc=0.684 | F1=0.664 | AUC=0.702 | Time=0.37s\n",
            "Epoch 010 | Loss=0.3090 | TestAcc=0.684 | F1=0.676 | AUC=0.734 | Time=0.41s\n",
            "Epoch 011 | Loss=0.3478 | TestAcc=0.763 | F1=0.760 | AUC=0.824 | Time=0.45s\n",
            "Epoch 012 | Loss=0.3259 | TestAcc=0.816 | F1=0.800 | AUC=0.949 | Time=0.49s\n",
            "Epoch 013 | Loss=0.3419 | TestAcc=0.816 | F1=0.800 | AUC=0.971 | Time=0.53s\n",
            "Epoch 014 | Loss=0.3037 | TestAcc=0.842 | F1=0.832 | AUC=0.965 | Time=0.57s\n",
            "Epoch 015 | Loss=0.2739 | TestAcc=0.895 | F1=0.892 | AUC=0.958 | Time=0.61s\n",
            "Epoch 016 | Loss=0.3174 | TestAcc=0.895 | F1=0.895 | AUC=0.958 | Time=0.65s\n",
            "Epoch 017 | Loss=0.2937 | TestAcc=0.947 | F1=0.947 | AUC=0.981 | Time=0.69s\n",
            "Epoch 018 | Loss=0.3274 | TestAcc=0.947 | F1=0.947 | AUC=0.984 | Time=0.73s\n",
            "Epoch 019 | Loss=0.2471 | TestAcc=0.947 | F1=0.947 | AUC=0.990 | Time=0.77s\n",
            "Epoch 020 | Loss=0.2582 | TestAcc=0.921 | F1=0.920 | AUC=0.994 | Time=0.81s\n",
            "Epoch 021 | Loss=0.2764 | TestAcc=0.921 | F1=0.920 | AUC=0.994 | Time=0.86s\n",
            "Epoch 022 | Loss=0.3252 | TestAcc=0.921 | F1=0.920 | AUC=0.994 | Time=0.90s\n",
            "Epoch 023 | Loss=0.2338 | TestAcc=0.947 | F1=0.946 | AUC=0.994 | Time=0.94s\n",
            "Epoch 024 | Loss=0.2685 | TestAcc=0.947 | F1=0.947 | AUC=0.994 | Time=0.98s\n",
            "Epoch 025 | Loss=0.3365 | TestAcc=0.947 | F1=0.948 | AUC=0.997 | Time=1.02s\n",
            "Epoch 026 | Loss=0.3448 | TestAcc=0.947 | F1=0.947 | AUC=0.994 | Time=1.06s\n",
            "Epoch 027 | Loss=0.2658 | TestAcc=0.947 | F1=0.947 | AUC=0.994 | Time=1.10s\n",
            "Epoch 028 | Loss=0.2828 | TestAcc=0.921 | F1=0.922 | AUC=0.984 | Time=1.14s\n",
            "Epoch 029 | Loss=0.3001 | TestAcc=0.947 | F1=0.947 | AUC=0.987 | Time=1.18s\n",
            "Epoch 030 | Loss=0.2629 | TestAcc=0.947 | F1=0.947 | AUC=0.984 | Time=1.22s\n",
            "Epoch 031 | Loss=0.2485 | TestAcc=0.921 | F1=0.920 | AUC=0.994 | Time=1.26s\n",
            "Epoch 032 | Loss=0.2036 | TestAcc=0.921 | F1=0.920 | AUC=0.994 | Time=1.30s\n",
            "Epoch 033 | Loss=0.2400 | TestAcc=0.921 | F1=0.920 | AUC=0.990 | Time=1.35s\n",
            "Epoch 034 | Loss=0.3146 | TestAcc=0.947 | F1=0.946 | AUC=0.987 | Time=1.39s\n",
            "Epoch 035 | Loss=0.2133 | TestAcc=0.947 | F1=0.947 | AUC=0.987 | Time=1.43s\n",
            "Epoch 036 | Loss=0.2852 | TestAcc=0.921 | F1=0.922 | AUC=0.965 | Time=1.47s\n",
            "Epoch 037 | Loss=0.2795 | TestAcc=0.895 | F1=0.892 | AUC=0.984 | Time=1.51s\n",
            "Epoch 038 | Loss=0.1914 | TestAcc=0.947 | F1=0.946 | AUC=0.994 | Time=1.55s\n",
            "Epoch 039 | Loss=0.1902 | TestAcc=0.947 | F1=0.946 | AUC=0.994 | Time=1.59s\n",
            "Epoch 040 | Loss=0.1971 | TestAcc=0.921 | F1=0.920 | AUC=0.994 | Time=1.63s\n",
            "Epoch 041 | Loss=0.1989 | TestAcc=0.947 | F1=0.947 | AUC=0.984 | Time=1.66s\n",
            "Epoch 042 | Loss=0.2307 | TestAcc=0.947 | F1=0.947 | AUC=0.990 | Time=1.70s\n",
            "Epoch 043 | Loss=0.2721 | TestAcc=0.947 | F1=0.947 | AUC=0.994 | Time=1.75s\n",
            "Epoch 044 | Loss=0.1859 | TestAcc=0.921 | F1=0.920 | AUC=0.990 | Time=1.78s\n",
            "Epoch 045 | Loss=0.1996 | TestAcc=0.947 | F1=0.947 | AUC=0.994 | Time=1.82s\n",
            "Epoch 046 | Loss=0.3124 | TestAcc=0.947 | F1=0.947 | AUC=0.994 | Time=1.87s\n",
            "Epoch 047 | Loss=0.1798 | TestAcc=0.947 | F1=0.947 | AUC=0.997 | Time=1.92s\n",
            "Epoch 048 | Loss=0.2132 | TestAcc=0.974 | F1=0.973 | AUC=0.994 | Time=1.96s\n",
            "Epoch 049 | Loss=0.2045 | TestAcc=0.947 | F1=0.947 | AUC=0.990 | Time=2.00s\n",
            "Epoch 050 | Loss=0.1742 | TestAcc=0.947 | F1=0.947 | AUC=0.984 | Time=2.04s\n",
            "Epoch 051 | Loss=0.1850 | TestAcc=0.947 | F1=0.947 | AUC=0.990 | Time=2.08s\n",
            "Epoch 052 | Loss=0.1533 | TestAcc=0.947 | F1=0.947 | AUC=0.981 | Time=2.12s\n",
            "Epoch 053 | Loss=0.1990 | TestAcc=0.974 | F1=0.973 | AUC=0.990 | Time=2.16s\n",
            "Epoch 054 | Loss=0.1959 | TestAcc=0.921 | F1=0.920 | AUC=0.974 | Time=2.20s\n",
            "Epoch 055 | Loss=0.2146 | TestAcc=0.895 | F1=0.888 | AUC=0.974 | Time=2.24s\n",
            "Epoch 056 | Loss=0.1681 | TestAcc=0.895 | F1=0.888 | AUC=0.978 | Time=2.28s\n",
            "Epoch 057 | Loss=0.2188 | TestAcc=0.947 | F1=0.946 | AUC=0.978 | Time=2.32s\n",
            "Epoch 058 | Loss=0.1597 | TestAcc=0.947 | F1=0.946 | AUC=0.987 | Time=2.36s\n",
            "Epoch 059 | Loss=0.1643 | TestAcc=0.947 | F1=0.947 | AUC=0.994 | Time=2.40s\n",
            "Epoch 060 | Loss=0.1522 | TestAcc=0.947 | F1=0.947 | AUC=0.994 | Time=2.45s\n",
            "Epoch 061 | Loss=0.1382 | TestAcc=0.921 | F1=0.922 | AUC=0.990 | Time=2.48s\n",
            "Epoch 062 | Loss=0.1314 | TestAcc=0.895 | F1=0.895 | AUC=0.984 | Time=2.52s\n",
            "Epoch 063 | Loss=0.1832 | TestAcc=0.895 | F1=0.892 | AUC=0.984 | Time=2.56s\n",
            "Epoch 064 | Loss=0.1462 | TestAcc=0.921 | F1=0.918 | AUC=0.981 | Time=2.60s\n",
            "Epoch 065 | Loss=0.1363 | TestAcc=0.895 | F1=0.892 | AUC=0.981 | Time=2.65s\n",
            "Epoch 066 | Loss=0.1496 | TestAcc=0.947 | F1=0.947 | AUC=0.987 | Time=2.68s\n",
            "Epoch 067 | Loss=0.1384 | TestAcc=0.947 | F1=0.947 | AUC=0.987 | Time=2.73s\n",
            "Epoch 068 | Loss=0.1624 | TestAcc=0.947 | F1=0.947 | AUC=0.990 | Time=2.76s\n",
            "Epoch 069 | Loss=0.1371 | TestAcc=0.921 | F1=0.918 | AUC=0.987 | Time=2.81s\n",
            "Epoch 070 | Loss=0.1311 | TestAcc=0.921 | F1=0.918 | AUC=0.981 | Time=2.85s\n",
            "Epoch 071 | Loss=0.1441 | TestAcc=0.921 | F1=0.918 | AUC=0.984 | Time=2.89s\n",
            "Epoch 072 | Loss=0.1134 | TestAcc=0.947 | F1=0.946 | AUC=0.994 | Time=2.94s\n",
            "Epoch 073 | Loss=0.1356 | TestAcc=0.974 | F1=0.973 | AUC=0.994 | Time=2.98s\n",
            "Epoch 074 | Loss=0.0985 | TestAcc=0.974 | F1=0.973 | AUC=0.994 | Time=3.02s\n",
            "Epoch 075 | Loss=0.1056 | TestAcc=0.974 | F1=0.973 | AUC=0.994 | Time=3.06s\n",
            "Epoch 076 | Loss=0.1165 | TestAcc=0.974 | F1=0.973 | AUC=0.994 | Time=3.10s\n",
            "Epoch 077 | Loss=0.1172 | TestAcc=0.921 | F1=0.918 | AUC=0.994 | Time=3.14s\n",
            "Epoch 078 | Loss=0.1656 | TestAcc=0.895 | F1=0.892 | AUC=0.955 | Time=3.18s\n",
            "Epoch 079 | Loss=0.1099 | TestAcc=0.895 | F1=0.888 | AUC=0.987 | Time=3.23s\n",
            "Epoch 080 | Loss=0.1406 | TestAcc=0.895 | F1=0.888 | AUC=0.994 | Time=3.27s\n",
            "Epoch 081 | Loss=0.1267 | TestAcc=0.895 | F1=0.888 | AUC=0.997 | Time=3.31s\n",
            "Epoch 082 | Loss=0.1080 | TestAcc=0.947 | F1=0.946 | AUC=1.000 | Time=3.35s\n",
            "Epoch 083 | Loss=0.1521 | TestAcc=0.947 | F1=0.946 | AUC=1.000 | Time=3.39s\n",
            "Epoch 084 | Loss=0.1162 | TestAcc=0.947 | F1=0.946 | AUC=0.997 | Time=3.43s\n",
            "Epoch 085 | Loss=0.1401 | TestAcc=0.947 | F1=0.946 | AUC=0.994 | Time=3.49s\n",
            "Epoch 086 | Loss=0.0991 | TestAcc=0.895 | F1=0.892 | AUC=0.990 | Time=3.56s\n",
            "Epoch 087 | Loss=0.3263 | TestAcc=0.921 | F1=0.920 | AUC=0.987 | Time=3.62s\n",
            "Epoch 088 | Loss=0.1469 | TestAcc=0.947 | F1=0.947 | AUC=0.990 | Time=3.67s\n",
            "Epoch 089 | Loss=0.2495 | TestAcc=0.921 | F1=0.922 | AUC=0.987 | Time=3.73s\n",
            "Epoch 090 | Loss=0.1403 | TestAcc=0.947 | F1=0.947 | AUC=0.990 | Time=3.78s\n",
            "Epoch 091 | Loss=0.1730 | TestAcc=0.974 | F1=0.973 | AUC=1.000 | Time=3.84s\n",
            "Epoch 092 | Loss=0.1591 | TestAcc=0.974 | F1=0.973 | AUC=0.997 | Time=3.89s\n",
            "Epoch 093 | Loss=0.1191 | TestAcc=0.974 | F1=0.973 | AUC=0.994 | Time=3.94s\n",
            "Epoch 094 | Loss=0.2538 | TestAcc=0.974 | F1=0.973 | AUC=0.997 | Time=4.00s\n",
            "Epoch 095 | Loss=0.2048 | TestAcc=0.947 | F1=0.947 | AUC=0.994 | Time=4.05s\n",
            "Epoch 096 | Loss=0.1465 | TestAcc=0.842 | F1=0.842 | AUC=0.952 | Time=4.10s\n",
            "Epoch 097 | Loss=0.1917 | TestAcc=0.895 | F1=0.892 | AUC=0.942 | Time=4.16s\n",
            "Epoch 098 | Loss=0.1378 | TestAcc=0.895 | F1=0.888 | AUC=0.968 | Time=4.22s\n",
            "Epoch 099 | Loss=0.1298 | TestAcc=0.842 | F1=0.824 | AUC=0.984 | Time=4.27s\n",
            "Epoch 100 | Loss=0.1375 | TestAcc=0.921 | F1=0.918 | AUC=0.987 | Time=4.33s\n",
            "Epoch 101 | Loss=0.1274 | TestAcc=0.921 | F1=0.918 | AUC=0.994 | Time=4.38s\n",
            "Epoch 102 | Loss=0.1067 | TestAcc=0.974 | F1=0.973 | AUC=0.997 | Time=4.43s\n",
            "Epoch 103 | Loss=0.1162 | TestAcc=0.974 | F1=0.973 | AUC=0.994 | Time=4.48s\n",
            "Epoch 104 | Loss=0.1378 | TestAcc=0.921 | F1=0.920 | AUC=0.981 | Time=4.53s\n",
            "Epoch 105 | Loss=0.1253 | TestAcc=0.921 | F1=0.920 | AUC=0.946 | Time=4.58s\n",
            "Epoch 106 | Loss=0.1234 | TestAcc=0.895 | F1=0.892 | AUC=0.978 | Time=4.63s\n",
            "Epoch 107 | Loss=0.0821 | TestAcc=0.895 | F1=0.892 | AUC=0.981 | Time=4.69s\n",
            "Epoch 108 | Loss=0.1278 | TestAcc=0.895 | F1=0.892 | AUC=0.987 | Time=4.74s\n",
            "Epoch 109 | Loss=0.0855 | TestAcc=0.921 | F1=0.920 | AUC=0.990 | Time=4.79s\n",
            "Epoch 110 | Loss=0.1140 | TestAcc=0.947 | F1=0.947 | AUC=0.987 | Time=4.85s\n",
            "Epoch 111 | Loss=0.0830 | TestAcc=0.921 | F1=0.922 | AUC=0.987 | Time=4.91s\n",
            "Epoch 112 | Loss=0.0829 | TestAcc=0.947 | F1=0.946 | AUC=0.987 | Time=4.96s\n",
            "Epoch 113 | Loss=0.0726 | TestAcc=0.921 | F1=0.918 | AUC=0.990 | Time=5.02s\n",
            "Epoch 114 | Loss=0.0861 | TestAcc=0.921 | F1=0.918 | AUC=0.987 | Time=5.09s\n",
            "Epoch 115 | Loss=0.0902 | TestAcc=0.947 | F1=0.946 | AUC=0.987 | Time=5.14s\n",
            "Epoch 116 | Loss=0.1255 | TestAcc=0.921 | F1=0.920 | AUC=0.990 | Time=5.20s\n",
            "Epoch 117 | Loss=0.1138 | TestAcc=0.974 | F1=0.973 | AUC=0.994 | Time=5.25s\n",
            "Epoch 118 | Loss=0.1449 | TestAcc=0.895 | F1=0.897 | AUC=0.984 | Time=5.30s\n",
            "Epoch 119 | Loss=0.1324 | TestAcc=0.895 | F1=0.897 | AUC=0.981 | Time=5.36s\n",
            "Epoch 120 | Loss=0.1027 | TestAcc=0.947 | F1=0.947 | AUC=0.994 | Time=5.41s\n",
            "Epoch 121 | Loss=0.1542 | TestAcc=0.947 | F1=0.947 | AUC=0.994 | Time=5.46s\n",
            "Epoch 122 | Loss=0.1297 | TestAcc=0.947 | F1=0.946 | AUC=0.987 | Time=5.51s\n",
            "Epoch 123 | Loss=0.1176 | TestAcc=0.947 | F1=0.946 | AUC=0.974 | Time=5.56s\n",
            "Epoch 124 | Loss=0.1720 | TestAcc=0.947 | F1=0.947 | AUC=0.955 | Time=5.62s\n",
            "Epoch 125 | Loss=0.1435 | TestAcc=0.921 | F1=0.920 | AUC=0.965 | Time=5.68s\n",
            "Epoch 126 | Loss=0.1354 | TestAcc=0.921 | F1=0.920 | AUC=0.971 | Time=5.73s\n",
            "Epoch 127 | Loss=0.0880 | TestAcc=0.842 | F1=0.824 | AUC=0.949 | Time=5.79s\n",
            "Epoch 128 | Loss=0.1996 | TestAcc=0.895 | F1=0.888 | AUC=0.984 | Time=5.85s\n",
            "Epoch 129 | Loss=0.1819 | TestAcc=0.895 | F1=0.888 | AUC=0.994 | Time=5.92s\n",
            "Epoch 130 | Loss=0.1413 | TestAcc=0.816 | F1=0.822 | AUC=0.958 | Time=5.97s\n",
            "Epoch 131 | Loss=0.1273 | TestAcc=0.895 | F1=0.895 | AUC=0.958 | Time=6.02s\n",
            "Epoch 132 | Loss=0.1485 | TestAcc=0.895 | F1=0.888 | AUC=0.978 | Time=6.09s\n",
            "Epoch 133 | Loss=0.1111 | TestAcc=0.868 | F1=0.857 | AUC=0.981 | Time=6.16s\n",
            "Epoch 134 | Loss=0.1527 | TestAcc=0.868 | F1=0.863 | AUC=0.981 | Time=6.22s\n",
            "Epoch 135 | Loss=0.0899 | TestAcc=0.921 | F1=0.920 | AUC=0.987 | Time=6.28s\n",
            "Epoch 136 | Loss=0.1320 | TestAcc=0.947 | F1=0.946 | AUC=0.997 | Time=6.35s\n",
            "Epoch 137 | Loss=0.1060 | TestAcc=0.974 | F1=0.973 | AUC=0.997 | Time=6.41s\n",
            "Epoch 138 | Loss=0.0893 | TestAcc=0.974 | F1=0.973 | AUC=1.000 | Time=6.47s\n",
            "Epoch 139 | Loss=0.1104 | TestAcc=0.974 | F1=0.973 | AUC=1.000 | Time=6.51s\n",
            "Epoch 140 | Loss=0.1003 | TestAcc=0.895 | F1=0.892 | AUC=0.984 | Time=6.58s\n",
            "Epoch 141 | Loss=0.0678 | TestAcc=0.947 | F1=0.947 | AUC=0.984 | Time=6.62s\n",
            "Epoch 142 | Loss=0.1187 | TestAcc=0.921 | F1=0.920 | AUC=0.981 | Time=6.66s\n",
            "Epoch 143 | Loss=0.0791 | TestAcc=0.895 | F1=0.892 | AUC=0.974 | Time=6.70s\n",
            "Epoch 144 | Loss=0.0748 | TestAcc=0.947 | F1=0.947 | AUC=0.978 | Time=6.74s\n",
            "Epoch 145 | Loss=0.0645 | TestAcc=0.947 | F1=0.947 | AUC=0.981 | Time=6.78s\n",
            "Epoch 146 | Loss=0.0810 | TestAcc=0.895 | F1=0.892 | AUC=0.984 | Time=6.82s\n",
            "Epoch 147 | Loss=0.0609 | TestAcc=0.895 | F1=0.892 | AUC=0.987 | Time=6.86s\n",
            "Epoch 148 | Loss=0.0681 | TestAcc=0.947 | F1=0.947 | AUC=0.981 | Time=6.90s\n",
            "Epoch 149 | Loss=0.0847 | TestAcc=0.947 | F1=0.947 | AUC=0.978 | Time=6.94s\n",
            "Epoch 150 | Loss=0.0691 | TestAcc=0.947 | F1=0.947 | AUC=0.990 | Time=6.98s\n",
            "Epoch 151 | Loss=0.0889 | TestAcc=0.974 | F1=0.973 | AUC=0.994 | Time=7.02s\n",
            "Epoch 152 | Loss=0.0700 | TestAcc=0.974 | F1=0.973 | AUC=0.994 | Time=7.06s\n",
            "Epoch 153 | Loss=0.0750 | TestAcc=0.947 | F1=0.946 | AUC=0.984 | Time=7.10s\n",
            "Epoch 154 | Loss=0.0738 | TestAcc=0.868 | F1=0.863 | AUC=0.962 | Time=7.14s\n",
            "Epoch 155 | Loss=0.0644 | TestAcc=0.842 | F1=0.832 | AUC=0.926 | Time=7.19s\n",
            "Epoch 156 | Loss=0.0729 | TestAcc=0.868 | F1=0.857 | AUC=0.958 | Time=7.23s\n",
            "Epoch 157 | Loss=0.0741 | TestAcc=0.895 | F1=0.888 | AUC=0.984 | Time=7.27s\n",
            "Epoch 158 | Loss=0.1315 | TestAcc=0.895 | F1=0.888 | AUC=0.987 | Time=7.31s\n",
            "Epoch 159 | Loss=0.1290 | TestAcc=0.947 | F1=0.946 | AUC=0.994 | Time=7.36s\n",
            "Epoch 160 | Loss=0.0821 | TestAcc=0.921 | F1=0.920 | AUC=0.990 | Time=7.40s\n",
            "Epoch 161 | Loss=0.0919 | TestAcc=0.947 | F1=0.947 | AUC=0.994 | Time=7.43s\n",
            "Epoch 162 | Loss=0.0814 | TestAcc=0.947 | F1=0.947 | AUC=0.994 | Time=7.49s\n",
            "Epoch 163 | Loss=0.0993 | TestAcc=0.947 | F1=0.947 | AUC=0.990 | Time=7.53s\n",
            "Epoch 164 | Loss=0.0651 | TestAcc=0.921 | F1=0.918 | AUC=0.990 | Time=7.57s\n",
            "Epoch 165 | Loss=0.0839 | TestAcc=0.947 | F1=0.946 | AUC=0.994 | Time=7.60s\n",
            "Epoch 166 | Loss=0.0989 | TestAcc=0.921 | F1=0.918 | AUC=0.994 | Time=7.64s\n",
            "Epoch 167 | Loss=0.0871 | TestAcc=0.921 | F1=0.918 | AUC=0.994 | Time=7.68s\n",
            "Epoch 168 | Loss=0.0835 | TestAcc=0.921 | F1=0.918 | AUC=0.994 | Time=7.72s\n",
            "Epoch 169 | Loss=0.0916 | TestAcc=0.921 | F1=0.918 | AUC=0.984 | Time=7.76s\n",
            "Epoch 170 | Loss=0.0719 | TestAcc=0.921 | F1=0.918 | AUC=0.971 | Time=7.80s\n",
            "Epoch 171 | Loss=0.1040 | TestAcc=0.947 | F1=0.947 | AUC=0.974 | Time=7.84s\n",
            "Epoch 172 | Loss=0.0421 | TestAcc=0.947 | F1=0.946 | AUC=0.987 | Time=7.89s\n",
            "Epoch 173 | Loss=0.1198 | TestAcc=0.921 | F1=0.920 | AUC=0.987 | Time=7.92s\n",
            "Epoch 174 | Loss=0.0947 | TestAcc=0.895 | F1=0.892 | AUC=0.984 | Time=7.97s\n",
            "Epoch 175 | Loss=0.0968 | TestAcc=0.921 | F1=0.918 | AUC=0.984 | Time=8.01s\n",
            "Epoch 176 | Loss=0.0659 | TestAcc=0.921 | F1=0.918 | AUC=0.974 | Time=8.05s\n",
            "Epoch 177 | Loss=0.0891 | TestAcc=0.868 | F1=0.857 | AUC=0.984 | Time=8.09s\n",
            "Epoch 178 | Loss=0.1050 | TestAcc=0.895 | F1=0.888 | AUC=0.987 | Time=8.13s\n",
            "Epoch 179 | Loss=0.0666 | TestAcc=0.895 | F1=0.888 | AUC=0.994 | Time=8.17s\n",
            "Epoch 180 | Loss=0.0993 | TestAcc=0.947 | F1=0.946 | AUC=0.994 | Time=8.23s\n",
            "Epoch 181 | Loss=0.0439 | TestAcc=0.947 | F1=0.946 | AUC=0.994 | Time=8.27s\n",
            "Epoch 182 | Loss=0.0700 | TestAcc=0.974 | F1=0.973 | AUC=0.994 | Time=8.31s\n",
            "Epoch 183 | Loss=0.0562 | TestAcc=0.921 | F1=0.918 | AUC=0.994 | Time=8.35s\n",
            "Epoch 184 | Loss=0.0679 | TestAcc=0.921 | F1=0.918 | AUC=0.994 | Time=8.39s\n",
            "Epoch 185 | Loss=0.0752 | TestAcc=0.921 | F1=0.918 | AUC=0.994 | Time=8.43s\n",
            "Epoch 186 | Loss=0.0496 | TestAcc=0.947 | F1=0.946 | AUC=0.990 | Time=8.47s\n",
            "Epoch 187 | Loss=0.1377 | TestAcc=0.947 | F1=0.947 | AUC=0.981 | Time=8.50s\n",
            "Epoch 188 | Loss=0.0539 | TestAcc=0.921 | F1=0.920 | AUC=0.987 | Time=8.54s\n",
            "Epoch 189 | Loss=0.0579 | TestAcc=0.921 | F1=0.920 | AUC=0.990 | Time=8.58s\n",
            "Epoch 190 | Loss=0.0519 | TestAcc=0.921 | F1=0.920 | AUC=0.987 | Time=8.62s\n",
            "Epoch 191 | Loss=0.0776 | TestAcc=0.974 | F1=0.973 | AUC=0.994 | Time=8.66s\n",
            "Epoch 192 | Loss=0.0963 | TestAcc=0.947 | F1=0.947 | AUC=0.990 | Time=8.70s\n",
            "Epoch 193 | Loss=0.0716 | TestAcc=0.895 | F1=0.892 | AUC=0.984 | Time=8.74s\n",
            "Epoch 194 | Loss=0.0848 | TestAcc=0.842 | F1=0.824 | AUC=0.946 | Time=8.78s\n",
            "Epoch 195 | Loss=0.0682 | TestAcc=0.895 | F1=0.888 | AUC=0.987 | Time=8.82s\n",
            "Epoch 196 | Loss=0.0539 | TestAcc=0.895 | F1=0.888 | AUC=0.984 | Time=8.86s\n",
            "Epoch 197 | Loss=0.0496 | TestAcc=0.895 | F1=0.888 | AUC=0.962 | Time=8.90s\n",
            "Epoch 198 | Loss=0.1036 | TestAcc=0.868 | F1=0.857 | AUC=0.962 | Time=8.94s\n",
            "Epoch 199 | Loss=0.0850 | TestAcc=0.868 | F1=0.857 | AUC=0.974 | Time=8.98s\n",
            "Epoch 200 | Loss=0.0885 | TestAcc=0.895 | F1=0.892 | AUC=0.990 | Time=9.02s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:13:17,068] A new study created in memory with name: no-name-f29e21b1-9dee-4ddf-a2b9-2de388592fc8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training summary stored in : /content/drive/MyDrive/InformationSystems/Classification/results/classification/gin.csv\n",
            "  method  seed dataset optimization_enabled  embedding_dimension  \\\n",
            "0    GIN    43   MUTAG                  yes                   64   \n",
            "\n",
            "  objective_weights  num_layers   dropout        lr  weight_decay  epochs  \\\n",
            "0     (0.5,0.3,0.2)           3  0.502575  0.000437      0.000958     200   \n",
            "\n",
            "   best_epoch  best_loss  eval_loss  eval_acc  eval_f1  eval_auc  \\\n",
            "0          48     0.2023     0.2023    0.9737   0.9734    0.9936   \n",
            "\n",
            "   training_time (s)  generation_time (s)  memory_usage (MB)  \n",
            "0               9.02                 2.18            1461.79  \n",
            "Saved model: /content/drive/MyDrive/InformationSystems/Classification/models/GIN_MUTAG_43.pth\n",
            "Loaded dataset MUTAG: 188 graphs, 7 node features, 2 classes\n",
            "Running Optuna for hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:13:17,347] Trial 0 finished with value: 0.5576063722524702 and parameters: {'num_layers': 6, 'dropout': 0.4153601045616967, 'lr': 0.0004660340572952553, 'weight_decay': 2.892937038967969e-06}. Best is trial 0 with value: 0.5576063722524702.\n",
            "[I 2026-01-22 11:13:17,571] Trial 1 finished with value: 0.8680975307291097 and parameters: {'num_layers': 5, 'dropout': 0.2934514489816703, 'lr': 0.001179732267573371, 'weight_decay': 4.924644094938498e-05}. Best is trial 1 with value: 0.8680975307291097.\n",
            "[I 2026-01-22 11:13:17,770] Trial 2 finished with value: 0.2773954116059379 and parameters: {'num_layers': 4, 'dropout': 0.1340953522563698, 'lr': 0.0004330377685736948, 'weight_decay': 1.9462223689985306e-06}. Best is trial 1 with value: 0.8680975307291097.\n",
            "[I 2026-01-22 11:13:17,941] Trial 3 finished with value: 0.5531619278080258 and parameters: {'num_layers': 3, 'dropout': 0.5673622775924636, 'lr': 0.00016431367163934142, 'weight_decay': 8.626772341502056e-05}. Best is trial 1 with value: 0.8680975307291097.\n",
            "[I 2026-01-22 11:13:18,139] Trial 4 finished with value: 0.7421192473938469 and parameters: {'num_layers': 4, 'dropout': 0.5629657910320759, 'lr': 0.003035948797776517, 'weight_decay': 6.8710022381813665e-06}. Best is trial 1 with value: 0.8680975307291097.\n",
            "[I 2026-01-22 11:13:18,356] Trial 5 finished with value: 0.8484436581650203 and parameters: {'num_layers': 4, 'dropout': 0.41778256868758795, 'lr': 0.0033300629221131753, 'weight_decay': 0.0007281738480226563}. Best is trial 1 with value: 0.8680975307291097.\n",
            "[I 2026-01-22 11:13:18,596] Trial 6 finished with value: 0.5188011695906433 and parameters: {'num_layers': 6, 'dropout': 0.26093676067350813, 'lr': 0.00011078896266989065, 'weight_decay': 1.616929041871791e-06}. Best is trial 1 with value: 0.8680975307291097.\n",
            "[I 2026-01-22 11:13:18,836] Trial 7 finished with value: 0.25409476071130205 and parameters: {'num_layers': 6, 'dropout': 0.17761575616472855, 'lr': 0.000517197839685372, 'weight_decay': 1.843427859469704e-06}. Best is trial 1 with value: 0.8680975307291097.\n",
            "[I 2026-01-22 11:13:19,087] Trial 8 finished with value: 0.7920769666100735 and parameters: {'num_layers': 6, 'dropout': 0.3013797958441166, 'lr': 0.005304036677572355, 'weight_decay': 1.1319261781986726e-06}. Best is trial 1 with value: 0.8680975307291097.\n",
            "[I 2026-01-22 11:13:19,348] Trial 9 finished with value: 0.7710081362827358 and parameters: {'num_layers': 6, 'dropout': 0.1546816226634808, 'lr': 0.0005139227564349573, 'weight_decay': 6.097320325998051e-05}. Best is trial 1 with value: 0.8680975307291097.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'num_layers': 5, 'dropout': 0.2934514489816703, 'lr': 0.001179732267573371, 'weight_decay': 4.924644094938498e-05}\n",
            "\n",
            "Running final training GIN...\n",
            "{'num_layers': 5, 'dropout': 0.2934514489816703, 'lr': 0.001179732267573371, 'weight_decay': 4.924644094938498e-05}\n",
            "Epoch 001 | Loss=1.1386 | TestAcc=0.526 | F1=0.363 | AUC=0.306 | Time=0.05s\n",
            "Epoch 002 | Loss=0.6492 | TestAcc=0.526 | F1=0.363 | AUC=0.342 | Time=0.10s\n",
            "Epoch 003 | Loss=0.4360 | TestAcc=0.474 | F1=0.305 | AUC=0.100 | Time=0.15s\n",
            "Epoch 004 | Loss=0.4529 | TestAcc=0.474 | F1=0.305 | AUC=0.131 | Time=0.20s\n",
            "Epoch 005 | Loss=0.3224 | TestAcc=0.474 | F1=0.305 | AUC=0.083 | Time=0.25s\n",
            "Epoch 006 | Loss=0.3108 | TestAcc=0.474 | F1=0.305 | AUC=0.081 | Time=0.30s\n",
            "Epoch 007 | Loss=0.4187 | TestAcc=0.474 | F1=0.305 | AUC=0.100 | Time=0.35s\n",
            "Epoch 008 | Loss=0.2647 | TestAcc=0.474 | F1=0.305 | AUC=0.408 | Time=0.40s\n",
            "Epoch 009 | Loss=0.2882 | TestAcc=0.474 | F1=0.305 | AUC=0.447 | Time=0.46s\n",
            "Epoch 010 | Loss=0.2338 | TestAcc=0.474 | F1=0.305 | AUC=0.422 | Time=0.50s\n",
            "Epoch 011 | Loss=0.3234 | TestAcc=0.763 | F1=0.757 | AUC=0.917 | Time=0.55s\n",
            "Epoch 012 | Loss=0.1966 | TestAcc=0.921 | F1=0.921 | AUC=0.994 | Time=0.60s\n",
            "Epoch 013 | Loss=0.2165 | TestAcc=0.868 | F1=0.869 | AUC=0.928 | Time=0.65s\n",
            "Epoch 014 | Loss=0.2798 | TestAcc=0.789 | F1=0.786 | AUC=0.856 | Time=0.71s\n",
            "Epoch 015 | Loss=0.2261 | TestAcc=0.895 | F1=0.894 | AUC=0.914 | Time=0.76s\n",
            "Epoch 016 | Loss=0.2523 | TestAcc=0.842 | F1=0.842 | AUC=0.925 | Time=0.81s\n",
            "Epoch 017 | Loss=0.2537 | TestAcc=0.842 | F1=0.839 | AUC=0.892 | Time=0.87s\n",
            "Epoch 018 | Loss=0.2704 | TestAcc=0.816 | F1=0.816 | AUC=0.897 | Time=0.92s\n",
            "Epoch 019 | Loss=0.2433 | TestAcc=0.684 | F1=0.665 | AUC=0.611 | Time=0.98s\n",
            "Epoch 020 | Loss=0.2101 | TestAcc=0.737 | F1=0.735 | AUC=0.675 | Time=1.03s\n",
            "Epoch 021 | Loss=0.1305 | TestAcc=0.526 | F1=0.465 | AUC=0.678 | Time=1.08s\n",
            "Epoch 022 | Loss=0.1289 | TestAcc=0.474 | F1=0.379 | AUC=0.694 | Time=1.13s\n",
            "Epoch 023 | Loss=0.1434 | TestAcc=0.684 | F1=0.673 | AUC=0.814 | Time=1.18s\n",
            "Epoch 024 | Loss=0.1498 | TestAcc=0.816 | F1=0.816 | AUC=0.800 | Time=1.23s\n",
            "Epoch 025 | Loss=0.1090 | TestAcc=0.842 | F1=0.842 | AUC=0.803 | Time=1.28s\n",
            "Epoch 026 | Loss=0.1012 | TestAcc=0.526 | F1=0.363 | AUC=0.529 | Time=1.34s\n",
            "Epoch 027 | Loss=0.0614 | TestAcc=0.526 | F1=0.363 | AUC=0.528 | Time=1.39s\n",
            "Epoch 028 | Loss=0.2508 | TestAcc=0.526 | F1=0.405 | AUC=0.754 | Time=1.45s\n",
            "Epoch 029 | Loss=0.1807 | TestAcc=0.789 | F1=0.786 | AUC=0.911 | Time=1.50s\n",
            "Epoch 030 | Loss=0.1576 | TestAcc=0.816 | F1=0.815 | AUC=0.897 | Time=1.55s\n",
            "Epoch 031 | Loss=0.1402 | TestAcc=0.816 | F1=0.811 | AUC=0.928 | Time=1.60s\n",
            "Epoch 032 | Loss=0.2013 | TestAcc=0.816 | F1=0.814 | AUC=0.908 | Time=1.65s\n",
            "Epoch 033 | Loss=0.2335 | TestAcc=0.711 | F1=0.689 | AUC=0.908 | Time=1.70s\n",
            "Epoch 034 | Loss=0.2070 | TestAcc=0.605 | F1=0.542 | AUC=0.917 | Time=1.75s\n",
            "Epoch 035 | Loss=0.3203 | TestAcc=0.658 | F1=0.603 | AUC=0.875 | Time=1.80s\n",
            "Epoch 036 | Loss=0.2590 | TestAcc=0.763 | F1=0.745 | AUC=0.896 | Time=1.86s\n",
            "Epoch 037 | Loss=0.2559 | TestAcc=0.921 | F1=0.921 | AUC=0.953 | Time=1.90s\n",
            "Epoch 038 | Loss=0.1947 | TestAcc=0.842 | F1=0.839 | AUC=0.908 | Time=1.96s\n",
            "Epoch 039 | Loss=0.1522 | TestAcc=0.816 | F1=0.811 | AUC=0.878 | Time=2.02s\n",
            "Epoch 040 | Loss=0.1728 | TestAcc=0.789 | F1=0.786 | AUC=0.829 | Time=2.07s\n",
            "Epoch 041 | Loss=0.1115 | TestAcc=0.816 | F1=0.811 | AUC=0.867 | Time=2.12s\n",
            "Epoch 042 | Loss=0.1097 | TestAcc=0.816 | F1=0.811 | AUC=0.869 | Time=2.17s\n",
            "Epoch 043 | Loss=0.1012 | TestAcc=0.895 | F1=0.895 | AUC=0.908 | Time=2.22s\n",
            "Epoch 044 | Loss=0.0979 | TestAcc=0.816 | F1=0.815 | AUC=0.797 | Time=2.27s\n",
            "Epoch 045 | Loss=0.1059 | TestAcc=0.737 | F1=0.735 | AUC=0.697 | Time=2.32s\n",
            "Epoch 046 | Loss=0.1413 | TestAcc=0.737 | F1=0.735 | AUC=0.658 | Time=2.37s\n",
            "Epoch 047 | Loss=0.1526 | TestAcc=0.895 | F1=0.895 | AUC=0.892 | Time=2.42s\n",
            "Epoch 048 | Loss=0.1459 | TestAcc=0.816 | F1=0.815 | AUC=0.847 | Time=2.47s\n",
            "Epoch 049 | Loss=0.1632 | TestAcc=0.632 | F1=0.630 | AUC=0.725 | Time=2.52s\n",
            "Epoch 050 | Loss=0.2072 | TestAcc=0.737 | F1=0.737 | AUC=0.808 | Time=2.57s\n",
            "Epoch 051 | Loss=0.1171 | TestAcc=0.789 | F1=0.789 | AUC=0.869 | Time=2.62s\n",
            "Epoch 052 | Loss=0.1419 | TestAcc=0.789 | F1=0.788 | AUC=0.861 | Time=2.67s\n",
            "Epoch 053 | Loss=0.0890 | TestAcc=0.711 | F1=0.708 | AUC=0.789 | Time=2.73s\n",
            "Epoch 054 | Loss=0.1013 | TestAcc=0.711 | F1=0.708 | AUC=0.811 | Time=2.78s\n",
            "Epoch 055 | Loss=0.1556 | TestAcc=0.868 | F1=0.868 | AUC=0.878 | Time=2.83s\n",
            "Epoch 056 | Loss=0.1065 | TestAcc=0.868 | F1=0.867 | AUC=0.919 | Time=2.88s\n",
            "Epoch 057 | Loss=0.1649 | TestAcc=0.789 | F1=0.788 | AUC=0.858 | Time=2.93s\n",
            "Epoch 058 | Loss=0.1093 | TestAcc=0.763 | F1=0.761 | AUC=0.847 | Time=2.99s\n",
            "Epoch 059 | Loss=0.1563 | TestAcc=0.632 | F1=0.625 | AUC=0.728 | Time=3.05s\n",
            "Epoch 060 | Loss=0.1339 | TestAcc=0.658 | F1=0.654 | AUC=0.708 | Time=3.10s\n",
            "Epoch 061 | Loss=0.0885 | TestAcc=0.737 | F1=0.735 | AUC=0.853 | Time=3.15s\n",
            "Epoch 062 | Loss=0.1192 | TestAcc=0.816 | F1=0.811 | AUC=0.858 | Time=3.20s\n",
            "Epoch 063 | Loss=0.0875 | TestAcc=0.763 | F1=0.757 | AUC=0.769 | Time=3.25s\n",
            "Epoch 064 | Loss=0.0950 | TestAcc=0.763 | F1=0.761 | AUC=0.819 | Time=3.30s\n",
            "Epoch 065 | Loss=0.1868 | TestAcc=0.526 | F1=0.484 | AUC=0.511 | Time=3.35s\n",
            "Epoch 066 | Loss=0.1808 | TestAcc=0.842 | F1=0.839 | AUC=0.958 | Time=3.40s\n",
            "Epoch 067 | Loss=0.0904 | TestAcc=0.816 | F1=0.811 | AUC=0.939 | Time=3.45s\n",
            "Epoch 068 | Loss=0.0838 | TestAcc=0.868 | F1=0.867 | AUC=0.942 | Time=3.51s\n",
            "Epoch 069 | Loss=0.1033 | TestAcc=0.789 | F1=0.786 | AUC=0.919 | Time=3.55s\n",
            "Epoch 070 | Loss=0.0920 | TestAcc=0.842 | F1=0.841 | AUC=0.911 | Time=3.61s\n",
            "Epoch 071 | Loss=0.0398 | TestAcc=0.737 | F1=0.737 | AUC=0.889 | Time=3.66s\n",
            "Epoch 072 | Loss=0.1201 | TestAcc=0.711 | F1=0.710 | AUC=0.792 | Time=3.71s\n",
            "Epoch 073 | Loss=0.0986 | TestAcc=0.868 | F1=0.868 | AUC=0.933 | Time=3.76s\n",
            "Epoch 074 | Loss=0.1592 | TestAcc=0.816 | F1=0.811 | AUC=0.900 | Time=3.81s\n",
            "Epoch 075 | Loss=0.1720 | TestAcc=0.789 | F1=0.782 | AUC=0.906 | Time=3.86s\n",
            "Epoch 076 | Loss=0.0902 | TestAcc=0.842 | F1=0.841 | AUC=0.919 | Time=3.91s\n",
            "Epoch 077 | Loss=0.1429 | TestAcc=0.895 | F1=0.894 | AUC=0.928 | Time=3.96s\n",
            "Epoch 078 | Loss=0.0877 | TestAcc=0.895 | F1=0.894 | AUC=0.919 | Time=4.01s\n",
            "Epoch 079 | Loss=0.1133 | TestAcc=0.763 | F1=0.761 | AUC=0.872 | Time=4.08s\n",
            "Epoch 080 | Loss=0.2094 | TestAcc=0.763 | F1=0.752 | AUC=0.889 | Time=4.13s\n",
            "Epoch 081 | Loss=0.0586 | TestAcc=0.816 | F1=0.814 | AUC=0.879 | Time=4.18s\n",
            "Epoch 082 | Loss=0.0597 | TestAcc=0.789 | F1=0.786 | AUC=0.860 | Time=4.23s\n",
            "Epoch 083 | Loss=0.1024 | TestAcc=0.816 | F1=0.814 | AUC=0.883 | Time=4.28s\n",
            "Epoch 084 | Loss=0.0964 | TestAcc=0.895 | F1=0.895 | AUC=0.911 | Time=4.33s\n",
            "Epoch 085 | Loss=0.0704 | TestAcc=0.842 | F1=0.842 | AUC=0.922 | Time=4.38s\n",
            "Epoch 086 | Loss=0.0904 | TestAcc=0.842 | F1=0.842 | AUC=0.886 | Time=4.43s\n",
            "Epoch 087 | Loss=0.0699 | TestAcc=0.842 | F1=0.842 | AUC=0.889 | Time=4.48s\n",
            "Epoch 088 | Loss=0.0567 | TestAcc=0.842 | F1=0.842 | AUC=0.914 | Time=4.53s\n",
            "Epoch 089 | Loss=0.0494 | TestAcc=0.842 | F1=0.841 | AUC=0.919 | Time=4.58s\n",
            "Epoch 090 | Loss=0.0606 | TestAcc=0.842 | F1=0.841 | AUC=0.939 | Time=4.63s\n",
            "Epoch 091 | Loss=0.0715 | TestAcc=0.868 | F1=0.868 | AUC=0.931 | Time=4.68s\n",
            "Epoch 092 | Loss=0.0850 | TestAcc=0.816 | F1=0.811 | AUC=0.903 | Time=4.73s\n",
            "Epoch 093 | Loss=0.0429 | TestAcc=0.789 | F1=0.786 | AUC=0.847 | Time=4.78s\n",
            "Epoch 094 | Loss=0.0366 | TestAcc=0.789 | F1=0.786 | AUC=0.879 | Time=4.83s\n",
            "Epoch 095 | Loss=0.0613 | TestAcc=0.842 | F1=0.841 | AUC=0.922 | Time=4.88s\n",
            "Epoch 096 | Loss=0.0574 | TestAcc=0.763 | F1=0.763 | AUC=0.922 | Time=4.93s\n",
            "Epoch 097 | Loss=0.0500 | TestAcc=0.789 | F1=0.789 | AUC=0.881 | Time=4.98s\n",
            "Epoch 098 | Loss=0.0755 | TestAcc=0.895 | F1=0.894 | AUC=0.882 | Time=5.03s\n",
            "Epoch 099 | Loss=0.1069 | TestAcc=0.763 | F1=0.763 | AUC=0.831 | Time=5.10s\n",
            "Epoch 100 | Loss=0.1034 | TestAcc=0.553 | F1=0.542 | AUC=0.581 | Time=5.19s\n",
            "Epoch 101 | Loss=0.1052 | TestAcc=0.763 | F1=0.757 | AUC=0.885 | Time=5.26s\n",
            "Epoch 102 | Loss=0.0717 | TestAcc=0.658 | F1=0.620 | AUC=0.922 | Time=5.33s\n",
            "Epoch 103 | Loss=0.1502 | TestAcc=0.711 | F1=0.678 | AUC=0.939 | Time=5.40s\n",
            "Epoch 104 | Loss=0.1581 | TestAcc=0.789 | F1=0.786 | AUC=0.900 | Time=5.47s\n",
            "Epoch 105 | Loss=0.0798 | TestAcc=0.763 | F1=0.763 | AUC=0.853 | Time=5.54s\n",
            "Epoch 106 | Loss=0.0984 | TestAcc=0.816 | F1=0.815 | AUC=0.900 | Time=5.61s\n",
            "Epoch 107 | Loss=0.0442 | TestAcc=0.895 | F1=0.895 | AUC=0.914 | Time=5.67s\n",
            "Epoch 108 | Loss=0.1033 | TestAcc=0.816 | F1=0.814 | AUC=0.894 | Time=5.74s\n",
            "Epoch 109 | Loss=0.1230 | TestAcc=0.737 | F1=0.732 | AUC=0.889 | Time=5.81s\n",
            "Epoch 110 | Loss=0.1155 | TestAcc=0.711 | F1=0.703 | AUC=0.881 | Time=5.88s\n",
            "Epoch 111 | Loss=0.0983 | TestAcc=0.763 | F1=0.752 | AUC=0.922 | Time=5.95s\n",
            "Epoch 112 | Loss=0.0565 | TestAcc=0.763 | F1=0.757 | AUC=0.914 | Time=6.01s\n",
            "Epoch 113 | Loss=0.0508 | TestAcc=0.789 | F1=0.786 | AUC=0.908 | Time=6.08s\n",
            "Epoch 114 | Loss=0.0458 | TestAcc=0.816 | F1=0.811 | AUC=0.881 | Time=6.15s\n",
            "Epoch 115 | Loss=0.0336 | TestAcc=0.816 | F1=0.811 | AUC=0.894 | Time=6.22s\n",
            "Epoch 116 | Loss=0.0404 | TestAcc=0.789 | F1=0.786 | AUC=0.917 | Time=6.29s\n",
            "Epoch 117 | Loss=0.0383 | TestAcc=0.816 | F1=0.814 | AUC=0.889 | Time=6.35s\n",
            "Epoch 118 | Loss=0.0578 | TestAcc=0.789 | F1=0.786 | AUC=0.894 | Time=6.42s\n",
            "Epoch 119 | Loss=0.0539 | TestAcc=0.763 | F1=0.757 | AUC=0.922 | Time=6.50s\n",
            "Epoch 120 | Loss=0.0585 | TestAcc=0.763 | F1=0.757 | AUC=0.944 | Time=6.57s\n",
            "Epoch 121 | Loss=0.0438 | TestAcc=0.868 | F1=0.868 | AUC=0.939 | Time=6.63s\n",
            "Epoch 122 | Loss=0.0711 | TestAcc=0.711 | F1=0.703 | AUC=0.781 | Time=6.70s\n",
            "Epoch 123 | Loss=0.1233 | TestAcc=0.684 | F1=0.679 | AUC=0.811 | Time=6.77s\n",
            "Epoch 124 | Loss=0.0828 | TestAcc=0.842 | F1=0.839 | AUC=0.854 | Time=6.84s\n",
            "Epoch 125 | Loss=0.0244 | TestAcc=0.763 | F1=0.757 | AUC=0.826 | Time=6.91s\n",
            "Epoch 126 | Loss=0.0454 | TestAcc=0.868 | F1=0.867 | AUC=0.950 | Time=6.97s\n",
            "Epoch 127 | Loss=0.1354 | TestAcc=0.868 | F1=0.868 | AUC=0.906 | Time=7.04s\n",
            "Epoch 128 | Loss=0.0265 | TestAcc=0.789 | F1=0.786 | AUC=0.903 | Time=7.11s\n",
            "Epoch 129 | Loss=0.0292 | TestAcc=0.816 | F1=0.811 | AUC=0.889 | Time=7.17s\n",
            "Epoch 130 | Loss=0.0698 | TestAcc=0.789 | F1=0.786 | AUC=0.914 | Time=7.25s\n",
            "Epoch 131 | Loss=0.0636 | TestAcc=0.842 | F1=0.841 | AUC=0.917 | Time=7.32s\n",
            "Epoch 132 | Loss=0.0585 | TestAcc=0.816 | F1=0.815 | AUC=0.922 | Time=7.40s\n",
            "Epoch 133 | Loss=0.0319 | TestAcc=0.789 | F1=0.782 | AUC=0.931 | Time=7.47s\n",
            "Epoch 134 | Loss=0.0325 | TestAcc=0.842 | F1=0.839 | AUC=0.931 | Time=7.55s\n",
            "Epoch 135 | Loss=0.0140 | TestAcc=0.842 | F1=0.839 | AUC=0.933 | Time=7.63s\n",
            "Epoch 136 | Loss=0.0287 | TestAcc=0.816 | F1=0.815 | AUC=0.925 | Time=7.70s\n",
            "Epoch 137 | Loss=0.0133 | TestAcc=0.842 | F1=0.842 | AUC=0.906 | Time=7.77s\n",
            "Epoch 138 | Loss=0.0383 | TestAcc=0.789 | F1=0.789 | AUC=0.900 | Time=7.85s\n",
            "Epoch 139 | Loss=0.0128 | TestAcc=0.763 | F1=0.763 | AUC=0.861 | Time=7.93s\n",
            "Epoch 140 | Loss=0.1080 | TestAcc=0.684 | F1=0.684 | AUC=0.839 | Time=8.01s\n",
            "Epoch 141 | Loss=0.0244 | TestAcc=0.684 | F1=0.682 | AUC=0.800 | Time=8.09s\n",
            "Epoch 142 | Loss=0.0360 | TestAcc=0.816 | F1=0.816 | AUC=0.875 | Time=8.16s\n",
            "Epoch 143 | Loss=0.0055 | TestAcc=0.789 | F1=0.789 | AUC=0.875 | Time=8.21s\n",
            "Epoch 144 | Loss=0.0431 | TestAcc=0.816 | F1=0.815 | AUC=0.872 | Time=8.28s\n",
            "Epoch 145 | Loss=0.0112 | TestAcc=0.684 | F1=0.682 | AUC=0.811 | Time=8.33s\n",
            "Epoch 146 | Loss=0.0428 | TestAcc=0.789 | F1=0.789 | AUC=0.883 | Time=8.38s\n",
            "Epoch 147 | Loss=0.0095 | TestAcc=0.868 | F1=0.867 | AUC=0.950 | Time=8.43s\n",
            "Epoch 148 | Loss=0.0198 | TestAcc=0.895 | F1=0.894 | AUC=0.956 | Time=8.48s\n",
            "Epoch 149 | Loss=0.0238 | TestAcc=0.816 | F1=0.811 | AUC=0.919 | Time=8.53s\n",
            "Epoch 150 | Loss=0.0115 | TestAcc=0.816 | F1=0.814 | AUC=0.903 | Time=8.58s\n",
            "Epoch 151 | Loss=0.0186 | TestAcc=0.816 | F1=0.814 | AUC=0.897 | Time=8.63s\n",
            "Epoch 152 | Loss=0.0109 | TestAcc=0.842 | F1=0.839 | AUC=0.900 | Time=8.68s\n",
            "Epoch 153 | Loss=0.0244 | TestAcc=0.868 | F1=0.867 | AUC=0.906 | Time=8.73s\n",
            "Epoch 154 | Loss=0.0076 | TestAcc=0.842 | F1=0.839 | AUC=0.894 | Time=8.78s\n",
            "Epoch 155 | Loss=0.0102 | TestAcc=0.868 | F1=0.867 | AUC=0.906 | Time=8.83s\n",
            "Epoch 156 | Loss=0.0042 | TestAcc=0.868 | F1=0.867 | AUC=0.919 | Time=8.89s\n",
            "Epoch 157 | Loss=0.0217 | TestAcc=0.895 | F1=0.894 | AUC=0.947 | Time=8.94s\n",
            "Epoch 158 | Loss=0.0126 | TestAcc=0.868 | F1=0.867 | AUC=0.942 | Time=8.99s\n",
            "Epoch 159 | Loss=0.0043 | TestAcc=0.895 | F1=0.894 | AUC=0.939 | Time=9.04s\n",
            "Epoch 160 | Loss=0.0022 | TestAcc=0.895 | F1=0.894 | AUC=0.939 | Time=9.09s\n",
            "Epoch 161 | Loss=0.0616 | TestAcc=0.842 | F1=0.841 | AUC=0.878 | Time=9.14s\n",
            "Epoch 162 | Loss=0.0098 | TestAcc=0.816 | F1=0.814 | AUC=0.850 | Time=9.19s\n",
            "Epoch 163 | Loss=0.0255 | TestAcc=0.789 | F1=0.786 | AUC=0.847 | Time=9.24s\n",
            "Epoch 164 | Loss=0.0174 | TestAcc=0.816 | F1=0.811 | AUC=0.894 | Time=9.30s\n",
            "Epoch 165 | Loss=0.0115 | TestAcc=0.868 | F1=0.867 | AUC=0.908 | Time=9.35s\n",
            "Epoch 166 | Loss=0.0052 | TestAcc=0.868 | F1=0.868 | AUC=0.911 | Time=9.40s\n",
            "Epoch 167 | Loss=0.0115 | TestAcc=0.816 | F1=0.814 | AUC=0.892 | Time=9.46s\n",
            "Epoch 168 | Loss=0.0093 | TestAcc=0.789 | F1=0.786 | AUC=0.835 | Time=9.51s\n",
            "Epoch 169 | Loss=0.0064 | TestAcc=0.789 | F1=0.786 | AUC=0.886 | Time=9.56s\n",
            "Epoch 170 | Loss=0.0059 | TestAcc=0.789 | F1=0.786 | AUC=0.886 | Time=9.61s\n",
            "Epoch 171 | Loss=0.0096 | TestAcc=0.868 | F1=0.868 | AUC=0.928 | Time=9.66s\n",
            "Epoch 172 | Loss=0.0240 | TestAcc=0.842 | F1=0.841 | AUC=0.922 | Time=9.71s\n",
            "Epoch 173 | Loss=0.0085 | TestAcc=0.816 | F1=0.811 | AUC=0.919 | Time=9.77s\n",
            "Epoch 174 | Loss=0.0163 | TestAcc=0.816 | F1=0.811 | AUC=0.917 | Time=9.83s\n",
            "Epoch 175 | Loss=0.0055 | TestAcc=0.816 | F1=0.814 | AUC=0.922 | Time=9.88s\n",
            "Epoch 176 | Loss=0.0090 | TestAcc=0.816 | F1=0.814 | AUC=0.914 | Time=9.93s\n",
            "Epoch 177 | Loss=0.0046 | TestAcc=0.816 | F1=0.815 | AUC=0.903 | Time=9.99s\n",
            "Epoch 178 | Loss=0.0211 | TestAcc=0.842 | F1=0.841 | AUC=0.928 | Time=10.04s\n",
            "Epoch 179 | Loss=0.0195 | TestAcc=0.868 | F1=0.865 | AUC=0.946 | Time=10.09s\n",
            "Epoch 180 | Loss=0.0393 | TestAcc=0.789 | F1=0.786 | AUC=0.911 | Time=10.14s\n",
            "Epoch 181 | Loss=0.0106 | TestAcc=0.789 | F1=0.786 | AUC=0.897 | Time=10.19s\n",
            "Epoch 182 | Loss=0.0020 | TestAcc=0.789 | F1=0.786 | AUC=0.872 | Time=10.24s\n",
            "Epoch 183 | Loss=0.0053 | TestAcc=0.816 | F1=0.811 | AUC=0.833 | Time=10.29s\n",
            "Epoch 184 | Loss=0.0177 | TestAcc=0.816 | F1=0.811 | AUC=0.842 | Time=10.35s\n",
            "Epoch 185 | Loss=0.0247 | TestAcc=0.868 | F1=0.867 | AUC=0.969 | Time=10.40s\n",
            "Epoch 186 | Loss=0.1629 | TestAcc=0.816 | F1=0.814 | AUC=0.947 | Time=10.46s\n",
            "Epoch 187 | Loss=0.1018 | TestAcc=0.605 | F1=0.595 | AUC=0.779 | Time=10.51s\n",
            "Epoch 188 | Loss=0.1590 | TestAcc=0.711 | F1=0.697 | AUC=0.894 | Time=10.56s\n",
            "Epoch 189 | Loss=0.0837 | TestAcc=0.763 | F1=0.757 | AUC=0.925 | Time=10.61s\n",
            "Epoch 190 | Loss=0.0979 | TestAcc=0.737 | F1=0.713 | AUC=0.978 | Time=10.66s\n",
            "Epoch 191 | Loss=0.0718 | TestAcc=0.684 | F1=0.642 | AUC=0.958 | Time=10.71s\n",
            "Epoch 192 | Loss=0.1756 | TestAcc=0.711 | F1=0.697 | AUC=0.853 | Time=10.76s\n",
            "Epoch 193 | Loss=0.0819 | TestAcc=0.632 | F1=0.632 | AUC=0.800 | Time=10.81s\n",
            "Epoch 194 | Loss=0.1252 | TestAcc=0.684 | F1=0.673 | AUC=0.769 | Time=10.87s\n",
            "Epoch 195 | Loss=0.0945 | TestAcc=0.553 | F1=0.484 | AUC=0.767 | Time=10.92s\n",
            "Epoch 196 | Loss=0.1224 | TestAcc=0.658 | F1=0.642 | AUC=0.786 | Time=10.98s\n",
            "Epoch 197 | Loss=0.0750 | TestAcc=0.684 | F1=0.682 | AUC=0.842 | Time=11.03s\n",
            "Epoch 198 | Loss=0.0713 | TestAcc=0.816 | F1=0.815 | AUC=0.892 | Time=11.08s\n",
            "Epoch 199 | Loss=0.0371 | TestAcc=0.842 | F1=0.841 | AUC=0.883 | Time=11.13s\n",
            "Epoch 200 | Loss=0.0529 | TestAcc=0.816 | F1=0.816 | AUC=0.900 | Time=11.18s\n",
            "\n",
            "Training summary stored in : /content/drive/MyDrive/InformationSystems/Classification/results/classification/gin.csv\n",
            "  method  seed dataset optimization_enabled  embedding_dimension  \\\n",
            "0    GIN    44   MUTAG                  yes                   64   \n",
            "\n",
            "  objective_weights  num_layers   dropout       lr  weight_decay  epochs  \\\n",
            "0     (0.5,0.3,0.2)           5  0.293451  0.00118      0.000049     200   \n",
            "\n",
            "   best_epoch  best_loss  eval_loss  eval_acc  eval_f1  eval_auc  \\\n",
            "0          12     0.1015     0.1015    0.9211   0.9209    0.9944   \n",
            "\n",
            "   training_time (s)  generation_time (s)  memory_usage (MB)  \n",
            "0              11.18                 2.28            1461.79  \n",
            "Saved model: /content/drive/MyDrive/InformationSystems/Classification/models/GIN_MUTAG_44.pth\n"
          ]
        }
      ],
      "source": [
        "for dataset_name in [\"IMDB-MULTI\", \"ENZYMES\", \"MUTAG\"]:\n",
        "  for seed in [42, 43, 44]:\n",
        "      run_gin_pipeline(\n",
        "          dataset_name=dataset_name,\n",
        "          seed=seed,\n",
        "          use_optuna=True,\n",
        "          w_acc=0.5,\n",
        "          w_f1=0.3,\n",
        "          w_auc=0.2,\n",
        "          hidden_dim=64,\n",
        "          epochs=200,\n",
        "          batch_size=32,\n",
        "          n_trials=10,\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUWkRRajqf-k",
        "outputId": "c6ac2a59-7cd8-47a4-f967-bb23a9e3a08e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset IMDB-MULTI for Graph2Vec: 1500 graphs, 3 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:13:31,082] A new study created in memory with name: no-name-d4d30493-2121-464b-b5f8-2d2bec0a83c5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Optuna for Graph2Vec+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:13:40,224] Trial 0 finished with value: 0.4660962631118881 and parameters: {'C': 3.7081609439242307, 'gamma': 0.794867896034824}. Best is trial 0 with value: 0.4660962631118881.\n",
            "[I 2026-01-22 11:13:48,847] Trial 1 finished with value: 0.4511775212712713 and parameters: {'C': 7.04920217670471, 'gamma': 4.7734681587402745}. Best is trial 0 with value: 0.4660962631118881.\n",
            "[I 2026-01-22 11:13:57,497] Trial 2 finished with value: 0.4822643395800988 and parameters: {'C': 13.502181952579566, 'gamma': 0.003697059566281911}. Best is trial 2 with value: 0.4822643395800988.\n",
            "[I 2026-01-22 11:14:06,197] Trial 3 finished with value: 0.4376106181362238 and parameters: {'C': 0.031197952076748407, 'gamma': 0.09059631512150243}. Best is trial 2 with value: 0.4822643395800988.\n",
            "[I 2026-01-22 11:14:14,997] Trial 4 finished with value: 0.5106928852653914 and parameters: {'C': 0.26920531358498506, 'gamma': 0.015584615031485491}. Best is trial 4 with value: 0.5106928852653914.\n",
            "[I 2026-01-22 11:14:23,643] Trial 5 finished with value: 0.48078683506262787 and parameters: {'C': 0.10422430889469304, 'gamma': 0.047470544816188336}. Best is trial 4 with value: 0.5106928852653914.\n",
            "[I 2026-01-22 11:14:32,365] Trial 6 finished with value: 0.47543758756427235 and parameters: {'C': 0.12015929421598134, 'gamma': 0.06538609675893109}. Best is trial 4 with value: 0.5106928852653914.\n",
            "[I 2026-01-22 11:14:40,933] Trial 7 finished with value: 0.505807227382963 and parameters: {'C': 17.932252170397092, 'gamma': 0.0012386663401217652}. Best is trial 4 with value: 0.5106928852653914.\n",
            "[I 2026-01-22 11:14:49,538] Trial 8 finished with value: 0.48427967434799957 and parameters: {'C': 32.72026543902426, 'gamma': 0.0026009640296628246}. Best is trial 4 with value: 0.5106928852653914.\n",
            "[I 2026-01-22 11:14:58,385] Trial 9 finished with value: 0.4452560995989304 and parameters: {'C': 0.14152035179167113, 'gamma': 2.848800411559274}. Best is trial 4 with value: 0.5106928852653914.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (Graph2Vec+SVM): {'C': 0.26920531358498506, 'gamma': 0.015584615031485491}\n",
            "Running final Graph2Vec embedding on train+test graphs...\n",
            "Training final SVM on Graph2Vec embeddings...\n",
            "Graph2Vec Results on IMDB-MULTI -> Acc: 0.490, F1: 0.470, AUC: 0.668, Score: 0.520\n",
            "Embedding time: 10.62s | SVM training time: 0.67s | Optuna time: 87.31s | Memory usage: 1480.48 MB\n",
            "Graph2Vec summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/g2v.csv\n",
            "Loaded dataset IMDB-MULTI for Graph2Vec: 1500 graphs, 3 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:15:10,350] A new study created in memory with name: no-name-38b0449a-790c-45bf-b745-4e81b5a22845\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Optuna for Graph2Vec+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:15:19,102] Trial 0 finished with value: 0.5463009575201325 and parameters: {'C': 10.398708466288678, 'gamma': 0.0031955740369566865}. Best is trial 0 with value: 0.5463009575201325.\n",
            "[I 2026-01-22 11:15:27,804] Trial 1 finished with value: 0.4471662286440273 and parameters: {'C': 0.025860801699517287, 'gamma': 3.0371446073555823}. Best is trial 0 with value: 0.5463009575201325.\n",
            "[I 2026-01-22 11:15:36,629] Trial 2 finished with value: 0.4700200072454712 and parameters: {'C': 0.026576547391656193, 'gamma': 0.002648894630342802}. Best is trial 0 with value: 0.5463009575201325.\n",
            "[I 2026-01-22 11:15:45,387] Trial 3 finished with value: 0.4794217106092106 and parameters: {'C': 78.1098561687723, 'gamma': 0.24484064826708488}. Best is trial 0 with value: 0.5463009575201325.\n",
            "[I 2026-01-22 11:15:54,322] Trial 4 finished with value: 0.5272799910081837 and parameters: {'C': 18.982943291161355, 'gamma': 0.31881844095772877}. Best is trial 0 with value: 0.5463009575201325.\n",
            "[I 2026-01-22 11:16:02,854] Trial 5 finished with value: 0.4782650714573269 and parameters: {'C': 0.04530740544247895, 'gamma': 0.00012564258160254802}. Best is trial 0 with value: 0.5463009575201325.\n",
            "[I 2026-01-22 11:16:11,557] Trial 6 finished with value: 0.5564360250337368 and parameters: {'C': 0.8520303048280019, 'gamma': 0.0442899555599358}. Best is trial 6 with value: 0.5564360250337368.\n",
            "[I 2026-01-22 11:16:20,285] Trial 7 finished with value: 0.5359255423213881 and parameters: {'C': 18.74592176023536, 'gamma': 0.014525626408922552}. Best is trial 6 with value: 0.5564360250337368.\n",
            "[I 2026-01-22 11:16:28,840] Trial 8 finished with value: 0.476174922058644 and parameters: {'C': 2.0789216443594443, 'gamma': 0.0005716190133106356}. Best is trial 6 with value: 0.5564360250337368.\n",
            "[I 2026-01-22 11:16:37,484] Trial 9 finished with value: 0.5162245190081588 and parameters: {'C': 0.5256678024353605, 'gamma': 2.8352904657321827}. Best is trial 6 with value: 0.5564360250337368.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (Graph2Vec+SVM): {'C': 0.8520303048280019, 'gamma': 0.0442899555599358}\n",
            "Running final Graph2Vec embedding on train+test graphs...\n",
            "Training final SVM on Graph2Vec embeddings...\n",
            "Graph2Vec Results on IMDB-MULTI -> Acc: 0.500, F1: 0.487, AUC: 0.641, Score: 0.524\n",
            "Embedding time: 10.46s | SVM training time: 0.59s | Optuna time: 87.14s | Memory usage: 1493.49 MB\n",
            "Graph2Vec summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/g2v.csv\n",
            "Loaded dataset IMDB-MULTI for Graph2Vec: 1500 graphs, 3 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:16:49,433] A new study created in memory with name: no-name-02dc74e4-d2a5-41c4-81bd-b8e0ec98c055\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Optuna for Graph2Vec+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:16:58,201] Trial 0 finished with value: 0.46948471266210307 and parameters: {'C': 9.445042359802896, 'gamma': 0.022023385524781277}. Best is trial 0 with value: 0.46948471266210307.\n",
            "[I 2026-01-22 11:17:07,106] Trial 1 finished with value: 0.3761206556413912 and parameters: {'C': 22.121965434226645, 'gamma': 2.523731707906396}. Best is trial 0 with value: 0.46948471266210307.\n",
            "[I 2026-01-22 11:17:15,787] Trial 2 finished with value: 0.4047580677378431 and parameters: {'C': 0.0499580795005519, 'gamma': 0.06064856574433549}. Best is trial 0 with value: 0.46948471266210307.\n",
            "[I 2026-01-22 11:17:24,015] Trial 3 finished with value: 0.4931038306451613 and parameters: {'C': 2.121914997751497, 'gamma': 0.03122484103237649}. Best is trial 3 with value: 0.4931038306451613.\n",
            "[I 2026-01-22 11:17:32,772] Trial 4 finished with value: 0.4675831185846706 and parameters: {'C': 0.45426982293148893, 'gamma': 0.03371278273550514}. Best is trial 3 with value: 0.4931038306451613.\n",
            "[I 2026-01-22 11:17:41,625] Trial 5 finished with value: 0.4756812617334168 and parameters: {'C': 68.54868694277722, 'gamma': 0.020064906991717195}. Best is trial 3 with value: 0.4931038306451613.\n",
            "[I 2026-01-22 11:17:50,029] Trial 6 finished with value: 0.43828864013934343 and parameters: {'C': 13.499085613527898, 'gamma': 6.182963485356047}. Best is trial 3 with value: 0.4931038306451613.\n",
            "[I 2026-01-22 11:17:58,637] Trial 7 finished with value: 0.4564898516322948 and parameters: {'C': 1.5917567659192173, 'gamma': 0.22488232378075454}. Best is trial 3 with value: 0.4931038306451613.\n",
            "[I 2026-01-22 11:18:07,387] Trial 8 finished with value: 0.4185729093309859 and parameters: {'C': 0.04683770648737743, 'gamma': 1.0525620106487263}. Best is trial 3 with value: 0.4931038306451613.\n",
            "[I 2026-01-22 11:18:15,747] Trial 9 finished with value: 0.3953469948363759 and parameters: {'C': 0.017804349079232607, 'gamma': 0.005339025155384352}. Best is trial 3 with value: 0.4931038306451613.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (Graph2Vec+SVM): {'C': 2.121914997751497, 'gamma': 0.03122484103237649}\n",
            "Running final Graph2Vec embedding on train+test graphs...\n",
            "Training final SVM on Graph2Vec embeddings...\n",
            "Graph2Vec Results on IMDB-MULTI -> Acc: 0.470, F1: 0.454, AUC: 0.641, Score: 0.499\n",
            "Embedding time: 10.54s | SVM training time: 0.56s | Optuna time: 86.31s | Memory usage: 1493.59 MB\n",
            "Graph2Vec summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/g2v.csv\n",
            "Loaded dataset ENZYMES for Graph2Vec: 600 graphs, 6 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:18:27,253] A new study created in memory with name: no-name-03277db2-fa3a-4180-9c30-dc4ccd14cefb\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES filtering: removed 1 graphs with < 3 nodes, kept 599 graphs.\n",
            "Running Optuna for Graph2Vec+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:18:32,232] Trial 0 finished with value: 0.17461210664335666 and parameters: {'C': 0.029355811521368482, 'gamma': 0.5737096068707317}. Best is trial 0 with value: 0.17461210664335666.\n",
            "[I 2026-01-22 11:18:37,631] Trial 1 finished with value: 0.3720294718272764 and parameters: {'C': 47.298590125145, 'gamma': 0.0003452452267624942}. Best is trial 1 with value: 0.3720294718272764.\n",
            "[I 2026-01-22 11:18:42,447] Trial 2 finished with value: 0.2763126430771905 and parameters: {'C': 0.1746727725603006, 'gamma': 0.02539125185015932}. Best is trial 1 with value: 0.3720294718272764.\n",
            "[I 2026-01-22 11:18:47,761] Trial 3 finished with value: 0.2775296631859132 and parameters: {'C': 0.040800394548855805, 'gamma': 0.00019011660432997462}. Best is trial 1 with value: 0.3720294718272764.\n",
            "[I 2026-01-22 11:18:52,636] Trial 4 finished with value: 0.19530979756289307 and parameters: {'C': 10.88543892469145, 'gamma': 5.708514091482259}. Best is trial 1 with value: 0.3720294718272764.\n",
            "[I 2026-01-22 11:18:57,513] Trial 5 finished with value: 0.23499849133396156 and parameters: {'C': 0.15137576907452985, 'gamma': 0.00017568150274531062}. Best is trial 1 with value: 0.3720294718272764.\n",
            "[I 2026-01-22 11:19:02,766] Trial 6 finished with value: 0.28640292917866444 and parameters: {'C': 0.02391016588914902, 'gamma': 0.0344359320846179}. Best is trial 1 with value: 0.3720294718272764.\n",
            "[I 2026-01-22 11:19:07,509] Trial 7 finished with value: 0.27669411599099103 and parameters: {'C': 0.011070849004577658, 'gamma': 0.001338217247775756}. Best is trial 1 with value: 0.3720294718272764.\n",
            "[I 2026-01-22 11:19:12,561] Trial 8 finished with value: 0.24351347595360354 and parameters: {'C': 1.5605479769515824, 'gamma': 0.00013859263462821808}. Best is trial 1 with value: 0.3720294718272764.\n",
            "[I 2026-01-22 11:19:17,640] Trial 9 finished with value: 0.24438693027826508 and parameters: {'C': 0.018956586280505384, 'gamma': 0.0006384895708230085}. Best is trial 1 with value: 0.3720294718272764.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (Graph2Vec+SVM): {'C': 47.298590125145, 'gamma': 0.0003452452267624942}\n",
            "Running final Graph2Vec embedding on train+test graphs...\n",
            "Training final SVM on Graph2Vec embeddings...\n",
            "Graph2Vec Results on ENZYMES -> Acc: 0.325, F1: 0.312, AUC: 0.652, Score: 0.386\n",
            "Embedding time: 5.97s | SVM training time: 0.12s | Optuna time: 50.39s | Memory usage: 1492.79 MB\n",
            "Graph2Vec summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/g2v.csv\n",
            "Loaded dataset ENZYMES for Graph2Vec: 600 graphs, 6 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:19:24,544] A new study created in memory with name: no-name-7b54c871-295b-46c4-b631-3f5567f44a22\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES filtering: removed 1 graphs with < 3 nodes, kept 599 graphs.\n",
            "Running Optuna for Graph2Vec+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:19:29,614] Trial 0 finished with value: 0.4081511292016807 and parameters: {'C': 14.699763495313501, 'gamma': 0.254035437370183}. Best is trial 0 with value: 0.4081511292016807.\n",
            "[I 2026-01-22 11:19:34,377] Trial 1 finished with value: 0.25010437717925 and parameters: {'C': 0.2316263118062569, 'gamma': 0.012205789905843171}. Best is trial 0 with value: 0.4081511292016807.\n",
            "[I 2026-01-22 11:19:39,384] Trial 2 finished with value: 0.40710048223834994 and parameters: {'C': 2.528589409709576, 'gamma': 0.23295562659745997}. Best is trial 0 with value: 0.4081511292016807.\n",
            "[I 2026-01-22 11:19:44,256] Trial 3 finished with value: 0.2750583974848681 and parameters: {'C': 1.17450327025407, 'gamma': 0.000243308036522096}. Best is trial 0 with value: 0.4081511292016807.\n",
            "[I 2026-01-22 11:19:48,954] Trial 4 finished with value: 0.3105383297258297 and parameters: {'C': 2.544346806851815, 'gamma': 0.0011701790045504186}. Best is trial 0 with value: 0.4081511292016807.\n",
            "[I 2026-01-22 11:19:54,117] Trial 5 finished with value: 0.25777776167285094 and parameters: {'C': 0.21277655427699274, 'gamma': 0.0003180716082195081}. Best is trial 0 with value: 0.4081511292016807.\n",
            "[I 2026-01-22 11:19:58,787] Trial 6 finished with value: 0.37421720639884304 and parameters: {'C': 3.490021942919437, 'gamma': 0.0355034929849869}. Best is trial 0 with value: 0.4081511292016807.\n",
            "[I 2026-01-22 11:20:03,467] Trial 7 finished with value: 0.24555541223014915 and parameters: {'C': 0.014073973377421684, 'gamma': 0.00019735850748326726}. Best is trial 0 with value: 0.4081511292016807.\n",
            "[I 2026-01-22 11:20:08,619] Trial 8 finished with value: 0.258447579277686 and parameters: {'C': 0.6641918857650154, 'gamma': 0.00010993806791349695}. Best is trial 0 with value: 0.4081511292016807.\n",
            "[I 2026-01-22 11:20:13,554] Trial 9 finished with value: 0.2652655063374878 and parameters: {'C': 18.540015667803324, 'gamma': 2.1558945240660905}. Best is trial 0 with value: 0.4081511292016807.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (Graph2Vec+SVM): {'C': 14.699763495313501, 'gamma': 0.254035437370183}\n",
            "Running final Graph2Vec embedding on train+test graphs...\n",
            "Training final SVM on Graph2Vec embeddings...\n",
            "Graph2Vec Results on ENZYMES -> Acc: 0.375, F1: 0.379, AUC: 0.704, Score: 0.442\n",
            "Embedding time: 6.39s | SVM training time: 0.22s | Optuna time: 49.01s | Memory usage: 1492.80 MB\n",
            "Graph2Vec summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/g2v.csv\n",
            "Loaded dataset ENZYMES for Graph2Vec: 600 graphs, 6 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:20:20,516] A new study created in memory with name: no-name-5d8ac67e-a5c1-4d1e-84b4-400b5059ff4e\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES filtering: removed 1 graphs with < 3 nodes, kept 599 graphs.\n",
            "Running Optuna for Graph2Vec+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:20:25,229] Trial 0 finished with value: 0.19836771514161222 and parameters: {'C': 0.022168464255333798, 'gamma': 1.826866581219186}. Best is trial 0 with value: 0.19836771514161222.\n",
            "[I 2026-01-22 11:20:30,032] Trial 1 finished with value: 0.23151276510941757 and parameters: {'C': 92.62693044042634, 'gamma': 1.9255834746350706}. Best is trial 1 with value: 0.23151276510941757.\n",
            "[I 2026-01-22 11:20:35,152] Trial 2 finished with value: 0.2350431236865821 and parameters: {'C': 0.09243074178871809, 'gamma': 0.007064431631151016}. Best is trial 2 with value: 0.2350431236865821.\n",
            "[I 2026-01-22 11:20:39,866] Trial 3 finished with value: 0.25204261488255575 and parameters: {'C': 0.05070534035476401, 'gamma': 0.006680144988523193}. Best is trial 3 with value: 0.25204261488255575.\n",
            "[I 2026-01-22 11:20:44,746] Trial 4 finished with value: 0.24943107987558416 and parameters: {'C': 0.08657289805603843, 'gamma': 0.01745852280203085}. Best is trial 3 with value: 0.25204261488255575.\n",
            "[I 2026-01-22 11:20:49,846] Trial 5 finished with value: 0.43596020612444314 and parameters: {'C': 29.63425370329404, 'gamma': 0.0012649893394279035}. Best is trial 5 with value: 0.43596020612444314.\n",
            "[I 2026-01-22 11:20:54,609] Trial 6 finished with value: 0.2379997884570495 and parameters: {'C': 2.0326239329137112, 'gamma': 4.976760678931076}. Best is trial 5 with value: 0.43596020612444314.\n",
            "[I 2026-01-22 11:20:59,971] Trial 7 finished with value: 0.23481691141712693 and parameters: {'C': 0.9320310091427892, 'gamma': 0.0002229342645126881}. Best is trial 5 with value: 0.43596020612444314.\n",
            "[I 2026-01-22 11:21:04,897] Trial 8 finished with value: 0.24516124679038281 and parameters: {'C': 3.4687838670925513, 'gamma': 9.171635041490237}. Best is trial 5 with value: 0.43596020612444314.\n",
            "[I 2026-01-22 11:21:09,594] Trial 9 finished with value: 0.43151247475412713 and parameters: {'C': 6.2990454808342395, 'gamma': 0.5332185772404894}. Best is trial 5 with value: 0.43596020612444314.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (Graph2Vec+SVM): {'C': 29.63425370329404, 'gamma': 0.0012649893394279035}\n",
            "Running final Graph2Vec embedding on train+test graphs...\n",
            "Training final SVM on Graph2Vec embeddings...\n",
            "Graph2Vec Results on ENZYMES -> Acc: 0.275, F1: 0.258, AUC: 0.632, Score: 0.341\n",
            "Embedding time: 6.31s | SVM training time: 0.11s | Optuna time: 49.08s | Memory usage: 1492.80 MB\n",
            "Graph2Vec summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/g2v.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:21:16,148] A new study created in memory with name: no-name-1bdaffe9-eafc-4d67-9060-f74a7f41a1c9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset MUTAG for Graph2Vec: 188 graphs, 2 classes\n",
            "Running Optuna for Graph2Vec+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:21:16,899] Trial 0 finished with value: 0.7895987232102144 and parameters: {'C': 2.928792356879726, 'gamma': 0.07150737926991069}. Best is trial 0 with value: 0.7895987232102144.\n",
            "[I 2026-01-22 11:21:17,606] Trial 1 finished with value: 0.8761515151515151 and parameters: {'C': 0.43840233822834634, 'gamma': 4.088779811094886}. Best is trial 1 with value: 0.8761515151515151.\n",
            "[I 2026-01-22 11:21:18,341] Trial 2 finished with value: 0.6723333333333333 and parameters: {'C': 0.8151738375649625, 'gamma': 0.003055336732902248}. Best is trial 1 with value: 0.8761515151515151.\n",
            "[I 2026-01-22 11:21:19,054] Trial 3 finished with value: 0.6723333333333333 and parameters: {'C': 5.311945882934876, 'gamma': 0.011262831277629606}. Best is trial 1 with value: 0.8761515151515151.\n",
            "[I 2026-01-22 11:21:19,758] Trial 4 finished with value: 0.8833333333333333 and parameters: {'C': 3.682030723364377, 'gamma': 1.464667820602452}. Best is trial 4 with value: 0.8833333333333333.\n",
            "[I 2026-01-22 11:21:20,475] Trial 5 finished with value: 0.9167403055229142 and parameters: {'C': 14.850086994440911, 'gamma': 1.0170546306639692}. Best is trial 5 with value: 0.9167403055229142.\n",
            "[I 2026-01-22 11:21:21,196] Trial 6 finished with value: 0.6653333333333333 and parameters: {'C': 0.01574335165439543, 'gamma': 0.00157250175093947}. Best is trial 5 with value: 0.9167403055229142.\n",
            "[I 2026-01-22 11:21:21,925] Trial 7 finished with value: 0.6783333333333333 and parameters: {'C': 0.027851536307588423, 'gamma': 4.367793107401382}. Best is trial 5 with value: 0.9167403055229142.\n",
            "[I 2026-01-22 11:21:22,622] Trial 8 finished with value: 0.6743333333333333 and parameters: {'C': 3.4446390148114383, 'gamma': 0.0002081736727385025}. Best is trial 5 with value: 0.9167403055229142.\n",
            "[I 2026-01-22 11:21:23,337] Trial 9 finished with value: 0.6683333333333333 and parameters: {'C': 0.703355133863198, 'gamma': 0.0002786948865397276}. Best is trial 5 with value: 0.9167403055229142.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (Graph2Vec+SVM): {'C': 14.850086994440911, 'gamma': 1.0170546306639692}\n",
            "Running final Graph2Vec embedding on train+test graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:21:24,729] A new study created in memory with name: no-name-afaeed18-21d1-4948-9144-2b705252f029\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final SVM on Graph2Vec embeddings...\n",
            "Graph2Vec Results on MUTAG -> Acc: 0.816, F1: 0.819, AUC: 0.898, Score: 0.833\n",
            "Embedding time: 1.26s | SVM training time: 0.01s | Optuna time: 7.19s | Memory usage: 1492.80 MB\n",
            "Graph2Vec summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/g2v.csv\n",
            "Loaded dataset MUTAG for Graph2Vec: 188 graphs, 2 classes\n",
            "Running Optuna for Graph2Vec+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:21:25,800] Trial 0 finished with value: 0.6243333333333333 and parameters: {'C': 1.3328625769972589, 'gamma': 0.00010677219995905622}. Best is trial 0 with value: 0.6243333333333333.\n",
            "[I 2026-01-22 11:21:26,827] Trial 1 finished with value: 0.6563333333333333 and parameters: {'C': 0.044127573806444005, 'gamma': 8.790132002099744}. Best is trial 1 with value: 0.6563333333333333.\n",
            "[I 2026-01-22 11:21:27,544] Trial 2 finished with value: 0.6203333333333333 and parameters: {'C': 0.08114085364199364, 'gamma': 0.0005136534944908572}. Best is trial 1 with value: 0.6563333333333333.\n",
            "[I 2026-01-22 11:21:28,255] Trial 3 finished with value: 0.6243333333333333 and parameters: {'C': 0.047012720926921006, 'gamma': 0.004652035326956229}. Best is trial 1 with value: 0.6563333333333333.\n",
            "[I 2026-01-22 11:21:28,956] Trial 4 finished with value: 0.7241983584131327 and parameters: {'C': 3.177004130420317, 'gamma': 0.853982670131212}. Best is trial 4 with value: 0.7241983584131327.\n",
            "[I 2026-01-22 11:21:29,671] Trial 5 finished with value: 0.8690988835725679 and parameters: {'C': 25.682886783000537, 'gamma': 2.4017344636119597}. Best is trial 5 with value: 0.8690988835725679.\n",
            "[I 2026-01-22 11:21:30,379] Trial 6 finished with value: 0.6313333333333333 and parameters: {'C': 21.827908783589443, 'gamma': 0.0010460345993105688}. Best is trial 5 with value: 0.8690988835725679.\n",
            "[I 2026-01-22 11:21:31,076] Trial 7 finished with value: 0.6243333333333333 and parameters: {'C': 0.3056434554123703, 'gamma': 0.0006740305780705468}. Best is trial 5 with value: 0.8690988835725679.\n",
            "[I 2026-01-22 11:21:31,796] Trial 8 finished with value: 0.6293333333333333 and parameters: {'C': 0.48331337313878187, 'gamma': 0.12962030020320786}. Best is trial 5 with value: 0.8690988835725679.\n",
            "[I 2026-01-22 11:21:32,511] Trial 9 finished with value: 0.6543333333333333 and parameters: {'C': 0.013707936108926906, 'gamma': 7.151839089566598}. Best is trial 5 with value: 0.8690988835725679.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (Graph2Vec+SVM): {'C': 25.682886783000537, 'gamma': 2.4017344636119597}\n",
            "Running final Graph2Vec embedding on train+test graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:21:33,493] A new study created in memory with name: no-name-7ec3d2df-d1be-44e3-983d-3e99dd858ef4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final SVM on Graph2Vec embeddings...\n",
            "Graph2Vec Results on MUTAG -> Acc: 0.789, F1: 0.789, AUC: 0.855, Score: 0.803\n",
            "Embedding time: 0.86s | SVM training time: 0.01s | Optuna time: 7.78s | Memory usage: 1492.80 MB\n",
            "Graph2Vec summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/g2v.csv\n",
            "Loaded dataset MUTAG for Graph2Vec: 188 graphs, 2 classes\n",
            "Running Optuna for Graph2Vec+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:21:34,196] Trial 0 finished with value: 0.8721515151515152 and parameters: {'C': 58.78223212855365, 'gamma': 0.0907127870714943}. Best is trial 0 with value: 0.8721515151515152.\n",
            "[I 2026-01-22 11:21:34,918] Trial 1 finished with value: 0.6743333333333333 and parameters: {'C': 22.330691439991902, 'gamma': 0.004020741990001376}. Best is trial 0 with value: 0.8721515151515152.\n",
            "[I 2026-01-22 11:21:35,624] Trial 2 finished with value: 0.8721515151515152 and parameters: {'C': 97.55333163674517, 'gamma': 0.059473586967023}. Best is trial 0 with value: 0.8721515151515152.\n",
            "[I 2026-01-22 11:21:36,321] Trial 3 finished with value: 0.8451111111111111 and parameters: {'C': 24.789645785580827, 'gamma': 0.015011709325307066}. Best is trial 0 with value: 0.8721515151515152.\n",
            "[I 2026-01-22 11:21:37,253] Trial 4 finished with value: 0.8701515151515152 and parameters: {'C': 16.104547148202524, 'gamma': 0.1271690869888018}. Best is trial 0 with value: 0.8721515151515152.\n",
            "[I 2026-01-22 11:21:38,312] Trial 5 finished with value: 0.6653333333333333 and parameters: {'C': 0.034218704906328196, 'gamma': 0.0011996858707680816}. Best is trial 0 with value: 0.8721515151515152.\n",
            "[I 2026-01-22 11:21:39,417] Trial 6 finished with value: 0.846999088007296 and parameters: {'C': 73.4897849314858, 'gamma': 0.04701126610238443}. Best is trial 0 with value: 0.8721515151515152.\n",
            "[I 2026-01-22 11:21:40,273] Trial 7 finished with value: 0.6743333333333333 and parameters: {'C': 5.078217524852292, 'gamma': 0.015689500343866455}. Best is trial 0 with value: 0.8721515151515152.\n",
            "[I 2026-01-22 11:21:40,997] Trial 8 finished with value: 0.6763333333333333 and parameters: {'C': 0.12025045916807985, 'gamma': 0.003491747714473471}. Best is trial 0 with value: 0.8721515151515152.\n",
            "[I 2026-01-22 11:21:41,703] Trial 9 finished with value: 0.6773333333333333 and parameters: {'C': 0.46156253246389706, 'gamma': 0.0011271988560510769}. Best is trial 0 with value: 0.8721515151515152.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (Graph2Vec+SVM): {'C': 58.78223212855365, 'gamma': 0.0907127870714943}\n",
            "Running final Graph2Vec embedding on train+test graphs...\n",
            "Training final SVM on Graph2Vec embeddings...\n",
            "Graph2Vec Results on MUTAG -> Acc: 0.842, F1: 0.839, AUC: 0.926, Score: 0.858\n",
            "Embedding time: 0.85s | SVM training time: 0.01s | Optuna time: 8.21s | Memory usage: 1492.80 MB\n",
            "Graph2Vec summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/g2v.csv\n"
          ]
        }
      ],
      "source": [
        "for dataset_name in [\"IMDB-MULTI\", \"ENZYMES\", \"MUTAG\"]:\n",
        "  for seed in [42, 43, 44]:\n",
        "      run_graph2vec_pipeline(\n",
        "          dataset_name=dataset_name,\n",
        "          seed=seed,\n",
        "          w_acc=0.5, w_f1=0.3, w_auc=0.2,\n",
        "          embedding_dim=128,\n",
        "          epochs=200,\n",
        "          test_size=0.2,\n",
        "          use_optuna=True,\n",
        "          n_trials=10,\n",
        "      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6az0J-SNqi11",
        "outputId": "a465fdf7-87ba-4392-dcc4-155dec5bb046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded dataset IMDB-MULTI for NetLSD: 1500 graphs, 3 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:21:43,083] A new study created in memory with name: no-name-45df0d62-72fe-4456-8c04-7d06b022d65b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Optuna for NetLSD+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:21:47,247] Trial 0 finished with value: 0.5374135279619351 and parameters: {'C': 81.96916636320651, 'gamma': 2.4651663073193326}. Best is trial 0 with value: 0.5374135279619351.\n",
            "[I 2026-01-22 11:21:52,255] Trial 1 finished with value: 0.4608146234804839 and parameters: {'C': 0.0746529154705982, 'gamma': 0.0008905262824697937}. Best is trial 0 with value: 0.5374135279619351.\n",
            "[I 2026-01-22 11:21:56,654] Trial 2 finished with value: 0.40970297107127385 and parameters: {'C': 0.4164293674594436, 'gamma': 0.22513247021379945}. Best is trial 0 with value: 0.5374135279619351.\n",
            "[I 2026-01-22 11:22:00,772] Trial 3 finished with value: 0.5237962046731358 and parameters: {'C': 26.965277702042147, 'gamma': 0.12546595180590156}. Best is trial 0 with value: 0.5374135279619351.\n",
            "[I 2026-01-22 11:22:06,010] Trial 4 finished with value: 0.5050250408031368 and parameters: {'C': 64.85674419033406, 'gamma': 0.009910377943724668}. Best is trial 0 with value: 0.5374135279619351.\n",
            "[I 2026-01-22 11:22:10,427] Trial 5 finished with value: 0.4802044441214369 and parameters: {'C': 27.253458469996723, 'gamma': 1.4915012150869817}. Best is trial 0 with value: 0.5374135279619351.\n",
            "[I 2026-01-22 11:22:14,637] Trial 6 finished with value: 0.4988420875985904 and parameters: {'C': 1.3816410718097332, 'gamma': 0.3078932130862316}. Best is trial 0 with value: 0.5374135279619351.\n",
            "[I 2026-01-22 11:22:20,000] Trial 7 finished with value: 0.4609969151471506 and parameters: {'C': 1.9359161126225066, 'gamma': 0.0019129760873888635}. Best is trial 0 with value: 0.5374135279619351.\n",
            "[I 2026-01-22 11:22:24,098] Trial 8 finished with value: 0.46157504014715056 and parameters: {'C': 4.642406471767988, 'gamma': 0.005923415726304702}. Best is trial 0 with value: 0.5374135279619351.\n",
            "[I 2026-01-22 11:22:28,211] Trial 9 finished with value: 0.46055420681381726 and parameters: {'C': 1.413302293150821, 'gamma': 0.00016398468918552147}. Best is trial 0 with value: 0.5374135279619351.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (NetLSD+SVM): {'C': 81.96916636320651, 'gamma': 2.4651663073193326}\n",
            "Running final NetLSD embedding on train+test graphs...\n",
            "Training final SVM on NetLSD embeddings...\n",
            "NetLSD Results on IMDB-MULTI -> Acc: 0.473, F1: 0.461, AUC: 0.624, Score: 0.500\n",
            "Embedding time: 5.28s | SVM training time: 1.23s | Optuna time: 45.13s | Memory usage: 1496.86 MB\n",
            "NetLSD summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/netlsd.csv\n",
            "Loaded dataset IMDB-MULTI for NetLSD: 1500 graphs, 3 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:22:35,446] A new study created in memory with name: no-name-4d95c660-98ff-4ee1-b336-1cf73cb44222\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Optuna for NetLSD+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:22:39,585] Trial 0 finished with value: 0.4538656910286838 and parameters: {'C': 27.728491204309382, 'gamma': 0.028854218987831713}. Best is trial 0 with value: 0.4538656910286838.\n",
            "[I 2026-01-22 11:22:44,254] Trial 1 finished with value: 0.42963809570182965 and parameters: {'C': 86.28763744739474, 'gamma': 0.00032002558678674684}. Best is trial 0 with value: 0.4538656910286838.\n",
            "[I 2026-01-22 11:22:48,990] Trial 2 finished with value: 0.4816382990439209 and parameters: {'C': 10.744086790110487, 'gamma': 0.43859992018651184}. Best is trial 2 with value: 0.4816382990439209.\n",
            "[I 2026-01-22 11:22:53,077] Trial 3 finished with value: 0.4450307568204061 and parameters: {'C': 68.76825374332157, 'gamma': 0.0071545592333708075}. Best is trial 2 with value: 0.4816382990439209.\n",
            "[I 2026-01-22 11:22:57,887] Trial 4 finished with value: 0.429127679035163 and parameters: {'C': 0.08323949895705819, 'gamma': 0.0037039387993028586}. Best is trial 2 with value: 0.4816382990439209.\n",
            "[I 2026-01-22 11:23:02,410] Trial 5 finished with value: 0.3812598831567512 and parameters: {'C': 0.1433137774499384, 'gamma': 2.0774930102515543}. Best is trial 2 with value: 0.4816382990439209.\n",
            "[I 2026-01-22 11:23:06,504] Trial 6 finished with value: 0.37841092482341787 and parameters: {'C': 0.020312079528691525, 'gamma': 0.6642298196958366}. Best is trial 2 with value: 0.4816382990439209.\n",
            "[I 2026-01-22 11:23:11,510] Trial 7 finished with value: 0.43602475642674776 and parameters: {'C': 0.28593916940428915, 'gamma': 0.03655617282362976}. Best is trial 2 with value: 0.4816382990439209.\n",
            "[I 2026-01-22 11:23:15,935] Trial 8 finished with value: 0.435158420962015 and parameters: {'C': 9.058699280492393, 'gamma': 0.015195475626184902}. Best is trial 2 with value: 0.4816382990439209.\n",
            "[I 2026-01-22 11:23:20,075] Trial 9 finished with value: 0.43185293404540603 and parameters: {'C': 0.015917391348064976, 'gamma': 0.048846342305254}. Best is trial 2 with value: 0.4816382990439209.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (NetLSD+SVM): {'C': 10.744086790110487, 'gamma': 0.43859992018651184}\n",
            "Running final NetLSD embedding on train+test graphs...\n",
            "Training final SVM on NetLSD embeddings...\n",
            "NetLSD Results on IMDB-MULTI -> Acc: 0.477, F1: 0.443, AUC: 0.647, Score: 0.501\n",
            "Embedding time: 5.27s | SVM training time: 1.25s | Optuna time: 44.63s | Memory usage: 1507.71 MB\n",
            "NetLSD summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/netlsd.csv\n",
            "Loaded dataset IMDB-MULTI for NetLSD: 1500 graphs, 3 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:23:27,326] A new study created in memory with name: no-name-4590de40-3897-42a6-8301-264d1cff7354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Optuna for NetLSD+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:23:31,464] Trial 0 finished with value: 0.4402854536931876 and parameters: {'C': 2.0276239287485764, 'gamma': 0.15450989340625354}. Best is trial 0 with value: 0.4402854536931876.\n",
            "[I 2026-01-22 11:23:35,752] Trial 1 finished with value: 0.41586133889315374 and parameters: {'C': 1.941779481806329, 'gamma': 0.0008072621101406602}. Best is trial 0 with value: 0.4402854536931876.\n",
            "[I 2026-01-22 11:23:40,957] Trial 2 finished with value: 0.41501758889315377 and parameters: {'C': 9.508945997521808, 'gamma': 0.004728998391681479}. Best is trial 0 with value: 0.4402854536931876.\n",
            "[I 2026-01-22 11:23:45,084] Trial 3 finished with value: 0.4037060596733153 and parameters: {'C': 0.17762446557499553, 'gamma': 3.409686831491044}. Best is trial 0 with value: 0.4402854536931876.\n",
            "[I 2026-01-22 11:23:49,502] Trial 4 finished with value: 0.4133457138931537 and parameters: {'C': 18.491924825339403, 'gamma': 0.006629493160303805}. Best is trial 0 with value: 0.4402854536931876.\n",
            "[I 2026-01-22 11:23:54,586] Trial 5 finished with value: 0.4367452198181652 and parameters: {'C': 7.392134529948319, 'gamma': 0.025084428717339805}. Best is trial 0 with value: 0.4402854536931876.\n",
            "[I 2026-01-22 11:23:58,715] Trial 6 finished with value: 0.41553842222648707 and parameters: {'C': 0.011155258898449934, 'gamma': 0.000632431783213395}. Best is trial 0 with value: 0.4402854536931876.\n",
            "[I 2026-01-22 11:24:03,420] Trial 7 finished with value: 0.41525717222648706 and parameters: {'C': 1.0537531357106764, 'gamma': 0.00024158251236688008}. Best is trial 0 with value: 0.4402854536931876.\n",
            "[I 2026-01-22 11:24:08,095] Trial 8 finished with value: 0.4527292913973671 and parameters: {'C': 1.4389304517615789, 'gamma': 5.23885861682527}. Best is trial 8 with value: 0.4527292913973671.\n",
            "[I 2026-01-22 11:24:12,215] Trial 9 finished with value: 0.4158405055598204 and parameters: {'C': 48.913060809969714, 'gamma': 0.000494036573273512}. Best is trial 8 with value: 0.4527292913973671.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (NetLSD+SVM): {'C': 1.4389304517615789, 'gamma': 5.23885861682527}\n",
            "Running final NetLSD embedding on train+test graphs...\n",
            "Training final SVM on NetLSD embeddings...\n",
            "NetLSD Results on IMDB-MULTI -> Acc: 0.430, F1: 0.405, AUC: 0.590, Score: 0.454\n",
            "Embedding time: 4.77s | SVM training time: 1.59s | Optuna time: 44.89s | Memory usage: 1525.23 MB\n",
            "NetLSD summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/netlsd.csv\n",
            "Loaded dataset ENZYMES for NetLSD: 600 graphs, 6 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:24:19,393] A new study created in memory with name: no-name-5fa58de2-345b-4833-8f21-e629a92d4974\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES filtering: removed 1 graphs with < 3 nodes, kept 599 graphs.\n",
            "Running Optuna for NetLSD+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:24:21,099] Trial 0 finished with value: 0.19562593843843845 and parameters: {'C': 0.2830399222844576, 'gamma': 0.029930716256350735}. Best is trial 0 with value: 0.19562593843843845.\n",
            "[I 2026-01-22 11:24:22,779] Trial 1 finished with value: 0.27553918608134953 and parameters: {'C': 1.1892659256829006, 'gamma': 5.238665902247061}. Best is trial 1 with value: 0.27553918608134953.\n",
            "[I 2026-01-22 11:24:24,477] Trial 2 finished with value: 0.1978655217717718 and parameters: {'C': 0.014515960701637869, 'gamma': 0.000520427216502338}. Best is trial 1 with value: 0.27553918608134953.\n",
            "[I 2026-01-22 11:24:26,148] Trial 3 finished with value: 0.19239677177177178 and parameters: {'C': 14.1441732505219, 'gamma': 0.0032604192166186888}. Best is trial 1 with value: 0.27553918608134953.\n",
            "[I 2026-01-22 11:24:27,841] Trial 4 finished with value: 0.2769459050912855 and parameters: {'C': 25.499141367917673, 'gamma': 0.7449549129165532}. Best is trial 4 with value: 0.2769459050912855.\n",
            "[I 2026-01-22 11:24:30,168] Trial 5 finished with value: 0.19770927177177178 and parameters: {'C': 1.0511385989991078, 'gamma': 0.0009681558644947692}. Best is trial 4 with value: 0.2769459050912855.\n",
            "[I 2026-01-22 11:24:32,400] Trial 6 finished with value: 0.2925823637618468 and parameters: {'C': 4.069546095892215, 'gamma': 4.667883400832885}. Best is trial 6 with value: 0.2925823637618468.\n",
            "[I 2026-01-22 11:24:34,075] Trial 7 finished with value: 0.23918912937010248 and parameters: {'C': 0.033657726367223646, 'gamma': 7.988077953076491}. Best is trial 6 with value: 0.2925823637618468.\n",
            "[I 2026-01-22 11:24:35,751] Trial 8 finished with value: 0.24723784858837 and parameters: {'C': 4.539086655118391, 'gamma': 0.036367639467523916}. Best is trial 6 with value: 0.2925823637618468.\n",
            "[I 2026-01-22 11:24:37,404] Trial 9 finished with value: 0.29056107954545457 and parameters: {'C': 71.26314927589958, 'gamma': 0.3274437406502989}. Best is trial 6 with value: 0.2925823637618468.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (NetLSD+SVM): {'C': 4.069546095892215, 'gamma': 4.667883400832885}\n",
            "Running final NetLSD embedding on train+test graphs...\n",
            "Training final SVM on NetLSD embeddings...\n",
            "NetLSD Results on ENZYMES -> Acc: 0.308, F1: 0.261, AUC: 0.647, Score: 0.362\n",
            "Embedding time: 1.91s | SVM training time: 0.17s | Optuna time: 18.01s | Memory usage: 1527.29 MB\n",
            "NetLSD summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/netlsd.csv\n",
            "Loaded dataset ENZYMES for NetLSD: 600 graphs, 6 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:24:39,812] A new study created in memory with name: no-name-ee3b0a52-04c4-44bf-9623-abedf1c21cc8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES filtering: removed 1 graphs with < 3 nodes, kept 599 graphs.\n",
            "Running Optuna for NetLSD+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:24:41,481] Trial 0 finished with value: 0.3515683155280195 and parameters: {'C': 36.14789786017126, 'gamma': 0.40767582037375194}. Best is trial 0 with value: 0.3515683155280195.\n",
            "[I 2026-01-22 11:24:44,022] Trial 1 finished with value: 0.2918718174696436 and parameters: {'C': 3.1285000072615605, 'gamma': 0.015875377193453114}. Best is trial 0 with value: 0.3515683155280195.\n",
            "[I 2026-01-22 11:24:46,059] Trial 2 finished with value: 0.33889540811415814 and parameters: {'C': 76.67306937849207, 'gamma': 1.2814820104449591}. Best is trial 0 with value: 0.3515683155280195.\n",
            "[I 2026-01-22 11:24:47,716] Trial 3 finished with value: 0.29838449862906385 and parameters: {'C': 0.042103251595216586, 'gamma': 0.0149122275362293}. Best is trial 0 with value: 0.3515683155280195.\n",
            "[I 2026-01-22 11:24:49,440] Trial 4 finished with value: 0.2832995989644768 and parameters: {'C': 7.355040685187284, 'gamma': 0.001198403268446751}. Best is trial 0 with value: 0.3515683155280195.\n",
            "[I 2026-01-22 11:24:51,109] Trial 5 finished with value: 0.2833256406311435 and parameters: {'C': 0.01013354170129473, 'gamma': 0.000258900746483483}. Best is trial 0 with value: 0.3515683155280195.\n",
            "[I 2026-01-22 11:24:52,796] Trial 6 finished with value: 0.24409104938271606 and parameters: {'C': 0.6462169506729759, 'gamma': 0.5018866848236422}. Best is trial 0 with value: 0.3515683155280195.\n",
            "[I 2026-01-22 11:24:54,479] Trial 7 finished with value: 0.30539484581510445 and parameters: {'C': 6.136923043764394, 'gamma': 1.2904801617374135}. Best is trial 0 with value: 0.3515683155280195.\n",
            "[I 2026-01-22 11:24:56,932] Trial 8 finished with value: 0.27964997978089845 and parameters: {'C': 21.86559532969537, 'gamma': 0.030768704506865596}. Best is trial 0 with value: 0.3515683155280195.\n",
            "[I 2026-01-22 11:24:59,065] Trial 9 finished with value: 0.2542002314814815 and parameters: {'C': 1.7739046205782238, 'gamma': 0.25665490346768505}. Best is trial 0 with value: 0.3515683155280195.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (NetLSD+SVM): {'C': 36.14789786017126, 'gamma': 0.40767582037375194}\n",
            "Running final NetLSD embedding on train+test graphs...\n",
            "Training final SVM on NetLSD embeddings...\n",
            "NetLSD Results on ENZYMES -> Acc: 0.358, F1: 0.328, AUC: 0.658, Score: 0.409\n",
            "Embedding time: 1.90s | SVM training time: 0.19s | Optuna time: 19.25s | Memory usage: 1527.29 MB\n",
            "NetLSD summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/netlsd.csv\n",
            "Loaded dataset ENZYMES for NetLSD: 600 graphs, 6 classes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:25:01,478] A new study created in memory with name: no-name-9e70939b-a081-4d8c-85a1-15207a49641b\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ENZYMES filtering: removed 1 graphs with < 3 nodes, kept 599 graphs.\n",
            "Running Optuna for NetLSD+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:25:03,155] Trial 0 finished with value: 0.26387963845950363 and parameters: {'C': 4.871321329551589, 'gamma': 0.00030547087348074424}. Best is trial 0 with value: 0.26387963845950363.\n",
            "[I 2026-01-22 11:25:04,820] Trial 1 finished with value: 0.26427026345950366 and parameters: {'C': 0.10465006561082349, 'gamma': 0.00033498745849534436}. Best is trial 1 with value: 0.26427026345950366.\n",
            "[I 2026-01-22 11:25:06,486] Trial 2 finished with value: 0.2590172532325275 and parameters: {'C': 12.216732047733386, 'gamma': 0.09913932481243938}. Best is trial 1 with value: 0.26427026345950366.\n",
            "[I 2026-01-22 11:25:08,198] Trial 3 finished with value: 0.23908218125960062 and parameters: {'C': 0.09264670202874987, 'gamma': 5.577821698302832}. Best is trial 1 with value: 0.26427026345950366.\n",
            "[I 2026-01-22 11:25:10,864] Trial 4 finished with value: 0.24398200145442792 and parameters: {'C': 8.905443630277045, 'gamma': 0.2561893422474579}. Best is trial 1 with value: 0.26427026345950366.\n",
            "[I 2026-01-22 11:25:12,727] Trial 5 finished with value: 0.4168417370016029 and parameters: {'C': 2.961013273496666, 'gamma': 6.276420427717911}. Best is trial 5 with value: 0.4168417370016029.\n",
            "[I 2026-01-22 11:25:14,404] Trial 6 finished with value: 0.3022730992351569 and parameters: {'C': 0.15460983218509253, 'gamma': 7.922647926673068}. Best is trial 5 with value: 0.4168417370016029.\n",
            "[I 2026-01-22 11:25:16,075] Trial 7 finished with value: 0.2639577634595036 and parameters: {'C': 0.16402174149900306, 'gamma': 0.000503431545067182}. Best is trial 5 with value: 0.4168417370016029.\n",
            "[I 2026-01-22 11:25:17,752] Trial 8 finished with value: 0.25468630036799744 and parameters: {'C': 9.480198385015438, 'gamma': 0.0008516784095766004}. Best is trial 5 with value: 0.4168417370016029.\n",
            "[I 2026-01-22 11:25:19,459] Trial 9 finished with value: 0.2547383837013308 and parameters: {'C': 0.13807901753406096, 'gamma': 0.0007543045514229029}. Best is trial 5 with value: 0.4168417370016029.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (NetLSD+SVM): {'C': 2.961013273496666, 'gamma': 6.276420427717911}\n",
            "Running final NetLSD embedding on train+test graphs...\n",
            "Training final SVM on NetLSD embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:25:21,843] A new study created in memory with name: no-name-641878fe-63e3-4860-a9d7-8d94453c4a74\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NetLSD Results on ENZYMES -> Acc: 0.308, F1: 0.271, AUC: 0.670, Score: 0.370\n",
            "Embedding time: 1.91s | SVM training time: 0.28s | Optuna time: 17.98s | Memory usage: 1527.30 MB\n",
            "NetLSD summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/netlsd.csv\n",
            "Loaded dataset MUTAG for NetLSD: 188 graphs, 2 classes\n",
            "Running Optuna for NetLSD+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:25:22,586] Trial 0 finished with value: 0.6633333333333333 and parameters: {'C': 0.05482130032706388, 'gamma': 0.2705452218606704}. Best is trial 0 with value: 0.6633333333333333.\n",
            "[I 2026-01-22 11:25:23,264] Trial 1 finished with value: 0.6633333333333333 and parameters: {'C': 0.01523618674017149, 'gamma': 2.8021059595524314}. Best is trial 0 with value: 0.6633333333333333.\n",
            "[I 2026-01-22 11:25:23,959] Trial 2 finished with value: 0.6633333333333333 and parameters: {'C': 0.479904891522026, 'gamma': 0.005974948338975592}. Best is trial 0 with value: 0.6633333333333333.\n",
            "[I 2026-01-22 11:25:24,657] Trial 3 finished with value: 0.6633333333333333 and parameters: {'C': 61.343487547175855, 'gamma': 0.0019624025284607975}. Best is trial 0 with value: 0.6633333333333333.\n",
            "[I 2026-01-22 11:25:25,079] Trial 4 finished with value: 0.8010952380952382 and parameters: {'C': 2.530452345102756, 'gamma': 1.7335090629433276}. Best is trial 4 with value: 0.8010952380952382.\n",
            "[I 2026-01-22 11:25:25,506] Trial 5 finished with value: 0.6633333333333333 and parameters: {'C': 0.01542826694807372, 'gamma': 0.5561229639194699}. Best is trial 4 with value: 0.8010952380952382.\n",
            "[I 2026-01-22 11:25:25,946] Trial 6 finished with value: 0.7820952380952382 and parameters: {'C': 17.268242263301694, 'gamma': 7.534013343256488}. Best is trial 4 with value: 0.8010952380952382.\n",
            "[I 2026-01-22 11:25:26,370] Trial 7 finished with value: 0.6633333333333333 and parameters: {'C': 0.01426224367787236, 'gamma': 1.227730014143201}. Best is trial 4 with value: 0.8010952380952382.\n",
            "[I 2026-01-22 11:25:26,803] Trial 8 finished with value: 0.7900952380952382 and parameters: {'C': 38.94921842821877, 'gamma': 2.574292094134803}. Best is trial 4 with value: 0.8010952380952382.\n",
            "[I 2026-01-22 11:25:27,240] Trial 9 finished with value: 0.6633333333333333 and parameters: {'C': 0.15405783830107478, 'gamma': 0.14091484371773233}. Best is trial 4 with value: 0.8010952380952382.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (NetLSD+SVM): {'C': 2.530452345102756, 'gamma': 1.7335090629433276}\n",
            "Running final NetLSD embedding on train+test graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:25:27,882] A new study created in memory with name: no-name-2562c71a-a7f5-4433-8949-aec76782bfab\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final SVM on NetLSD embeddings...\n",
            "NetLSD Results on MUTAG -> Acc: 0.921, F1: 0.918, AUC: 0.942, Score: 0.924\n",
            "Embedding time: 0.54s | SVM training time: 0.01s | Optuna time: 5.40s | Memory usage: 1527.30 MB\n",
            "NetLSD summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/netlsd.csv\n",
            "Loaded dataset MUTAG for NetLSD: 188 graphs, 2 classes\n",
            "Running Optuna for NetLSD+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:25:28,315] Trial 0 finished with value: 0.6663333333333333 and parameters: {'C': 5.625719364186944, 'gamma': 0.01104647759798478}. Best is trial 0 with value: 0.6663333333333333.\n",
            "[I 2026-01-22 11:25:28,754] Trial 1 finished with value: 0.6673333333333333 and parameters: {'C': 0.23134086652512803, 'gamma': 0.001308684784080036}. Best is trial 1 with value: 0.6673333333333333.\n",
            "[I 2026-01-22 11:25:29,186] Trial 2 finished with value: 0.6663333333333333 and parameters: {'C': 37.21032023139764, 'gamma': 0.0002431299592447946}. Best is trial 1 with value: 0.6673333333333333.\n",
            "[I 2026-01-22 11:25:29,608] Trial 3 finished with value: 0.7835987232102144 and parameters: {'C': 2.3722069344167003, 'gamma': 0.2879533852413583}. Best is trial 3 with value: 0.7835987232102144.\n",
            "[I 2026-01-22 11:25:30,044] Trial 4 finished with value: 0.6663333333333333 and parameters: {'C': 1.961432262918615, 'gamma': 0.060291559258676825}. Best is trial 3 with value: 0.7835987232102144.\n",
            "[I 2026-01-22 11:25:30,468] Trial 5 finished with value: 0.8399606332905435 and parameters: {'C': 48.85382512053904, 'gamma': 0.021565260350542452}. Best is trial 5 with value: 0.8399606332905435.\n",
            "[I 2026-01-22 11:25:30,898] Trial 6 finished with value: 0.7835987232102144 and parameters: {'C': 0.3444765215454282, 'gamma': 1.9110238808813087}. Best is trial 5 with value: 0.8399606332905435.\n",
            "[I 2026-01-22 11:25:31,319] Trial 7 finished with value: 0.8409606332905435 and parameters: {'C': 67.19009815548374, 'gamma': 5.843143340246375}. Best is trial 7 with value: 0.8409606332905435.\n",
            "[I 2026-01-22 11:25:31,740] Trial 8 finished with value: 0.6673333333333333 and parameters: {'C': 50.95172998910224, 'gamma': 0.0013247612271111036}. Best is trial 7 with value: 0.8409606332905435.\n",
            "[I 2026-01-22 11:25:32,174] Trial 9 finished with value: 0.6673333333333333 and parameters: {'C': 0.11311134135231944, 'gamma': 0.032202265436273636}. Best is trial 7 with value: 0.8409606332905435.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (NetLSD+SVM): {'C': 67.19009815548374, 'gamma': 5.843143340246375}\n",
            "Running final NetLSD embedding on train+test graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:25:32,809] A new study created in memory with name: no-name-c1a5c52e-7d15-4708-ab37-f8a6f3ad1f97\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training final SVM on NetLSD embeddings...\n",
            "NetLSD Results on MUTAG -> Acc: 0.842, F1: 0.834, AUC: 0.868, Score: 0.845\n",
            "Embedding time: 0.52s | SVM training time: 0.01s | Optuna time: 4.29s | Memory usage: 1527.30 MB\n",
            "NetLSD summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/netlsd.csv\n",
            "Loaded dataset MUTAG for NetLSD: 188 graphs, 2 classes\n",
            "Running Optuna for NetLSD+SVM hyperparameter tuning...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2026-01-22 11:25:33,258] Trial 0 finished with value: 0.6793333333333333 and parameters: {'C': 0.8880727709322209, 'gamma': 0.012999337729915883}. Best is trial 0 with value: 0.6793333333333333.\n",
            "[I 2026-01-22 11:25:33,683] Trial 1 finished with value: 0.6783333333333333 and parameters: {'C': 0.4629197614768261, 'gamma': 0.0003096684486221798}. Best is trial 0 with value: 0.6793333333333333.\n",
            "[I 2026-01-22 11:25:34,119] Trial 2 finished with value: 0.6793333333333333 and parameters: {'C': 13.88582700019075, 'gamma': 0.0001154011292969847}. Best is trial 0 with value: 0.6793333333333333.\n",
            "[I 2026-01-22 11:25:34,542] Trial 3 finished with value: 0.897576379974326 and parameters: {'C': 1.2909118671767277, 'gamma': 4.921756179466289}. Best is trial 3 with value: 0.897576379974326.\n",
            "[I 2026-01-22 11:25:35,177] Trial 4 finished with value: 0.8222272727272728 and parameters: {'C': 0.5501418134388858, 'gamma': 1.0464965915085642}. Best is trial 3 with value: 0.897576379974326.\n",
            "[I 2026-01-22 11:25:35,839] Trial 5 finished with value: 0.9216666666666666 and parameters: {'C': 29.56904956730278, 'gamma': 9.70897221600041}. Best is trial 5 with value: 0.9216666666666666.\n",
            "[I 2026-01-22 11:25:36,519] Trial 6 finished with value: 0.7856999454446264 and parameters: {'C': 1.3780380073478835, 'gamma': 0.3051207023739014}. Best is trial 5 with value: 0.9216666666666666.\n",
            "[I 2026-01-22 11:25:37,196] Trial 7 finished with value: 0.6793333333333333 and parameters: {'C': 16.819568808703828, 'gamma': 0.00012975300148029001}. Best is trial 5 with value: 0.9216666666666666.\n",
            "[I 2026-01-22 11:25:37,891] Trial 8 finished with value: 0.6793333333333333 and parameters: {'C': 18.41664644726129, 'gamma': 0.005530774993837819}. Best is trial 5 with value: 0.9216666666666666.\n",
            "[I 2026-01-22 11:25:38,330] Trial 9 finished with value: 0.9216666666666666 and parameters: {'C': 46.076075284667965, 'gamma': 8.317878466741153}. Best is trial 5 with value: 0.9216666666666666.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters (NetLSD+SVM): {'C': 29.56904956730278, 'gamma': 9.70897221600041}\n",
            "Running final NetLSD embedding on train+test graphs...\n",
            "Training final SVM on NetLSD embeddings...\n",
            "NetLSD Results on MUTAG -> Acc: 0.868, F1: 0.867, AUC: 0.914, Score: 0.877\n",
            "Embedding time: 0.52s | SVM training time: 0.01s | Optuna time: 5.52s | Memory usage: 1527.30 MB\n",
            "NetLSD summary stored in: /content/drive/MyDrive/InformationSystems/Classification/results/classification/netlsd.csv\n"
          ]
        }
      ],
      "source": [
        "for dataset_name in [\"IMDB-MULTI\", \"ENZYMES\", \"MUTAG\"]:\n",
        "  for seed in [42, 43, 44]:\n",
        "      run_netlsd_pipeline(\n",
        "          dataset_name=dataset_name,\n",
        "          seed=seed,\n",
        "          w_acc=0.5, w_f1=0.3, w_auc=0.2,\n",
        "          test_size=0.2,\n",
        "          use_optuna=True,\n",
        "          n_trials=10,\n",
        "      )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}